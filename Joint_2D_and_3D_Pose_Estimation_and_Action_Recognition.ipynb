{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeDy2MdLNigSnOy74DFsC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayane2907/Action_recognition_pose_estimation/blob/main/Joint_2D_and_3D_Pose_Estimation_and_Action_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Joint 2D and 3D Pose Estimation and Action Recognition**\n",
        "This whole project is based on the research paper of 2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning by Diogo C. Luvizon1\n",
        ", David Picard and Hedi Tabia and . Hence, most of the ideas are driven from that research paper, however, I changed the Network architecture from Inception-V4  into VGG 16.\n",
        "\n",
        "link: https://github.com/dluvizon/deephar/tree/master"
      ],
      "metadata": {
        "id": "t602a4hBSBum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow[and-cuda]"
      ],
      "metadata": {
        "id": "VQor3XyHR6Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import cv2\n",
        "import gzip\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import queue\n",
        "import random\n",
        "import scipy.io as sio\n",
        "import tensorflow as tf\n",
        "import threading\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "from keras import backend as K\n",
        "from keras.callbacks import Callback, LearningRateScheduler, ProgbarLogger, TensorBoard\n",
        "from keras.constraints import unit_norm\n",
        "from keras.layers import (Activation, AveragePooling2D, BatchNormalization, Conv1D, Conv2D, Conv2DTranspose, Conv3D,\n",
        "                          Dense, Dropout, Flatten, GlobalAveragePooling1D, GlobalAveragePooling2D, GlobalAveragePooling3D,\n",
        "                          GlobalMaxPooling1D, GlobalMaxPooling2D, GlobalMaxPooling3D, Input, Lambda, LeakyReLU, LocallyConnected1D,\n",
        "                          LSTM, MaxPooling2D, MaxPooling3D, SeparableConv2D, SimpleRNN, TimeDistributed, UpSampling2D, UpSampling3D,\n",
        "                          ZeroPadding2D, add, average, concatenate, maximum, multiply)\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.regularizers import l1\n",
        "from keras.utils import OrderedEnqueuer, Sequence\n",
        "from multiprocessing import Queue\n",
        "from PIL import Image\n",
        "from scipy.cluster.vq import kmeans\n",
        "from scipy.stats import multivariate_normal\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import (Conv2D, GlobalAveragePooling2D, MaxPooling2D, Reshape, TimeDistributed)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers.legacy import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "K.set_image_data_format('channels_last')"
      ],
      "metadata": {
        "id": "TRUFScyUS1yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utilities**"
      ],
      "metadata": {
        "id": "I2mUqoX1TLBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_valid_joints(x):\n",
        "    return np.apply_along_axis(_func_and, axis=1, arr=(x > -1e6))\n",
        "\n",
        "\n",
        "HEADER = '\\033[95m'\n",
        "OKBLUE = '\\033[94m'\n",
        "OKGREEN = '\\033[92m'\n",
        "WARNING = '\\033[93m'\n",
        "FAIL = '\\033[91m'\n",
        "ENDC = '\\033[0m'\n",
        "\n",
        "def printc(color, vmsg):\n",
        "    print (color + vmsg + ENDC, end='')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def printcn(color, vmsg):\n",
        "    print (color + vmsg + ENDC)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def printnl(vmsg):\n",
        "    sys.stdout.write(vmsg + '\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def warning(vmsg):\n",
        "    sys.stderr.write(WARNING + vmsg + ENDC + '\\n')\n",
        "    sys.stderr.flush()\n",
        "\n",
        "def sprintcn(color, vmsg):\n",
        "    return color + vmsg + ENDC + '\\n'\n",
        "\n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "def sizeof_eng_fmt(num):\n",
        "    for unit in ['','K','M','G','T','P','E','Z']:\n",
        "        if abs(num) < 1e3:\n",
        "            return \"%3.1f%s\" % (num, unit)\n",
        "        num /= 1e3\n",
        "    return \"%3.1f%s\" % (num, 'Y')\n",
        "\n",
        "relsize_std = 1.5\n",
        "square_std = True\n",
        "\n",
        "class PoseBBox():\n",
        "    def __init__(self, poses, relsize=relsize_std, square=square_std):\n",
        "        self.poses = poses\n",
        "        self.relsize = relsize\n",
        "        self.square = square\n",
        "        if len(poses.shape) == 4:\n",
        "            self.num_frames = poses.shape[1]\n",
        "        else:\n",
        "            self.num_frames = None\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        p = self.poses[key]\n",
        "        if isinstance(key, int):\n",
        "            return self._get_bbox(p)\n",
        "        if isinstance(key, slice):\n",
        "            indices = key.indices(len(self))\n",
        "            key = range(*indices)\n",
        "        x = np.zeros((len(key),) + self.shape[1:])\n",
        "        for i in range(len(key)):\n",
        "            x[i,:] = self._get_bbox(p[i])\n",
        "        return x\n",
        "\n",
        "    def _get_bbox(self, p):\n",
        "        if self.num_frames is None:\n",
        "            return get_valid_bbox(p, relsize=self.relsize, square=self.square)\n",
        "        else:\n",
        "            b = np.zeros(self.shape[1:])\n",
        "            for f in range(self.num_frames):\n",
        "                b[f, :] = get_valid_bbox(p[f], self.relsize, self.square)\n",
        "            return b\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.poses)\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        if self.num_frames is None:\n",
        "            return (len(self), 4)\n",
        "        else:\n",
        "            return (len(self), self.num_frames, 4)\n",
        "\n",
        "def get_valid_bbox(points, jprob=None, relsize=relsize_std, square=square_std):\n",
        "    if jprob is None:\n",
        "        v = get_valid_joints(points)\n",
        "    else:\n",
        "        v = np.squeeze(jprob > 0.5)\n",
        "\n",
        "    if v.any():\n",
        "        x = points[v==1, 0]\n",
        "        y = points[v==1, 1]\n",
        "    else:\n",
        "        raise ValueError('get_valid_bbox: all points are invalid!')\n",
        "        # printcn(WARNING, 'All points are invalid! ' + str(points))\n",
        "        # x = np.array([0.5])\n",
        "        # y = np.array([0.5])\n",
        "\n",
        "    cx = (min(x) + max(x)) / 2.\n",
        "    cy = (min(y) + max(y)) / 2.\n",
        "    rw = (relsize * (max(x) - min(x))) / 2.\n",
        "    rh = (relsize * (max(y) - min(y))) / 2.\n",
        "    if square:\n",
        "        rw = max(rw, rh)\n",
        "        rh = max(rw, rh)\n",
        "\n",
        "    return np.array([cx - rw, cy - rh, cx + rw, cy + rh])\n",
        "\n",
        "def get_valid_bbox_array(pointarray, jprob=None, relsize=relsize_std,\n",
        "        square=square_std):\n",
        "\n",
        "    bboxes = np.zeros((len(pointarray), 4))\n",
        "    v = None\n",
        "    for i in range(len(pointarray)):\n",
        "        if jprob is not None:\n",
        "            v = jprob[i]\n",
        "        bboxes[i, :] = get_valid_bbox(pointarray[i], jprob=v,\n",
        "                relsize=relsize, square=square)\n",
        "\n",
        "    return bboxes\n",
        "\n",
        "def get_objpos_winsize(points, relsize=relsize_std, square=square_std):\n",
        "    x = points[:, 0]\n",
        "    y = points[:, 1]\n",
        "    cx = (min(x) + max(x)) / 2.\n",
        "    cy = (min(y) + max(y)) / 2.\n",
        "    w = relsize * (max(x) - min(x))\n",
        "    h = relsize * (max(y) - min(y))\n",
        "    if square:\n",
        "        w = max(w, h)\n",
        "        h = max(w, h)\n",
        "\n",
        "    return np.array([cx, cy]), (w, h)\n",
        "\n",
        "def compute_grid_bboxes(frame_size, grid=(3, 2),\n",
        "        relsize=relsize_std,\n",
        "        square=square_std):\n",
        "\n",
        "    bb_cnt = 0\n",
        "    num_bb = 2 + grid[0]*grid[1]\n",
        "    bboxes = np.zeros((num_bb, 4))\n",
        "\n",
        "    def _smax(a, b):\n",
        "        if square:\n",
        "            return max(a, b), max(a, b)\n",
        "        return a, b\n",
        "\n",
        "    # Compute the first two bounding boxes as the full frame + relsize\n",
        "    cx = frame_size[0] / 2\n",
        "    cy = frame_size[1] / 2\n",
        "    rw, rh = _smax(cx, cy)\n",
        "    bboxes[bb_cnt, :] = np.array([cx-rw, cy-rh, cx+rw, cy+rh])\n",
        "    bb_cnt += 1\n",
        "\n",
        "    rw *= relsize\n",
        "    rh *= relsize\n",
        "    bboxes[bb_cnt, :] = np.array([cx-rw, cy-rh, cx+rw, cy+rh])\n",
        "    bb_cnt += 1\n",
        "\n",
        "    winrw = frame_size[0] / (grid[0]+1)\n",
        "    winrh = frame_size[1] / (grid[1]+1)\n",
        "    rw, rh = _smax(winrw, winrh)\n",
        "\n",
        "    for j in range(1, grid[1]+1):\n",
        "        for i in range(1, grid[0]+1):\n",
        "            cx = i * winrw\n",
        "            cy = j * winrh\n",
        "            bboxes[bb_cnt, :] = np.array([cx-rw, cy-rh, cx+rw, cy+rh])\n",
        "            bb_cnt += 1\n",
        "\n",
        "    return bboxes\n",
        "\n",
        "def bbox_to_objposwin(bbox):\n",
        "    cx = (bbox[0] + bbox[2]) / 2\n",
        "    cy = (bbox[1] + bbox[3]) / 2\n",
        "    wx = bbox[2] - bbox[0]\n",
        "    wy = bbox[3] - bbox[1]\n",
        "\n",
        "    return np.array([cx, cy]), (wx, wy)\n",
        "\n",
        "def objposwin_to_bbox(objpos, winsize):\n",
        "    x1 = objpos[0] - winsize[0]/2\n",
        "    y1 = objpos[1] - winsize[1]/2\n",
        "    x2 = objpos[0] + winsize[0]/2\n",
        "    y2 = objpos[1] + winsize[1]/2\n",
        "\n",
        "    return np.array([x1, y1, x2, y2])\n",
        "\n",
        "\n",
        "logkey_warn = set()\n",
        "def get_gt_bbox(pose, visible, image_size, scale=1.0, logkey=None):\n",
        "    assert len(pose.shape) == 3 and pose.shape[-1] >= 2, \\\n",
        "            'Invalid pose shape ({})'.format(pose.shape) \\\n",
        "            + ', expected (num_frames, num_joints, dim) vector'\n",
        "    assert len(pose) == len(visible), \\\n",
        "            'pose and visible should have the same langth'\n",
        "\n",
        "    if len(pose) == 1:\n",
        "        idx = [0]\n",
        "    else:\n",
        "        idx = [0, int(len(pose)/2 + 0.5), len(pose)-1]\n",
        "\n",
        "    clip_bbox = np.array([np.inf, np.inf, -np.inf, -np.inf])\n",
        "\n",
        "    for i in idx:\n",
        "        temp = pose[i, visible[i] >= 0.5]\n",
        "        if len(temp) == 0:\n",
        "            temp = pose[i, pose[i] > 0]\n",
        "\n",
        "        if len(temp) > 0:\n",
        "            b = get_valid_bbox(temp, relsize=1.5*scale)\n",
        "\n",
        "            clip_bbox[0] = min(b[0], clip_bbox[0])\n",
        "            clip_bbox[1] = min(b[1], clip_bbox[1])\n",
        "            clip_bbox[2] = max(b[2], clip_bbox[2])\n",
        "            clip_bbox[3] = max(b[3], clip_bbox[3])\n",
        "        else:\n",
        "            if logkey not in logkey_warn:\n",
        "                warning('No ground-truth bounding box, ' \\\n",
        "                        'using full image (key {})!'.format(logkey))\n",
        "            logkey_warn.add(logkey)\n",
        "\n",
        "            clip_bbox[0] = min(0, clip_bbox[0])\n",
        "            clip_bbox[1] = min(0, clip_bbox[1])\n",
        "            clip_bbox[2] = max(image_size[0], clip_bbox[2])\n",
        "            clip_bbox[3] = max(image_size[1], clip_bbox[3])\n",
        "\n",
        "    return clip_bbox\n",
        "\n",
        "\n",
        "def get_crop_params(rootj, imgsize, f, scale):\n",
        "    assert len(rootj.shape) == 2 and rootj.shape[-1] == 3, 'Invalid rootj ' \\\n",
        "            + 'shape ({}), expected (n, 3) vector'.format(rootj.shape)\n",
        "\n",
        "    if len(rootj) == 1:\n",
        "        idx = [0]\n",
        "    else:\n",
        "        idx = [0, int(len(rootj)/2 + 0.5), len(rootj)-1]\n",
        "\n",
        "    x1 = y1 = np.inf\n",
        "    x2 = y2 = -np.inf\n",
        "    zrange = np.array([np.inf, -np.inf])\n",
        "    for i in idx:\n",
        "        objpos = np.array([rootj[0, 0], rootj[0, 1] + scale])\n",
        "        d = rootj[0, 2]\n",
        "        winsize = (2.25*scale)*max(imgsize[0]*f[0, 0]/d, imgsize[1]*f[0, 1]/d)\n",
        "        bo = objposwin_to_bbox(objpos, (winsize, winsize))\n",
        "        x1 = min(x1, bo[0])\n",
        "        y1 = min(y1, bo[1])\n",
        "        x2 = max(x2, bo[2])\n",
        "        y2 = max(y2, bo[3])\n",
        "        zrange[0] = min(zrange[0], d - scale*1000.)\n",
        "        zrange[1] = max(zrange[1], d + scale*1000.)\n",
        "\n",
        "    objpos, winsize = bbox_to_objposwin([x1, y1, x2, y2])\n",
        "\n",
        "    return objpos, winsize, zrange"
      ],
      "metadata": {
        "id": "vdo_jGs8S9zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def transform_pose_sequence(A, poses, inverse=True):\n",
        "    \"\"\"For each pose in a sequence, apply the given affine transformation.\n",
        "\n",
        "    # Arguments\n",
        "        A: [3, 3] affine transformation matrix or\n",
        "           [num_samples, 3, 3] matrices.\n",
        "        poses: [num_samples, num_points, dim] vector of pose sequences.\n",
        "        inverse: flag to apply the inverse transformation on A.\n",
        "\n",
        "    # Return\n",
        "        The transformed points.\n",
        "    \"\"\"\n",
        "\n",
        "    assert (len(poses.shape) == 3), \\\n",
        "            'transform_pose_sequence: expected 3D tensor, got ' \\\n",
        "            + str(poses.shape)\n",
        "\n",
        "    if len(A.shape) == 3:\n",
        "        assert len(A) == len(poses), \\\n",
        "                'A is ' + str(A.shape) + ' and poses is ' + str(poses.shape)\n",
        "\n",
        "    if inverse:\n",
        "        if len(A.shape) == 3:\n",
        "            for i in range(len(A)):\n",
        "                A[i] = np.linalg.inv(A[i])\n",
        "        else:\n",
        "            A = np.linalg.inv(A)\n",
        "\n",
        "    y = np.empty(poses.shape)\n",
        "    for j in range(len(poses)):\n",
        "        if len(A.shape) == 3:\n",
        "            y[j, :, :] = transform_2d_points(A[j], poses[j], transpose=True)\n",
        "        else:\n",
        "            y[j, :, :] = transform_2d_points(A, poses[j], transpose=True)\n",
        "\n",
        "    return y\n",
        "\n",
        "class Camera(object):\n",
        "    \"\"\"Camera implementation.\n",
        "\n",
        "    # Arguments\n",
        "        R: Rotation matrix (3,3)\n",
        "        t: Translation vector world coordinate system (3, 1)\n",
        "        f: Focal length (1, 2)\n",
        "        c: Principal point (1, 2)\n",
        "        p: Skew (1, 2)\n",
        "        k: Distortion coefficients (3,), frequently not required.\n",
        "\n",
        "    # TODO\n",
        "        Implement distortion coefficients.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, R, t, f, c, p, k=None):\n",
        "        self.R = R\n",
        "        self.R_inv = np.linalg.inv(self.R)\n",
        "        self.t = np.reshape(t, (3, 1))\n",
        "        self.f = np.reshape(f, (1, 2))\n",
        "        self.c = np.reshape(c, (1, 2))\n",
        "        self.p = np.reshape(p, (1, 2))\n",
        "        self.k = k\n",
        "        if self.k is not None:\n",
        "            self.k = np.reshape(self.k, (3,))\n",
        "\n",
        "    def project(self, points_w):\n",
        "        \"\"\"Projects world 3D points (in millimeters) to the image coordinate\n",
        "        system (in x-y pixels and depth).\n",
        "        \"\"\"\n",
        "        assert len(points_w.shape) == 2 and points_w.shape[1] == 3, \\\n",
        "                'Invalid shape for points_w ({}),'.format(points_w.shape) \\\n",
        "                + 'expected (n, 3)'\n",
        "\n",
        "        x = np.matmul(self.R, points_w.T - self.t).T\n",
        "        x[:, 0:2] /= x[:, 2:3]\n",
        "\n",
        "        if self.k is not None:\n",
        "            r2, radial, tan = get_r2_radial_tan(x[:, 0:2], self.k, self.p)\n",
        "            x[:, 0:2] *= np.expand_dims(radial + tan, axis=-1)\n",
        "            x[:, 0:2] += np.dot(np.expand_dims(r2, axis=-1), self.p)\n",
        "\n",
        "        x[:, 0:2] = x[:, 0:2]*self.f + self.c\n",
        "\n",
        "        return x\n",
        "\n",
        "    def inverse_project(self, points_uvd):\n",
        "        \"\"\"Projects a point in the camera coordinate system (x-y in pixels and\n",
        "        depth) to world 3D coordinates (in millimeters).\n",
        "        \"\"\"\n",
        "        assert len(points_uvd.shape) == 2 and points_uvd.shape[1] == 3, \\\n",
        "                'Invalid shape for points_uvd ({}),'.format(points_uvd.shape) \\\n",
        "                + ' expected (n, 3)'\n",
        "\n",
        "        x = points_uvd.copy()\n",
        "        x[:, 0:2] = (x[:, 0:2] - self.c) / self.f\n",
        "\n",
        "        if self.k is not None:\n",
        "            r2, radial, tan = get_r2_radial_tan(x[:, 0:2], self.k, self.p)\n",
        "            x[:, 0:2] -= np.dot(np.expand_dims(r2, axis=-1), self.p)\n",
        "            x[:, 0:2] /= np.expand_dims(radial + tan, axis=-1)\n",
        "\n",
        "        x[:, 0:2] *= x[:, 2:3]\n",
        "        x = (np.matmul(self.R_inv, x.T) + self.t).T\n",
        "\n",
        "        return x\n",
        "\n",
        "    def serialize(self):\n",
        "        s = np.array(self.R).reshape((9,))\n",
        "        s = np.concatenate([s, np.array(self.t).reshape((3,))])\n",
        "        s = np.concatenate([s, np.array(self.f).reshape((2,))])\n",
        "        s = np.concatenate([s, np.array(self.c).reshape((2,))])\n",
        "        s = np.concatenate([s, np.array(self.p).reshape((2,))])\n",
        "        if self.k is not None:\n",
        "            s = np.concatenate([s, self.k])\n",
        "\n",
        "        return s\n",
        "\n",
        "def get_r2_radial_tan(x, k, p):\n",
        "    \"\"\"Given a set o points x [num_points, 2] in the image coordinate system,\n",
        "    compute the required vectors to apply the distortion coefficients.\n",
        "    \"\"\"\n",
        "    assert x.ndim == 2 and x.shape[1] == 2\n",
        "    assert k.shape == (3,) and p.shape == (1, 2)\n",
        "\n",
        "    r2 = np.power(x[:, 0], 2) + np.power(x[:, 1], 2)\n",
        "    radial = 1. + r2*k[0] + np.power(r2, 2)*k[1] + np.power(r2, 3)*k[2]\n",
        "    tan = np.sum(x * p, axis=-1)\n",
        "\n",
        "    return r2, radial, tan\n",
        "\n",
        "\n",
        "def camera_deserialize(s):\n",
        "    R, s = np.split(s, [9])\n",
        "    t, s = np.split(s, [3])\n",
        "    f, s = np.split(s, [2])\n",
        "    c, s = np.split(s, [2])\n",
        "    p, s = np.split(s, [2])\n",
        "\n",
        "    k = None\n",
        "    if len(s) > 0:\n",
        "        k, s = np.split(s, [3])\n",
        "\n",
        "    return Camera(np.reshape(R, (3, 3)), t, f, c, p, k)\n",
        "\n",
        "\n",
        "def project_pred_to_camera(pred, afmat, resol_z, root_z):\n",
        "    num_samples, num_joints, dim = pred.shape\n",
        "    root_z = np.expand_dims(root_z, axis=-1)\n",
        "\n",
        "    proj = np.zeros(pred.shape)\n",
        "    proj[:,:,0:2] = transform_pose_sequence(afmat, pred[:,:,0:2], inverse=True)\n",
        "    proj[:,:,2] = (resol_z * (pred[:,:,2] - 0.5)) + root_z\n",
        "\n",
        "    return proj\n"
      ],
      "metadata": {
        "id": "wO_SLBThTTIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def most_assigned(x, c):\n",
        "    nb_c = len(c)\n",
        "    assign = np.zeros(nb_c)\n",
        "    for i in range(len(x)):\n",
        "        y = x[i].reshape((1,2))\n",
        "        d = np.sqrt(np.sum(np.power(y.repeat(nb_c, axis=0) - c, 2), axis=1))\n",
        "        assign[d.argmin()] += 1\n",
        "    return assign.argmax()\n",
        "\n",
        "def mean_on_most_assigned(x, c):\n",
        "    nb_c = len(c)\n",
        "    assign = np.zeros(nb_c)\n",
        "    mean = np.zeros(c.shape)\n",
        "    for i in range(len(x)):\n",
        "        y = x[i].reshape((1,2))\n",
        "        d = np.sqrt(np.sum(np.power(y.repeat(nb_c, axis=0) - c, 2), axis=1))\n",
        "        idx = d.argmin()\n",
        "        assign[idx] += 1\n",
        "        mean[idx,:] += x[i]\n",
        "    idx = assign.argmax()\n",
        "    return mean[idx,:] / assign[idx]\n",
        "\n",
        "# def best_kmeans(pred):\n",
        "    # plt.scatter(pred[:,0], pred[:,1], color='b')\n",
        "    # c,v = kmeans(pred, 3)\n",
        "    # plt.scatter(c[:,0], c[:,1], color='g')\n",
        "    # n = most_assigned(pred, c)\n",
        "    # plt.scatter(c[n,0], c[n,1], color='r')\n",
        "    # plt.show()\n",
        "\n",
        "def clustering_joints(y_pred, k=3):\n",
        "    _,nb_spl,nb_joints,dim = y_pred.shape\n",
        "    y = np.zeros((nb_spl, nb_joints, dim))\n",
        "    for s in range(nb_spl):\n",
        "        for j in range(nb_joints):\n",
        "            d = y_pred[:,s,j]\n",
        "            c,v = kmeans(d, k)\n",
        "            n = most_assigned(d, c)\n",
        "            y[s,j,:] = c[n]\n",
        "    return y\n",
        "\n",
        "def clustering_grid(y_pred, size=10):\n",
        "    _, nb_spl, nb_joints, dim = y_pred.shape\n",
        "    assert dim == 2\n",
        "    yp = np.zeros((nb_spl, nb_joints, dim))\n",
        "    for s in range(nb_spl):\n",
        "        for j in range(nb_joints):\n",
        "            d = y_pred[:,s,j,:]\n",
        "            xmin = d[:,0].min()\n",
        "            ymin = d[:,1].min()\n",
        "            xmax = d[:,0].max()\n",
        "            ymax = d[:,1].max()\n",
        "            xstep = (xmax - xmin) / size\n",
        "            ystep = (ymax - ymin) / size\n",
        "            c = np.zeros((size * size, dim))\n",
        "            for x in range(size):\n",
        "                for y in range(size):\n",
        "                    c[x + size*y, 0] = xmin + (x + 0.5) * xstep\n",
        "                    c[x + size*y, 1] = ymin + (y + 0.5) * ystep\n",
        "            yp[s,j,:] = mean_on_most_assigned(d, c)\n",
        "    return yp\n",
        "\n",
        "def mean_joints(y_pred):\n",
        "    _, nb_spl, dim, nb_joints = y_pred.shape\n",
        "    assert dim == 2\n",
        "    yp = np.zeros((nb_spl, dim, nb_joints))\n",
        "    for s in range(nb_spl):\n",
        "        for j in range(nb_joints):\n",
        "            d = y_pred[:,s,:,j]\n",
        "            yp[s, 0, j] = d[:,0].mean()\n",
        "            yp[s, 1, j] = d[:,1].mean()\n",
        "    return yp\n"
      ],
      "metadata": {
        "id": "UZMbdQ7sTia-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnames = {\n",
        "        'aliceblue':            '#F0F8FF',\n",
        "        'antiquewhite':         '#FAEBD7',\n",
        "        'aqua':                 '#00FFFF',\n",
        "        'aquamarine':           '#7FFFD4',\n",
        "        'azure':                '#F0FFFF',\n",
        "        'beige':                '#F5F5DC',\n",
        "        'bisque':               '#FFE4C4',\n",
        "        'black':                '#000000',\n",
        "        'blanchedalmond':       '#FFEBCD',\n",
        "        'blue':                 '#0000FF',\n",
        "        'blueviolet':           '#8A2BE2',\n",
        "        'brown':                '#A52A2A',\n",
        "        'burlywood':            '#DEB887',\n",
        "        'cadetblue':            '#5F9EA0',\n",
        "        'chartreuse':           '#7FFF00',\n",
        "        'chocolate':            '#D2691E',\n",
        "        'coral':                '#FF7F50',\n",
        "        'cornflowerblue':       '#6495ED',\n",
        "        'cornsilk':             '#FFF8DC',\n",
        "        'crimson':              '#DC143C',\n",
        "        'cyan':                 '#00FFFF',\n",
        "        'darkblue':             '#00008B',\n",
        "        'darkcyan':             '#008B8B',\n",
        "        'darkgoldenrod':        '#B8860B',\n",
        "        'darkgray':             '#A9A9A9',\n",
        "        'darkgreen':            '#006400',\n",
        "        'darkkhaki':            '#BDB76B',\n",
        "        'darkmagenta':          '#8B008B',\n",
        "        'darkolivegreen':       '#556B2F',\n",
        "        'darkorange':           '#FF8C00',\n",
        "        'darkorchid':           '#9932CC',\n",
        "        'darkred':              '#8B0000',\n",
        "        'darksalmon':           '#E9967A',\n",
        "        'darkseagreen':         '#8FBC8F',\n",
        "        'darkslateblue':        '#483D8B',\n",
        "        'darkslategray':        '#2F4F4F',\n",
        "        'darkturquoise':        '#00CED1',\n",
        "        'darkviolet':           '#9400D3',\n",
        "        'deeppink':             '#FF1493',\n",
        "        'deepskyblue':          '#00BFFF',\n",
        "        'dimgray':              '#696969',\n",
        "        'dodgerblue':           '#1E90FF',\n",
        "        'firebrick':            '#B22222',\n",
        "        'floralwhite':          '#FFFAF0',\n",
        "        'forestgreen':          '#228B22',\n",
        "        'fuchsia':              '#FF00FF',\n",
        "        'gainsboro':            '#DCDCDC',\n",
        "        'ghostwhite':           '#F8F8FF',\n",
        "        'gold':                 '#FFD700',\n",
        "        'goldenrod':            '#DAA520',\n",
        "        'gray':                 '#808080',\n",
        "        'green':                '#008000',\n",
        "        'greenyellow':          '#ADFF2F',\n",
        "        'honeydew':             '#F0FFF0',\n",
        "        'hotpink':              '#FF69B4',\n",
        "        'indianred':            '#CD5C5C',\n",
        "        'indigo':               '#4B0082',\n",
        "        'ivory':                '#FFFFF0',\n",
        "        'khaki':                '#F0E68C',\n",
        "        'lavender':             '#E6E6FA',\n",
        "        'lavenderblush':        '#FFF0F5',\n",
        "        'lawngreen':            '#7CFC00',\n",
        "        'lemonchiffon':         '#FFFACD',\n",
        "        'lightblue':            '#ADD8E6',\n",
        "        'lightcoral':           '#F08080',\n",
        "        'lightcyan':            '#E0FFFF',\n",
        "        'lightgoldenrodyellow': '#FAFAD2',\n",
        "        'lightgreen':           '#90EE90',\n",
        "        'lightgray':            '#D3D3D3',\n",
        "        'lightpink':            '#FFB6C1',\n",
        "        'lightsalmon':          '#FFA07A',\n",
        "        'lightseagreen':        '#20B2AA',\n",
        "        'lightskyblue':         '#87CEFA',\n",
        "        'lightslategray':       '#778899',\n",
        "        'lightsteelblue':       '#B0C4DE',\n",
        "        'lightyellow':          '#FFFFE0',\n",
        "        'lime':                 '#00FF00',\n",
        "        'limegreen':            '#32CD32',\n",
        "        'linen':                '#FAF0E6',\n",
        "        'magenta':              '#FF00FF',\n",
        "        'maroon':               '#800000',\n",
        "        'mediumaquamarine':     '#66CDAA',\n",
        "        'mediumblue':           '#0000CD',\n",
        "        'mediumorchid':         '#BA55D3',\n",
        "        'mediumpurple':         '#9370DB',\n",
        "        'mediumseagreen':       '#3CB371',\n",
        "        'mediumslateblue':      '#7B68EE',\n",
        "        'mediumspringgreen':    '#00FA9A',\n",
        "        'mediumturquoise':      '#48D1CC',\n",
        "        'mediumvioletred':      '#C71585',\n",
        "        'midnightblue':         '#191970',\n",
        "        'mintcream':            '#F5FFFA',\n",
        "        'mistyrose':            '#FFE4E1',\n",
        "        'moccasin':             '#FFE4B5',\n",
        "        'navajowhite':          '#FFDEAD',\n",
        "        'navy':                 '#000080',\n",
        "        'oldlace':              '#FDF5E6',\n",
        "        'olive':                '#808000',\n",
        "        'olivedrab':            '#6B8E23',\n",
        "        'orange':               '#FFA500',\n",
        "        'orangered':            '#FF4500',\n",
        "        'orchid':               '#DA70D6',\n",
        "        'palegoldenrod':        '#EEE8AA',\n",
        "        'palegreen':            '#98FB98',\n",
        "        'paleturquoise':        '#AFEEEE',\n",
        "        'palevioletred':        '#DB7093',\n",
        "        'papayawhip':           '#FFEFD5',\n",
        "        'peachpuff':            '#FFDAB9',\n",
        "        'peru':                 '#CD853F',\n",
        "        'pink':                 '#FFC0CB',\n",
        "        'plum':                 '#DDA0DD',\n",
        "        'powderblue':           '#B0E0E6',\n",
        "        'purple':               '#800080',\n",
        "        'red':                  '#FF0000',\n",
        "        'rosybrown':            '#BC8F8F',\n",
        "        'royalblue':            '#4169E1',\n",
        "        'saddlebrown':          '#8B4513',\n",
        "        'salmon':               '#FA8072',\n",
        "        'sandybrown':           '#FAA460',\n",
        "        'seagreen':             '#2E8B57',\n",
        "        'seashell':             '#FFF5EE',\n",
        "        'sienna':               '#A0522D',\n",
        "        'silver':               '#C0C0C0',\n",
        "        'skyblue':              '#87CEEB',\n",
        "        'slateblue':            '#6A5ACD',\n",
        "        'slategray':            '#708090',\n",
        "        'snow':                 '#FFFAFA',\n",
        "        'springgreen':          '#00FF7F',\n",
        "        'steelblue':            '#4682B4',\n",
        "        'tan':                  '#D2B48C',\n",
        "        'teal':                 '#008080',\n",
        "        'thistle':              '#D8BFD8',\n",
        "        'tomato':               '#FF6347',\n",
        "        'turquoise':            '#40E0D0',\n",
        "        'violet':               '#EE82EE',\n",
        "        'wheat':                '#F5DEB3',\n",
        "        'white':                '#FFFFFF',\n",
        "        'whitesmoke':           '#F5F5F5',\n",
        "        'yellow':               '#FFFF00',\n",
        "        'yellowgreen':          '#9ACD32'\n",
        "}\n",
        "\n",
        "hex_colors = []\n",
        "for name in cnames:\n",
        "    hex_colors.append(cnames[name])\n",
        "\n",
        "def hexcolor2tuple(s):\n",
        "    return (int(s[1:3], 16)/255., int(s[3:5], 16)/255., int(s[5:7], 16)/255.)\n"
      ],
      "metadata": {
        "id": "ZbHEVf1sTmM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def mkdir(path):\n",
        "    if os.path.isdir(path) is False:\n",
        "        os.mkdir(path)\n"
      ],
      "metadata": {
        "id": "Vgt6yc1UTqz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def normalpdf2d(numbins, xmean, ymean, var):\n",
        "    lin = np.linspace(0, numbins-1, numbins)\n",
        "\n",
        "    # Produce a gaussian in X and in Y\n",
        "    x = multivariate_normal.pdf(lin, mean=xmean, cov=var)\n",
        "    x = x.reshape((1, numbins)).repeat(numbins, axis=0)\n",
        "    y = multivariate_normal.pdf(lin, mean=ymean, cov=var)\n",
        "    y = y.reshape((numbins, 1)).repeat(numbins, axis=1)\n",
        "    g = x * y\n",
        "\n",
        "    if g.sum() > K.epsilon():\n",
        "        return g / g.sum()\n",
        "\n",
        "    return np.zeros(g.shape)\n",
        "def get_visible_joints(x, margin=0.0):\n",
        "\n",
        "    visible = np.apply_along_axis(_func_and, axis=1, arr=(x > margin))\n",
        "    visible *= np.apply_along_axis(_func_and, axis=1, arr=(x < 1 - margin))\n",
        "\n",
        "    return visible\n",
        "\n",
        "class HeatMaps2D():\n",
        "    def __init__(self, poses, numbins, variance=0.3):\n",
        "        assert (poses.shape[-1] == 2) or ((poses.shape[-1] == 3)), \\\n",
        "                'Poses are expected to by 2D or 3D!'\n",
        "        self.poses = poses\n",
        "        if len(poses.shape) == 4:\n",
        "            self.num_frames = poses.shape[1]\n",
        "        else:\n",
        "            self.num_frames = None\n",
        "\n",
        "        self.numbins = numbins\n",
        "        self.variance = variance\n",
        "        self.num_joints = int(poses.shape[-2])\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        p = self.poses[key]\n",
        "        if isinstance(key, int):\n",
        "            return pose_heatmaps(p, self.numbins, self.num_joints,\n",
        "                    variance=self.variance, num_frames=self.num_frames)\n",
        "        if isinstance(key, slice):\n",
        "            indices = key.indices(len(self))\n",
        "            key = range(*indices)\n",
        "        x = np.zeros((len(key),) + self.shape[1:])\n",
        "        for i in range(len(key)):\n",
        "            x[i,:] = pose_heatmaps(p[i], self.numbins, self.num_joints,\n",
        "                    variance=self.variance, num_frames=self.num_frames)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.poses)\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        if self.num_frames is None:\n",
        "            return (len(self),) + (self.numbins, self.numbins, self.num_joints)\n",
        "        else:\n",
        "            return (len(self),) + (self.num_frames,\n",
        "                    self.numbins, self.numbins, self.num_joints)\n",
        "\n",
        "\n",
        "def pose_heatmaps(p, num_bins, num_joints, variance=0.1, num_frames=None):\n",
        "    if num_frames is None:\n",
        "        h = np.zeros((num_bins, num_bins, num_joints))\n",
        "        v = get_visible_joints(p[:, 0:2])\n",
        "        points = num_bins * p[:, 0:2]\n",
        "        for j in range(num_joints):\n",
        "            if v[j]:\n",
        "                h[:,:,j] = normalpdf2d(num_bins,\n",
        "                        points[j,0], points[j,1], variance)\n",
        "    else:\n",
        "        h = np.zeros((num_frames, num_bins, num_bins, num_joints))\n",
        "        for f in range(num_frames):\n",
        "            v = get_visible_joints(p[f][:, 0:2])\n",
        "            points = num_bins * p[f][:, 0:2]\n",
        "            for j in range(num_joints):\n",
        "                if v[j]:\n",
        "                    h[f,:,:,j] = normalpdf2d(num_bins,\n",
        "                            points[j,0], points[j,1], variance)\n",
        "    return h\n"
      ],
      "metadata": {
        "id": "eawe0GCwTwBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def linspace_2d(nb_rols, nb_cols, dim=0):\n",
        "\n",
        "    def _lin_sp_aux(size, nb_repeat, start, end):\n",
        "        linsp = np.linspace(start, end, num=size)\n",
        "        x = np.empty((nb_repeat, size), dtype=np.float32)\n",
        "\n",
        "        for d in range(nb_repeat):\n",
        "            x[d] = linsp\n",
        "\n",
        "        return x\n",
        "\n",
        "    if dim == 1:\n",
        "        return (_lin_sp_aux(nb_rols, nb_cols, 0.0, 1.0)).T\n",
        "    return _lin_sp_aux(nb_cols, nb_rols, 0.0, 1.0)\n"
      ],
      "metadata": {
        "id": "pm2C_7GrTzjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TEST_MODE = 0\n",
        "TRAIN_MODE = 1\n",
        "VALID_MODE = 2\n",
        "\n",
        "\n",
        "class BaseParser(object):\n",
        "\n",
        "    compute_dataset_info = True\n",
        "    avg_num_frames = 0\n",
        "    pose_min = np.array([np.inf, np.inf, np.inf])\n",
        "    pose_max = np.array([-np.inf, -np.inf, -np.inf])\n",
        "\n",
        "    def __init__(self, fid):\n",
        "        self.fid = fid\n",
        "\n",
        "    def getattr(self):\n",
        "        line = self.read_valid_line()\n",
        "        if line is None:\n",
        "            raise EOFError('File consumed!')\n",
        "\n",
        "        return self.get_par_attr(line)\n",
        "\n",
        "    def get_par_attr(self, line):\n",
        "        val = line.split(':')\n",
        "        assert len(val) == 2, 'Wrong \"parameter: attributes,\" content'\n",
        "\n",
        "        attr = val[1].split(',')\n",
        "        if ((attr[-1] == '') or (attr[-1] == '\\n')):\n",
        "            del attr[-1]\n",
        "\n",
        "        for i in range(len(attr)):\n",
        "            attr[i] = attr[i].strip()\n",
        "\n",
        "        return val[0], attr\n",
        "\n",
        "\n",
        "    def read_valid_line(self):\n",
        "        while True:\n",
        "            line = self.fid.readline()\n",
        "            if line:\n",
        "                s = line.decode('utf-8')\n",
        "                if ((s[0] != '\\0') and (s[0] != '\\n') and (s[0] != '#')):\n",
        "                    return s\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "class BaseElement(object):\n",
        "    def __init__(self, parent=None):\n",
        "        self.parent = parent\n",
        "\n",
        "    def copy(self):\n",
        "        return copy.deepcopy(self)\n",
        "\n",
        "    def get_camera(self):\n",
        "        if hasattr(self, 'camera'):\n",
        "            return self.camera\n",
        "        elif hasattr(self, 'parent'):\n",
        "            if self.parent is not None:\n",
        "                return self.parent.get_camera()\n",
        "        return None\n",
        "\n",
        "\n",
        "class Entity(BaseElement):\n",
        "    def __init__(self, parser, dim, num_joints, parent):\n",
        "        BaseElement.__init__(self, parent=parent)\n",
        "        self.pos = np.nan * np.ones((num_joints, dim))\n",
        "        self.vis = np.nan * np.ones((num_joints, 1))\n",
        "        self.mode = -1\n",
        "\n",
        "        while True:\n",
        "            par, attr = parser.getattr()\n",
        "\n",
        "            if par == 'action_id':\n",
        "                self.action_id = int(attr[0])\n",
        "\n",
        "            if par == 'viewpoint_id':\n",
        "                self.viewpoint_id = int(attr[0])\n",
        "\n",
        "            if par == 'scale':\n",
        "                self.scale = float(attr[0])\n",
        "\n",
        "            if par == 'objpos':\n",
        "                assert 2 == len(attr)\n",
        "                self.objpos = np.array([float(attr[0]), float(attr[1])])\n",
        "\n",
        "            if par == 'head':\n",
        "                assert 4 == len(attr)\n",
        "                self.head = np.array([float(attr[0]), float(attr[1]),\n",
        "                                      float(attr[2]), float(attr[3])])\n",
        "\n",
        "            if par == 'x':\n",
        "                assert num_joints == len(attr)\n",
        "                for i in range(num_joints):\n",
        "                    self.pos[i, 0] = float(attr[i])\n",
        "\n",
        "            if par == 'y':\n",
        "                assert num_joints == len(attr)\n",
        "                for i in range(num_joints):\n",
        "                    self.pos[i, 1] = float(attr[i])\n",
        "\n",
        "            if par == 'z':\n",
        "                assert num_joints == len(attr)\n",
        "                for i in range(num_joints):\n",
        "                    self.pos[i, 2] = float(attr[i])\n",
        "\n",
        "            if par == 'v':\n",
        "                assert num_joints == len(attr)\n",
        "                for i in range(num_joints):\n",
        "                    self.vis[i, 0] = float(attr[i])\n",
        "\n",
        "            if par == 'mode':\n",
        "                self.mode = int(attr[0])\n",
        "                break\n",
        "\n",
        "        if BaseParser.compute_dataset_info:\n",
        "            pmin = np.nan * np.ones((3,))\n",
        "            pmax = np.nan * np.ones((3,))\n",
        "            pmin[0:dim] = np.nanmin(self.pos, axis=0)\n",
        "            pmax[0:dim] = np.nanmax(self.pos, axis=0)\n",
        "            BaseParser.pose_min = \\\n",
        "                    np.nanmin(np.array([pmin, BaseParser.pose_min]), axis=0)\n",
        "            BaseParser.pose_max = \\\n",
        "                    np.nanmax(np.array([pmax, BaseParser.pose_max]), axis=0)\n",
        "\n",
        "\n",
        "class ImageFrame(BaseElement):\n",
        "    def __init__(self, parser, dim, num_joints, parent=None):\n",
        "        BaseElement.__init__(self, parent=parent)\n",
        "        self.mode = -1\n",
        "\n",
        "        while True:\n",
        "            par, attr = parser.getattr()\n",
        "\n",
        "            if par == 'image':\n",
        "                self.image = attr[0]\n",
        "\n",
        "            if par == 'res':\n",
        "                self.res = np.array([float(attr[0]), float(attr[1])])\n",
        "\n",
        "            if par == 'num_ent':\n",
        "                self.num_ent = int(attr[0])\n",
        "                self.entities = []\n",
        "                for i in range(self.num_ent):\n",
        "                    self.entities.append(\n",
        "                            Entity(parser, dim, num_joints, parent=self))\n",
        "\n",
        "            if par == 'mode':\n",
        "                self.mode = int(attr[0])\n",
        "                break\n",
        "\n",
        "\n",
        "class SequenceOld(BaseElement):\n",
        "    def __init__(self, parser, dim, num_joints):\n",
        "        BaseElement.__init__(self)\n",
        "        self.mode = -1\n",
        "\n",
        "        while True:\n",
        "            par, attr = parser.getattr()\n",
        "\n",
        "            # if par == 'camera_parameters':\n",
        "                # self.camera = Camera(attr)\n",
        "\n",
        "            if par == 'num_frames':\n",
        "                self.num_frames = int(attr[0])\n",
        "                if BaseParser.compute_dataset_info:\n",
        "                    BaseParser.avg_num_frames += self.num_frames\n",
        "                self.frames = []\n",
        "                for i in range(self.num_frames):\n",
        "                    self.frames.append(ImageFrame(parser, dim, num_joints,\n",
        "                        parent=self))\n",
        "\n",
        "            if par == 'mode':\n",
        "                self.mode = int(attr[0])\n",
        "                break\n",
        "\n",
        "\n",
        "def std_dat_parser(anno_obj, fid):\n",
        "    parser = BaseParser(fid)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            par, attr = parser.getattr()\n",
        "        except Exception as e:\n",
        "            print ('std_dat_parser: ' + str(e))\n",
        "            break\n",
        "\n",
        "        if par == 'action_labels':\n",
        "            anno_obj.action_labels = attr\n",
        "\n",
        "        if par == 'joint_labels':\n",
        "            anno_obj.joint_labels = attr\n",
        "\n",
        "        if par == 'viewpoint_labels':\n",
        "            anno_obj.viewpoint_labels = attr\n",
        "\n",
        "        if par == 'num_joints':\n",
        "            anno_obj.num_joints = int(attr[0])\n",
        "\n",
        "        if par == 'dim':\n",
        "            anno_obj.dim = int(attr[0])\n",
        "\n",
        "        if par == 'num_sequences':\n",
        "            anno_obj.num_sequences = int(attr[0])\n",
        "            anno_obj.sequences = []\n",
        "            for i in range(anno_obj.num_sequences):\n",
        "                anno_obj.sequences.append(\n",
        "                        SequenceOld(parser, anno_obj.dim, anno_obj.num_joints))\n",
        "            BaseParser.avg_num_frames /= len(anno_obj.sequences)\n",
        "\n",
        "class Annotation(object):\n",
        "    def __init__(self, dataset_path=None, custom_parser=None):\n",
        "        self.sequences = []\n",
        "        if custom_parser is None:\n",
        "            assert dataset_path, \\\n",
        "                    \"If a custom parser is not given, dataset_path is required\"\n",
        "        try:\n",
        "            if custom_parser is not None:\n",
        "                self.action_labels, \\\n",
        "                        self.joint_labels, \\\n",
        "                        self.viewpoint_labels,\\\n",
        "                        self.sequences = custom_parser(dataset_path)\n",
        "            else:\n",
        "                # Standard parser\n",
        "                filename = '%s/annotations.dat.gz' % dataset_path\n",
        "                fid = gzip.open(filename, 'r')\n",
        "                gz_header = fid.readline()\n",
        "                std_dat_parser(self, fid)\n",
        "                fid.close()\n",
        "\n",
        "            if BaseParser.compute_dataset_info:\n",
        "                printcn(HEADER, '## Info on dataset \"%s\" ##' % dataset_path)\n",
        "                printcn(OKBLUE, '  Average number of frames: %.0f' % \\\n",
        "                        BaseParser.avg_num_frames)\n",
        "                printcn(OKBLUE, '  Min pose values on X-Y-Z: {}'.format(\n",
        "                    BaseParser.pose_min))\n",
        "                printcn(OKBLUE, '  Max pose values on X-Y-Z: {}'.format(\n",
        "                    BaseParser.pose_max))\n",
        "\n",
        "        except Exception as e:\n",
        "            print ('Catch exception in Annotation class: ' + str(e))\n",
        "\n",
        "\n",
        "def appstr(s, a):\n",
        "    \"\"\"Safe appending strings.\"\"\"\n",
        "    try:\n",
        "        return s + a\n",
        "    except:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "9fkLKjFTT6ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class _pa16j():\n",
        "    \"\"\"Pose alternated with 16 joints (like Penn Action with three more\n",
        "    joints on the spine.\n",
        "    \"\"\"\n",
        "    num_joints = 16\n",
        "    joint_names = ['pelvis', 'thorax', 'neck', 'head',\n",
        "            'r_shoul', 'l_shoul', 'r_elb', 'l_elb', 'r_wrist', 'l_wrist',\n",
        "            'r_hip', 'l_hip', 'r_knww', 'l_knee', 'r_ankle', 'l_ankle']\n",
        "\n",
        "    \"\"\"Horizontal flip mapping\"\"\"\n",
        "    map_hflip = [0, 1, 2, 3, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14]\n",
        "\n",
        "    \"\"\"Projections from other layouts to the PA16J standard\"\"\"\n",
        "    map_from_mpii = [6, 7, 8, 9, 12, 13, 11, 14, 10, 15, 2, 3, 1, 4, 0, 5]\n",
        "    map_from_ntu = [0, 20, 2, 3, 4, 8, 5, 9, 6, 10, 12, 16, 13, 17, 14, 18]\n",
        "\n",
        "    \"\"\"Projections of PA16J to other formats\"\"\"\n",
        "    map_to_pa13j = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
        "    map_to_jhmdb = [2, 1, 3, 4, 5, 10, 11, 6, 7, 12, 13, 8, 9, 14, 15]\n",
        "    map_to_mpii = [14, 12, 10, 11, 13, 15, 0, 1, 2, 3, 8, 6, 4, 5, 7, 9]\n",
        "    map_to_lsp = [14, 12, 10, 11, 13, 15, 8, 6, 4, 5, 7, 9, 2, 3]\n",
        "\n",
        "    \"\"\"Color map\"\"\"\n",
        "    color = ['g', 'r', 'b', 'y', 'm']\n",
        "    cmap = [0, 0, 0, 0, 1, 2, 1, 2, 1, 2, 3, 4, 3, 4, 3, 4]\n",
        "    links = [[0, 1], [1, 2], [2, 3], [4, 6], [6, 8], [5, 7], [7, 9],\n",
        "            [10, 12], [12, 14], [11, 13], [13, 15]]\n",
        "\n",
        "class _pa17j():\n",
        "    \"\"\"Pose alternated with 17 joints (like _pa16j, with the middle spine).\n",
        "    \"\"\"\n",
        "    num_joints = 17\n",
        "\n",
        "    \"\"\"Horizontal flip mapping\"\"\"\n",
        "    map_hflip = [0, 1, 2, 3, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 16]\n",
        "\n",
        "    \"\"\"Projections from other layouts to the PA17J standard\"\"\"\n",
        "    map_from_h36m = \\\n",
        "            [0, 12, 13, 15, 25, 17, 26, 18, 27, 19, 1, 6, 2, 7, 3, 8, 11]\n",
        "    map_from_ntu = _pa16j.map_from_ntu + [1]\n",
        "    map_from_mpii3dhp = \\\n",
        "            [4, 5, 6, 7, 14, 9, 15, 10, 16, 11, 23, 18, 24, 19, 25, 20, 3]\n",
        "    map_from_mpii3dhp_te = \\\n",
        "            [14, 1, 16, 0, 2, 5, 3, 6, 4, 7, 8, 11, 9, 12, 10, 13, 15]\n",
        "\n",
        "    \"\"\"Projections of PA17J to other formats\"\"\"\n",
        "    map_to_pa13j = _pa16j.map_to_pa13j\n",
        "    map_to_mpii = [14, 12, 10, 11, 13, 15, 0, 1, 2, 3, 8, 6, 4, 5, 7, 9]\n",
        "    map_to_pa16j = list(range(16))\n",
        "\n",
        "    \"\"\"Color map\"\"\"\n",
        "    color = ['g', 'r', 'b', 'y', 'm']\n",
        "    cmap = [0, 0, 0, 0, 1, 2, 1, 2, 1, 2, 3, 4, 3, 4, 3, 4, 0]\n",
        "    links = [[0, 16], [16, 1], [1, 2], [2, 3], [4, 6], [6, 8], [5, 7], [7, 9],\n",
        "            [10, 12], [12, 14], [11, 13], [13, 15]]\n",
        "\n",
        "class _pa20j():\n",
        "    \"\"\"Pose alternated with 20 joints. Similar to _pa16j, but with one more\n",
        "    joint for hands and feet.\n",
        "    \"\"\"\n",
        "    num_joints = 20\n",
        "\n",
        "    \"\"\"Horizontal flip mapping\"\"\"\n",
        "    map_hflip = [0, 1, 2, 3, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17, 16,\n",
        "            19, 18]\n",
        "\n",
        "    \"\"\"Projections from other layouts to the PA20J standard\"\"\"\n",
        "    map_from_h36m = [0, 12, 13, 15, 25, 17, 26, 18, 27, 19, 30, 22, 1, 6, 2,\n",
        "            7, 3, 8, 4, 9]\n",
        "    map_from_ntu = [0, 20, 2, 3, 4, 8, 5, 9, 6, 10, 7, 11, 12, 16, 13, 17, 14,\n",
        "            18, 15, 19]\n",
        "\n",
        "    \"\"\"Projections of PA20J to other formats\"\"\"\n",
        "    map_to_mpii = [16, 14, 12, 13, 15, 17, 0, 1, 2, 3, 8, 6, 4, 5, 7, 9]\n",
        "    map_to_pa13j = [3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17]\n",
        "    map_to_pa16j = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17]\n",
        "\n",
        "    \"\"\"Color map\"\"\"\n",
        "    color = ['g', 'r', 'b', 'y', 'm']\n",
        "    cmap = [0, 0, 0, 0, 1, 2, 1, 2, 1, 2, 1, 2, 3, 4, 3, 4, 3, 4, 3, 4]\n",
        "    links = [[0, 1], [1, 2], [2, 3], [4, 6], [6, 8], [8, 10], [5, 7], [7, 9],\n",
        "            [9, 11], [12, 14], [14, 16], [16, 18], [13, 15], [15, 17], [17, 19]]\n",
        "\n",
        "class _pa21j():\n",
        "    \"\"\"Pose alternated with 21 joints. Similar to _pa20j, but with one more\n",
        "    joint referent to the 16th joint from _pa17j, for compatibility with H36M.\n",
        "    \"\"\"\n",
        "    num_joints = 21\n",
        "\n",
        "    \"\"\"Horizontal flip mapping\"\"\"\n",
        "    map_hflip = _pa20j.map_hflip + [20]\n",
        "\n",
        "    \"\"\"Projections from other layouts to the PA21J standard\"\"\"\n",
        "    map_from_h36m = _pa20j.map_from_h36m + [11]\n",
        "    map_from_ntu = _pa20j.map_from_ntu + [1]\n",
        "\n",
        "    \"\"\"Projections of PA20J to other formats\"\"\"\n",
        "    map_to_mpii = _pa20j.map_to_mpii\n",
        "    map_to_pa13j = _pa20j.map_to_pa13j\n",
        "    map_to_pa16j = _pa20j.map_to_pa16j\n",
        "    map_to_pa17j = _pa20j.map_to_pa16j + [20]\n",
        "\n",
        "    \"\"\"Color map\"\"\"\n",
        "    color = ['g', 'r', 'b', 'y', 'm']\n",
        "    cmap = [0, 0, 0, 0, 1, 2, 1, 2, 1, 2, 1, 2, 3, 4, 3, 4, 3, 4, 3, 4, 0]\n",
        "    links = [[0, 20], [20, 1], [1, 2], [2, 3], [4, 6], [6, 8], [8, 10], [5, 7], [7, 9],\n",
        "            [9, 11], [12, 14], [14, 16], [16, 18], [13, 15], [15, 17], [17, 19]]\n",
        "\n",
        "class coco17j():\n",
        "    \"\"\"Original layout for the MS COCO dataset.\"\"\"\n",
        "    num_joints = 17\n",
        "    dim = 2\n",
        "\n",
        "    \"\"\"Horizontal flip mapping\"\"\"\n",
        "    map_hflip = [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]\n",
        "\n",
        "    \"\"\"Color map\"\"\"\n",
        "    color = ['g', 'r', 'b', 'y', 'm', 'w']\n",
        "    cmap = [0, 0, 0, 5, 5, 0, 0, 2, 1, 2, 1, 0, 0, 4, 3, 4, 3]\n",
        "    links = [[13, 15], [13, 11], [14, 16], [14, 12], [11, 12], [5, 11], [6,\n",
        "        12], [5, 6], [7, 5], [8, 6], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n",
        "        [3, 1], [4, 2], [3, 5], [4, 6]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _func_and(x):\n",
        "    if x.all():\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def convert_pa17j3d_to_pa16j(p, dim=3):\n",
        "    assert p.shape == (pa17j3d.num_joints, pa17j3d.dim)\n",
        "    return p[pa17j3d.map_to_pa16j,0:dim].copy()\n",
        "\n",
        "def convert_sequence_pa17j3d_to_pa16j(seqp, dim=3):\n",
        "    assert seqp.shape[1:] == (pa17j3d.num_joints, pa17j3d.dim)\n",
        "    x = np.zeros((len(seqp), _pa16j.num_joints, dim))\n",
        "    for i in range(len(seqp)):\n",
        "        x[i,:] = convert_pa17j3d_to_pa16j(seqp[i], dim=dim)\n",
        "    return x\n",
        "\n",
        "def write_poselist(filename, poses):\n",
        "    \"\"\" Write a pose list to a text file.\n",
        "    In the text file, every row corresponds to one pose and the columns are:\n",
        "    {x1, y1, x2, y2, ...}\n",
        "\n",
        "        Inputs: 'filename'\n",
        "                'poses' [nb_samples, nb_joints, 2]\n",
        "    \"\"\"\n",
        "    nb_samples, nb_joints, dim = poses.shape\n",
        "    x = poses.copy()\n",
        "    x = np.reshape(x, (nb_samples, nb_joints * dim))\n",
        "    np.savetxt(filename, x, fmt='%.6f', delimiter=',')\n",
        "\n",
        "def assign_knn_confidence(c, num_iter=2):\n",
        "    assert c.ndim == 2 and c.shape[1] == 1, \\\n",
        "            'Invalid confidence shape {}'.format(c.shape)\n",
        "\n",
        "    def _search_knn(refp):\n",
        "        cs = c[list(refp), 0]\n",
        "        if np.isnan(cs).all():\n",
        "            return np.nan\n",
        "        if np.nanmean(cs) < 0.5:\n",
        "            return 0.1\n",
        "        return 0.9\n",
        "\n",
        "    for _ in range(num_iter):\n",
        "        for i in range(len(c)):\n",
        "            if np.isnan(c[i, 0]):\n",
        "                c[i, 0] = _search_knn(dsl80j3d.neighbors[i])\n",
        "\n",
        "class pa16j2d(_pa16j):\n",
        "    dim = 2\n",
        "\n",
        "class pa16j3d(_pa16j):\n",
        "    dim = 3\n",
        "\n",
        "class pa17j2d(_pa17j):\n",
        "    dim = 2\n",
        "\n",
        "class pa17j3d(_pa17j):\n",
        "    dim = 3\n",
        "\n",
        "class pa20j3d(_pa20j):\n",
        "    dim = 3\n",
        "\n",
        "class pa21j3d(_pa21j):\n",
        "    dim = 3\n",
        "\n",
        "class ntu25j3d():\n",
        "    num_joints = 25\n",
        "    dim = 3\n",
        "\n",
        "try:\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "    import matplotlib.pyplot as plt\n",
        "except Exception as e:\n",
        "    printcn(FAIL, str(e))\n",
        "    plt = None\n",
        "\n",
        "\n",
        "def data_to_image(x, gray_scale=False):\n",
        "    \"\"\" Convert 'x' to a RGB Image object.\n",
        "\n",
        "    # Arguments\n",
        "        x: image in the format (num_cols, num_rows, 3) for RGB images or\n",
        "            (num_cols, num_rows) for gray scale images. If None, return a\n",
        "            light gray image with size 100x100.\n",
        "        gray_scale: convert the RGB color space to a RGB gray scale space.\n",
        "    \"\"\"\n",
        "\n",
        "    if x is None:\n",
        "        x = 224 * np.ones((100, 100, 3), dtype=np.uint8)\n",
        "\n",
        "    if x.max() - x.min() > 0.:\n",
        "        buf = 255. * (x - x.min()) / (x.max() - x.min())\n",
        "    else:\n",
        "        buf = x.copy()\n",
        "\n",
        "    if len(buf.shape) == 3:\n",
        "        (w, h) = buf.shape[0:2]\n",
        "        num_ch = buf.shape[2]\n",
        "    else:\n",
        "        (h, w) = buf.shape\n",
        "        num_ch = 1\n",
        "\n",
        "    if ((num_ch is 3) and gray_scale):\n",
        "        g = 0.2989*buf[:,:,0] + 0.5870*buf[:,:,1] + 0.1140*buf[:,:,2]\n",
        "        buf[:,:,0] = g\n",
        "        buf[:,:,1] = g\n",
        "        buf[:,:,2] = g\n",
        "    elif num_ch is 1:\n",
        "        aux = np.zeros((h, w, 3), dtype=buf.dtype)\n",
        "        aux[:,:,0] = buf\n",
        "        aux[:,:,1] = buf\n",
        "        aux[:,:,2] = buf\n",
        "        buf = aux\n",
        "\n",
        "    return Image.fromarray(buf.astype(np.uint8), 'RGB')\n",
        "\n",
        "\n",
        "def show(x, gray_scale=False, jet_cmap=False, filename=None):\n",
        "    \"\"\" Show 'x' as an image on the screen.\n",
        "    \"\"\"\n",
        "    if jet_cmap is False:\n",
        "        img = data_to_image(x, gray_scale=gray_scale)\n",
        "    else:\n",
        "        if plt is None:\n",
        "            printcn(WARNING, 'pyplot not defined!')\n",
        "            return\n",
        "        cmap = plt.cm.jet\n",
        "        norm = plt.Normalize(vmin=x.min(), vmax=x.max())\n",
        "        img = cmap(norm(x))\n",
        "    if filename:\n",
        "        plt.imsave(filename, img)\n",
        "    else:\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def draw(x=None,\n",
        "        skels=[],\n",
        "        bboxes=[],\n",
        "        bbox_color='g',\n",
        "        abs_pos=False,\n",
        "        plot3d=False,\n",
        "        single_window=False,\n",
        "        figsize=(16,9),\n",
        "        axis='on',\n",
        "        facecolor='white',\n",
        "        azimuth=65,\n",
        "        dpi=100,\n",
        "        filename=None):\n",
        "\n",
        "    # Configure the ploting environment\n",
        "    if plt is None:\n",
        "        printcn(WARNING, 'pyplot not defined!')\n",
        "        return\n",
        "\n",
        "    \"\"\" Plot 'x' and draw over it the skeletons and the bounding boxes.\n",
        "    \"\"\"\n",
        "    img = data_to_image(x)\n",
        "    if abs_pos:\n",
        "        w = None\n",
        "        h = None\n",
        "    else:\n",
        "        w,h = img.size\n",
        "\n",
        "    def add_subimage(f, subplot, img):\n",
        "        ax = f.add_subplot(subplot)\n",
        "        plt.imshow(img, zorder=-1)\n",
        "        return ax\n",
        "\n",
        "    fig = [plt.figure(figsize=figsize)]\n",
        "    ax = []\n",
        "\n",
        "    if plot3d:\n",
        "        if single_window:\n",
        "            ax.append(add_subimage(fig[0], 121, img))\n",
        "            ax.append(fig[0].add_subplot(122, projection='3d'))\n",
        "        else:\n",
        "            ax.append(add_subimage(fig[0], 111, img))\n",
        "            fig.append(plt.figure(figsize=figsize))\n",
        "            ax.append(fig[1].add_subplot(111, projection='3d'))\n",
        "    else:\n",
        "        ax.append(add_subimage(fig[0], 111, img))\n",
        "\n",
        "    plt.axis(axis)\n",
        "\n",
        "    # Plotting skeletons if not None\n",
        "    if skels is not None:\n",
        "        if isinstance(skels, list) or len(skels.shape) == 3:\n",
        "            for s in skels:\n",
        "                plot_skeleton_2d(ax[0], s, h=h, w=w)\n",
        "            if plot3d:\n",
        "                plot_3d_pose(s, subplot=ax[-1], azimuth=azimuth)\n",
        "        else:\n",
        "            plot_skeleton_2d(ax[0], skels, h=h, w=w)\n",
        "            if plot3d:\n",
        "                plot_3d_pose(skels, subplot=ax[-1], azimuth=azimuth)\n",
        "\n",
        "    # Plotting bounding boxes if not None\n",
        "    if bboxes is not None:\n",
        "        if isinstance(bboxes, list) or len(bboxes.shape) == 3:\n",
        "            for b, c in zip(bboxes, bbox_color):\n",
        "                _plot_bbox(ax[0], b, h=h, w=w, c=c, lw=4)\n",
        "        else:\n",
        "            _plot_bbox(ax[0], bboxes, h=h, w=w, c=bbox_color, lw=4)\n",
        "\n",
        "\n",
        "    if filename:\n",
        "        fig[0].savefig(filename, bbox_inches='tight', pad_inches=0,\n",
        "                facecolor=facecolor, dpi=dpi)\n",
        "        if plot3d and (single_window is False):\n",
        "            fig[-1].savefig(filename + '.eps',\n",
        "                    bbox_inches='tight', pad_inches=0)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    for i in range(len(fig)):\n",
        "        plt.close(fig[i])\n",
        "\n",
        "\n",
        "def _get_poselayout(num_joints):\n",
        "    if num_joints == 16:\n",
        "        return pa16j2d.color, pa16j2d.cmap, pa16j2d.links\n",
        "    elif num_joints == 17:\n",
        "        return pa17j3d.color, pa17j3d.cmap, pa17j3d.links\n",
        "    elif num_joints == 20:\n",
        "        return pa20j3d.color, pa20j3d.cmap, pa20j3d.links\n",
        "\n",
        "\n",
        "def plot_3d_pose(pose, subplot=None, filename=None, color=None, lw=3,\n",
        "        azimuth=65):\n",
        "\n",
        "    if plt is None:\n",
        "        raise Exception('\"matplotlib\" is required for 3D pose plotting!')\n",
        "\n",
        "    num_joints, dim = pose.shape\n",
        "    assert dim in [2, 3], 'Invalid pose dimension (%d)' % dim\n",
        "    assert ((num_joints == 16) or (num_joints == 17)) or (num_joints == 20), \\\n",
        "            'Unsupported number of joints (%d)' % num_joints\n",
        "\n",
        "    col, cmap, links = _get_poselayout(num_joints)\n",
        "    if color is None:\n",
        "        color = col\n",
        "\n",
        "    def _func_and(x):\n",
        "        if x.all():\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    points = np.zeros((num_joints, 3))\n",
        "    for d in range(dim):\n",
        "        points[:,d] = pose[:,d]\n",
        "    for i in range(num_joints):\n",
        "        points[i, 2] = max(0, points[i, 2])\n",
        "\n",
        "    valid = np.apply_along_axis(_func_and, axis=1, arr=(points[:,0:2] > -1e6))\n",
        "\n",
        "    if subplot is None:\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "    else:\n",
        "        fig = None\n",
        "        ax = subplot\n",
        "\n",
        "    for j in range(num_joints):\n",
        "        if valid[j]:\n",
        "            x, y, z = points[j]\n",
        "            ax.scatter([z], [x], [y], lw=lw, c=color[cmap[j]])\n",
        "\n",
        "    for i in links:\n",
        "        if valid[i[0]] and valid[i[1]]:\n",
        "            c = color[cmap[i[0]]]\n",
        "            ax.plot(points[i, 2], points[i, 0], points[i, 1], c=c, lw=lw)\n",
        "\n",
        "    ax.view_init(10, azimuth)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlabel('Z (depth)')\n",
        "    ax.set_ylabel('X (width)')\n",
        "    ax.set_zlabel('Y (height)')\n",
        "    ax.set_xlim([0, 1.])\n",
        "    ax.set_ylim([0, 1.])\n",
        "    ax.set_zlim([0, 1.])\n",
        "    plt.gca().invert_xaxis()\n",
        "    plt.gca().invert_zaxis()\n",
        "\n",
        "    if fig is not None:\n",
        "        if filename:\n",
        "            fig.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
        "        else:\n",
        "            plt.show()\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "def _plot_bbox(subplot, bbox, h=None, w=None, scale=16, lw=2, c=None):\n",
        "    assert len(bbox) == 4\n",
        "\n",
        "    b = bbox.copy()\n",
        "    if w is not None:\n",
        "       b[0] *= w\n",
        "       b[2] *= w\n",
        "    if h is not None:\n",
        "       b[1] *= h\n",
        "       b[3] *= h\n",
        "\n",
        "    if c is None:\n",
        "        c = hex_colors[np.random.randint(len(hex_colors))]\n",
        "\n",
        "    x = np.array([b[0], b[2], b[2], b[0], b[0]])\n",
        "    y = np.array([b[1], b[1], b[3], b[3], b[1]])\n",
        "    subplot.plot(x, y, lw=lw, c=c, zorder=1)\n",
        "\n",
        "\n",
        "def plot_skeleton_2d(subplot, skel, h=None, w=None,\n",
        "        joints=True, links=True, scale=16, lw=4):\n",
        "\n",
        "    s = skel.copy()\n",
        "    num_joints = len(s)\n",
        "    assert ((num_joints == 16) or (num_joints == 17)) or (num_joints == 20), \\\n",
        "            'Unsupported number of joints (%d)' % num_joints\n",
        "\n",
        "    color, cmap, links = _get_poselayout(num_joints)\n",
        "\n",
        "    x = s[:,0]\n",
        "    y = s[:,1]\n",
        "    v = s > -1e6\n",
        "    v = v.any(axis=1).astype(np.float32)\n",
        "\n",
        "    # Convert normalized skeletons to image coordinates.\n",
        "    if w is not None:\n",
        "        x *= w\n",
        "    if h is not None:\n",
        "        y *= h\n",
        "\n",
        "    if joints:\n",
        "        for i in range(len(v)):\n",
        "            if v[i] > 0:\n",
        "                c = color[cmap[i]]\n",
        "                subplot.scatter(x=x[i], y=y[i], c=c, lw=lw, s=scale, zorder=2)\n",
        "\n",
        "    if links:\n",
        "        for i in links:\n",
        "            if ((v[i[0]] > 0) and (v[i[1]] > 0)):\n",
        "                c = color[cmap[i[0]]]\n",
        "                subplot.plot(x[i], y[i], lw=lw, c=c, zorder=1)"
      ],
      "metadata": {
        "id": "tWGjNn93UApp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "    import matplotlib.pyplot as plt\n",
        "except Exception as e:\n",
        "    printcn(FAIL, str(e))\n",
        "    plt = None\n",
        "\n",
        "\n",
        "def data_to_image(x, gray_scale=False):\n",
        "    \"\"\" Convert 'x' to a RGB Image object.\n",
        "\n",
        "    # Arguments\n",
        "        x: image in the format (num_cols, num_rows, 3) for RGB images or\n",
        "            (num_cols, num_rows) for gray scale images. If None, return a\n",
        "            light gray image with size 100x100.\n",
        "        gray_scale: convert the RGB color space to a RGB gray scale space.\n",
        "    \"\"\"\n",
        "\n",
        "    if x is None:\n",
        "        x = 224 * np.ones((100, 100, 3), dtype=np.uint8)\n",
        "\n",
        "    if x.max() - x.min() > 0.:\n",
        "        buf = 255. * (x - x.min()) / (x.max() - x.min())\n",
        "    else:\n",
        "        buf = x.copy()\n",
        "\n",
        "    if len(buf.shape) == 3:\n",
        "        (w, h) = buf.shape[0:2]\n",
        "        num_ch = buf.shape[2]\n",
        "    else:\n",
        "        (h, w) = buf.shape\n",
        "        num_ch = 1\n",
        "\n",
        "    if ((num_ch is 3) and gray_scale):\n",
        "        g = 0.2989*buf[:,:,0] + 0.5870*buf[:,:,1] + 0.1140*buf[:,:,2]\n",
        "        buf[:,:,0] = g\n",
        "        buf[:,:,1] = g\n",
        "        buf[:,:,2] = g\n",
        "    elif num_ch is 1:\n",
        "        aux = np.zeros((h, w, 3), dtype=buf.dtype)\n",
        "        aux[:,:,0] = buf\n",
        "        aux[:,:,1] = buf\n",
        "        aux[:,:,2] = buf\n",
        "        buf = aux\n",
        "\n",
        "    return Image.fromarray(buf.astype(np.uint8), 'RGB')\n",
        "\n",
        "\n",
        "def show(x, gray_scale=False, jet_cmap=False, filename=None):\n",
        "    \"\"\" Show 'x' as an image on the screen.\n",
        "    \"\"\"\n",
        "    if jet_cmap is False:\n",
        "        img = data_to_image(x, gray_scale=gray_scale)\n",
        "    else:\n",
        "        if plt is None:\n",
        "            printcn(WARNING, 'pyplot not defined!')\n",
        "            return\n",
        "        cmap = plt.cm.jet\n",
        "        norm = plt.Normalize(vmin=x.min(), vmax=x.max())\n",
        "        img = cmap(norm(x))\n",
        "    if filename:\n",
        "        plt.imsave(filename, img)\n",
        "    else:\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def draw(x=None,\n",
        "        skels=[],\n",
        "        bboxes=[],\n",
        "        bbox_color='g',\n",
        "        abs_pos=False,\n",
        "        plot3d=False,\n",
        "        single_window=False,\n",
        "        figsize=(16,9),\n",
        "        axis='on',\n",
        "        facecolor='white',\n",
        "        azimuth=65,\n",
        "        dpi=100,\n",
        "        filename=None):\n",
        "\n",
        "    # Configure the ploting environment\n",
        "    if plt is None:\n",
        "        printcn(WARNING, 'pyplot not defined!')\n",
        "        return\n",
        "\n",
        "    \"\"\" Plot 'x' and draw over it the skeletons and the bounding boxes.\n",
        "    \"\"\"\n",
        "    img = data_to_image(x)\n",
        "    if abs_pos:\n",
        "        w = None\n",
        "        h = None\n",
        "    else:\n",
        "        w,h = img.size\n",
        "\n",
        "    def add_subimage(f, subplot, img):\n",
        "        ax = f.add_subplot(subplot)\n",
        "        plt.imshow(img, zorder=-1)\n",
        "        return ax\n",
        "\n",
        "    fig = [plt.figure(figsize=figsize)]\n",
        "    ax = []\n",
        "\n",
        "    if plot3d:\n",
        "        if single_window:\n",
        "            ax.append(add_subimage(fig[0], 121, img))\n",
        "            ax.append(fig[0].add_subplot(122, projection='3d'))\n",
        "        else:\n",
        "            ax.append(add_subimage(fig[0], 111, img))\n",
        "            fig.append(plt.figure(figsize=figsize))\n",
        "            ax.append(fig[1].add_subplot(111, projection='3d'))\n",
        "    else:\n",
        "        ax.append(add_subimage(fig[0], 111, img))\n",
        "\n",
        "    plt.axis(axis)\n",
        "\n",
        "    # Plotting skeletons if not None\n",
        "    if skels is not None:\n",
        "        if isinstance(skels, list) or len(skels.shape) == 3:\n",
        "            for s in skels:\n",
        "                plot_skeleton_2d(ax[0], s, h=h, w=w)\n",
        "            if plot3d:\n",
        "                plot_3d_pose(s, subplot=ax[-1], azimuth=azimuth)\n",
        "        else:\n",
        "            plot_skeleton_2d(ax[0], skels, h=h, w=w)\n",
        "            if plot3d:\n",
        "                plot_3d_pose(skels, subplot=ax[-1], azimuth=azimuth)\n",
        "\n",
        "    # Plotting bounding boxes if not None\n",
        "    if bboxes is not None:\n",
        "        if isinstance(bboxes, list) or len(bboxes.shape) == 3:\n",
        "            for b, c in zip(bboxes, bbox_color):\n",
        "                _plot_bbox(ax[0], b, h=h, w=w, c=c, lw=4)\n",
        "        else:\n",
        "            _plot_bbox(ax[0], bboxes, h=h, w=w, c=bbox_color, lw=4)\n",
        "\n",
        "\n",
        "    if filename:\n",
        "        fig[0].savefig(filename, bbox_inches='tight', pad_inches=0,\n",
        "                facecolor=facecolor, dpi=dpi)\n",
        "        if plot3d and (single_window is False):\n",
        "            fig[-1].savefig(filename + '.eps',\n",
        "                    bbox_inches='tight', pad_inches=0)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "    for i in range(len(fig)):\n",
        "        plt.close(fig[i])\n",
        "\n",
        "\n",
        "def _get_poselayout(num_joints):\n",
        "    if num_joints == 16:\n",
        "        return pa16j2d.color, pa16j2d.cmap, pa16j2d.links\n",
        "    elif num_joints == 17:\n",
        "        return pa17j3d.color, pa17j3d.cmap, pa17j3d.links\n",
        "    elif num_joints == 20:\n",
        "        return pa20j3d.color, pa20j3d.cmap, pa20j3d.links\n",
        "\n",
        "\n",
        "def plot_3d_pose(pose, subplot=None, filename=None, color=None, lw=3,\n",
        "        azimuth=65):\n",
        "\n",
        "    if plt is None:\n",
        "        raise Exception('\"matplotlib\" is required for 3D pose plotting!')\n",
        "\n",
        "    num_joints, dim = pose.shape\n",
        "    assert dim in [2, 3], 'Invalid pose dimension (%d)' % dim\n",
        "    assert ((num_joints == 16) or (num_joints == 17)) or (num_joints == 20), \\\n",
        "            'Unsupported number of joints (%d)' % num_joints\n",
        "\n",
        "    col, cmap, links = _get_poselayout(num_joints)\n",
        "    if color is None:\n",
        "        color = col\n",
        "\n",
        "    def _func_and(x):\n",
        "        if x.all():\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    points = np.zeros((num_joints, 3))\n",
        "    for d in range(dim):\n",
        "        points[:,d] = pose[:,d]\n",
        "    for i in range(num_joints):\n",
        "        points[i, 2] = max(0, points[i, 2])\n",
        "\n",
        "    valid = np.apply_along_axis(_func_and, axis=1, arr=(points[:,0:2] > -1e6))\n",
        "\n",
        "    if subplot is None:\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "    else:\n",
        "        fig = None\n",
        "        ax = subplot\n",
        "\n",
        "    for j in range(num_joints):\n",
        "        if valid[j]:\n",
        "            x, y, z = points[j]\n",
        "            ax.scatter([z], [x], [y], lw=lw, c=color[cmap[j]])\n",
        "\n",
        "    for i in links:\n",
        "        if valid[i[0]] and valid[i[1]]:\n",
        "            c = color[cmap[i[0]]]\n",
        "            ax.plot(points[i, 2], points[i, 0], points[i, 1], c=c, lw=lw)\n",
        "\n",
        "    ax.view_init(10, azimuth)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_xlabel('Z (depth)')\n",
        "    ax.set_ylabel('X (width)')\n",
        "    ax.set_zlabel('Y (height)')\n",
        "    ax.set_xlim([0, 1.])\n",
        "    ax.set_ylim([0, 1.])\n",
        "    ax.set_zlim([0, 1.])\n",
        "    plt.gca().invert_xaxis()\n",
        "    plt.gca().invert_zaxis()\n",
        "\n",
        "    if fig is not None:\n",
        "        if filename:\n",
        "            fig.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
        "        else:\n",
        "            plt.show()\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "def _plot_bbox(subplot, bbox, h=None, w=None, scale=16, lw=2, c=None):\n",
        "    assert len(bbox) == 4\n",
        "\n",
        "    b = bbox.copy()\n",
        "    if w is not None:\n",
        "       b[0] *= w\n",
        "       b[2] *= w\n",
        "    if h is not None:\n",
        "       b[1] *= h\n",
        "       b[3] *= h\n",
        "\n",
        "    if c is None:\n",
        "        c = hex_colors[np.random.randint(len(hex_colors))]\n",
        "\n",
        "    x = np.array([b[0], b[2], b[2], b[0], b[0]])\n",
        "    y = np.array([b[1], b[1], b[3], b[3], b[1]])\n",
        "    subplot.plot(x, y, lw=lw, c=c, zorder=1)\n",
        "\n",
        "\n",
        "def plot_skeleton_2d(subplot, skel, h=None, w=None,\n",
        "        joints=True, links=True, scale=16, lw=4):\n",
        "\n",
        "    s = skel.copy()\n",
        "    num_joints = len(s)\n",
        "    assert ((num_joints == 16) or (num_joints == 17)) or (num_joints == 20), \\\n",
        "            'Unsupported number of joints (%d)' % num_joints\n",
        "\n",
        "    color, cmap, links = _get_poselayout(num_joints)\n",
        "\n",
        "    x = s[:,0]\n",
        "    y = s[:,1]\n",
        "    v = s > -1e6\n",
        "    v = v.any(axis=1).astype(np.float32)\n",
        "\n",
        "    # Convert normalized skeletons to image coordinates.\n",
        "    if w is not None:\n",
        "        x *= w\n",
        "    if h is not None:\n",
        "        y *= h\n",
        "\n",
        "    if joints:\n",
        "        for i in range(len(v)):\n",
        "            if v[i] > 0:\n",
        "                c = color[cmap[i]]\n",
        "                subplot.scatter(x=x[i], y=y[i], c=c, lw=lw, s=scale, zorder=2)\n",
        "\n",
        "    if links:\n",
        "        for i in links:\n",
        "            if ((v[i[0]] > 0) and (v[i[1]] > 0)):\n",
        "                c = color[cmap[i[0]]]\n",
        "                subplot.plot(x[i], y[i], lw=lw, c=c, zorder=1)\n"
      ],
      "metadata": {
        "id": "GTNwXcMFUFIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AffineTransform(object):\n",
        "    def __init__(self):\n",
        "        self.afmat = np.eye(3)\n",
        "\n",
        "    def _apply(self, t):\n",
        "        self.afmat = np.dot(t, self.afmat)\n",
        "\n",
        "    def scale(self, w, h):\n",
        "        t = np.eye(3)\n",
        "        t[0,0] *= w\n",
        "        t[1,1] *= h\n",
        "        self._apply(t)\n",
        "\n",
        "    def translate(self, x, y):\n",
        "        t = np.eye(3)\n",
        "        t[0,2] = x\n",
        "        t[1,2] = y\n",
        "        self._apply(t)\n",
        "\n",
        "    def rotate(self, angle, center):\n",
        "        self.translate(-center[0], -center[1])\n",
        "        self.rotate_center(angle)\n",
        "        self.translate(center[0], center[1])\n",
        "\n",
        "    def rotate_center(self, angle):\n",
        "        t = np.eye(3)\n",
        "        angle *= np.pi / 180\n",
        "        a = np.cos(angle)\n",
        "        b = np.sin(angle)\n",
        "        t[0,0] = a\n",
        "        t[0,1] = b\n",
        "        t[1,1] = a\n",
        "        t[1,0] = -b\n",
        "        self._apply(t)\n",
        "\n",
        "    def affine_hflip(self):\n",
        "        t = np.eye(3)\n",
        "        t[0,0] = -1\n",
        "        self._apply(t)\n",
        "\n",
        "\n",
        "class T(AffineTransform):\n",
        "    def __init__(self, img, img_size=None):\n",
        "        self.img = img\n",
        "        if img_size is not None:\n",
        "            self.img_size = tuple(img_size)\n",
        "        else:\n",
        "            self.img_size = img_size\n",
        "        self.hflip = False\n",
        "        AffineTransform.__init__(self)\n",
        "\n",
        "    def resize(self, size, resample=Image.BILINEAR):\n",
        "        t = self.scale(size[0] / self.size[0], size[1] / self.size[1])\n",
        "        if self.img is not None:\n",
        "            self.img = self.img.resize(size, resample)\n",
        "        else:\n",
        "            self.img_size = tuple(size)\n",
        "\n",
        "    def normalize_affinemap(self):\n",
        "        t = self.scale(1 / self.size[0], 1 / self.size[1])\n",
        "\n",
        "    def crop(self, box):\n",
        "        t = self.translate(-box[0], -box[1])\n",
        "        if self.img is not None:\n",
        "            self.img = self.img.crop(box)\n",
        "        else:\n",
        "            self.img_size = (box[2] - box[0], box[3] - box[1])\n",
        "\n",
        "    def rotate_crop(self, angle, center, winsize,\n",
        "            resample=Image.BILINEAR):\n",
        "        \"\"\"Rotate, crop, and resize the image.\n",
        "\n",
        "        # Arguments\n",
        "            angle: angle to rotate in degrees.\n",
        "            winsize: (w, h) window size to crop in the input image.\n",
        "            center: center point (x,y) to rotate from, None to use the\n",
        "                image center.\n",
        "            resample: rescaling method, according to PIL.Image.\n",
        "        \"\"\"\n",
        "\n",
        "        if center is None:\n",
        "            center = (self.size[0]/2, self.size[1]/2)\n",
        "\n",
        "        if angle != 0:\n",
        "            self.rotate(angle, center)\n",
        "\n",
        "        # Compute the margins after rotation\n",
        "        corners = np.array([\n",
        "            [0, 0],\n",
        "            [self.size[0], 0],\n",
        "            [0, self.size[1]],\n",
        "            [self.size[0], self.size[1]]\n",
        "            ]).transpose()\n",
        "        corners = transform_2d_points(self.afmat, corners)\n",
        "\n",
        "        # Translate to zero margin\n",
        "        self.translate(-min(corners[0,:]), -min(corners[1,:]))\n",
        "\n",
        "        # Rotate image\n",
        "        if (self.img is not None) and (angle != 0):\n",
        "            self.img = self.img.rotate(angle, resample, expand=True)\n",
        "\n",
        "        center = np.array([center[0], center[1]])\n",
        "        center = transform_2d_points(self.afmat, center)\n",
        "\n",
        "        crop = np.array([center[0] - winsize[0]/2, center[1] - winsize[1]/2,\n",
        "            center[0] + winsize[0]/2, center[1] + winsize[1]/2], dtype=int)\n",
        "        self.crop(crop)\n",
        "\n",
        "\n",
        "    def horizontal_flip(self):\n",
        "        self.affine_hflip()\n",
        "        self.translate(self.size[0], 0)\n",
        "        if self.img is not None:\n",
        "            self.img = self.img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        self.hflip = not self.hflip\n",
        "\n",
        "    def asarray(self, dtype=np.float32):\n",
        "        if self.img is not None:\n",
        "            return np.asarray(self.img, dtype=dtype)\n",
        "        else:\n",
        "            return np.zeros(self.img_size + (3,))\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        if self.img is not None:\n",
        "            return self.img.size\n",
        "        else:\n",
        "            return self.img_size\n",
        "\n",
        "\n",
        "def transform_2d_points(A, x, transpose=False, inverse=False):\n",
        "    \"\"\"Apply a given affine transformation to 2D points.\n",
        "\n",
        "    # Arguments\n",
        "        A: [3, 3] affine transformation map: T(x) = Ax.\n",
        "        x: [dim, N] points (normal case, otherwise, set the flag 'transpose').\n",
        "        transpose: flag to be setted if 'x' is [N, dim].\n",
        "        inverse: flag to apply the inverse transformation on A.\n",
        "\n",
        "    # Return\n",
        "        The transformed points.\n",
        "    \"\"\"\n",
        "\n",
        "    squeeze = False\n",
        "    if len(x.shape) == 1:\n",
        "        x = np.expand_dims(x, axis=-1)\n",
        "        squeeze = True\n",
        "    elif transpose:\n",
        "        x = np.transpose(x)\n",
        "\n",
        "    (dim, N) = x.shape\n",
        "    assert (dim == 2), \\\n",
        "            'transform_2d_points: Only 2D points are supported, get ' +str(dim)\n",
        "\n",
        "    if inverse:\n",
        "        A = np.linalg.inv(A)\n",
        "\n",
        "    y = np.ones((dim+1, N))\n",
        "    y[0:dim,:] = x[0:dim,:]\n",
        "    y = np.dot(A, y)[0:dim]\n",
        "\n",
        "    if squeeze:\n",
        "        return np.squeeze(y)\n",
        "    if transpose:\n",
        "        return np.transpose(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def normalize_channels(frame, channel_power=1):\n",
        "\n",
        "    if type(channel_power) is not int:\n",
        "        assert len(channel_power) == 3, 'channel_power expected to be int or ' \\\n",
        "                + 'tuple/list with len=3, {} given.'.format(channel_power)\n",
        "\n",
        "    frame /= 255.\n",
        "\n",
        "    if type(channel_power) is int:\n",
        "        if channel_power != 1:\n",
        "            frame = np.power(frame, channel_power)\n",
        "    else:\n",
        "        for c in range(3):\n",
        "            if channel_power[c] != 1:\n",
        "                frame[:,:, c] = np.power(frame[:,:, c], channel_power[c])\n",
        "\n",
        "    frame -= .5\n",
        "    frame *= 2.\n",
        "\n",
        "    return frame\n"
      ],
      "metadata": {
        "id": "Lzd8UaA1UJqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Into the Data**\n",
        "\n",
        "\n",
        "*   Reffer to the link above in order to fin the datasets of MPPI for pose estimation and Penn Action for action recognitioon.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BMhAteTrUQZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_clip_frame_index(sequence_size, subsample, num_frames,\n",
        "        random_clip=False):\n",
        "\n",
        "    # Assert that subsample is integer and positive\n",
        "    assert (type(subsample) == int) and subsample > 0\n",
        "\n",
        "    idx_coef = 1.\n",
        "    while idx_coef*sequence_size < num_frames:\n",
        "        idx_coef *= 1.5\n",
        "    sequence_size *= idx_coef\n",
        "\n",
        "    # Check if the given subsample value is feasible, otherwise, reduce\n",
        "    # it to the maximum acceptable value.\n",
        "    max_subsample = int(sequence_size / num_frames)\n",
        "    if subsample > max_subsample:\n",
        "        subsample = max_subsample\n",
        "\n",
        "    vidminf = subsample * (num_frames - 1) + 1 # Video min num of frames\n",
        "    maxs = sequence_size - vidminf # Maximum start\n",
        "    if random_clip:\n",
        "        start = np.random.randint(maxs + 1)\n",
        "    else:\n",
        "        start = int(maxs / 2)\n",
        "\n",
        "    frames = list(range(start, start + vidminf, subsample))\n",
        "    if idx_coef > 1:\n",
        "        for i in range(len(frames)):\n",
        "            frames[i] = int(frames[i] / idx_coef)\n",
        "\n",
        "    return frames"
      ],
      "metadata": {
        "id": "3btNRFBGUMkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "ACTION_LABELS = None\n",
        "\n",
        "def load_h36m_mat_annotation(filename):\n",
        "    mat = sio.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
        "\n",
        "    # Respect the order of TEST (0), TRAIN (1), and VALID (2)\n",
        "    sequences = [mat['sequences_te'], mat['sequences_tr'], mat['sequences_val']]\n",
        "    action_labels = mat['action_labels']\n",
        "    joint_labels = mat['joint_labels']\n",
        "\n",
        "    return sequences, action_labels, joint_labels\n",
        "\n",
        "\n",
        "def serialize_index_sequences(seq):\n",
        "    frames_idx = []\n",
        "    for s in range(len(seq)):\n",
        "        for f in range(len(seq[s].frames)):\n",
        "            frames_idx.append((s, f))\n",
        "\n",
        "    return frames_idx\n",
        "\n",
        "\n",
        "class Human36M(object):\n",
        "    \"\"\"Implementation of the Human3.6M dataset for 3D pose estimation and\n",
        "    action recognition.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path, dataconf, poselayout=pa17j3d,\n",
        "            topology='sequences', clip_size=16):\n",
        "\n",
        "        assert topology in ['sequences', 'frames'], \\\n",
        "                'Invalid topology ({})'.format(topology)\n",
        "\n",
        "        self.dataset_path = dataset_path\n",
        "        self.dataconf = dataconf\n",
        "        self.poselayout = poselayout\n",
        "        self.topology = topology\n",
        "        self.clip_size = clip_size\n",
        "        self.load_annotations(os.path.join(dataset_path, 'annotations.mat'))\n",
        "\n",
        "    def load_annotations(self, filename):\n",
        "        try:\n",
        "            self.sequences, self.action_labels, self.joint_labels = \\\n",
        "                    load_h36m_mat_annotation(filename)\n",
        "            self.frame_idx = [serialize_index_sequences(self.sequences[0]),\n",
        "                    serialize_index_sequences(self.sequences[1]),\n",
        "                    serialize_index_sequences(self.sequences[2])]\n",
        "\n",
        "            global ACTION_LABELS\n",
        "            ACTION_LABELS = self.action_labels\n",
        "\n",
        "        except:\n",
        "            warning('Error loading Human3.6M dataset!')\n",
        "            raise\n",
        "\n",
        "\n",
        "    def get_data(self, key, mode, frame_list=None, fast_crop=False):\n",
        "        output = {}\n",
        "\n",
        "        if mode == TRAIN_MODE:\n",
        "            dconf = self.dataconf.random_data_generator()\n",
        "            random_clip = True\n",
        "        else:\n",
        "            dconf = self.dataconf.get_fixed_config()\n",
        "            random_clip = False\n",
        "\n",
        "        if self.topology == 'sequences':\n",
        "            seq = self.sequences[mode][key]\n",
        "            if frame_list == None:\n",
        "                frame_list = get_clip_frame_index(len(seq.frames),\n",
        "                        dconf['subspl'], self.clip_size,\n",
        "                        random_clip=random_clip)\n",
        "            objframes = seq.frames[frame_list]\n",
        "        else:\n",
        "            seq_idx, frame_idx = self.frame_idx[mode][key]\n",
        "            seq = self.sequences[mode][seq_idx]\n",
        "            objframes = seq.frames[[frame_idx]]\n",
        "\n",
        "        \"\"\"Build a Camera object\"\"\"\n",
        "        cpar = seq.camera_parameters\n",
        "        cam = Camera(cpar.R, cpar.T, cpar.f, cpar.c, cpar.p, cpar.k)\n",
        "\n",
        "        \"\"\"Load and project the poses\"\"\"\n",
        "        pose_w = self.load_pose_annot(objframes)\n",
        "        pose_uvd = cam.project(np.reshape(pose_w, (-1, 3)))\n",
        "        pose_uvd = np.reshape(pose_uvd,\n",
        "                (len(objframes), self.poselayout.num_joints, 3))\n",
        "\n",
        "        \"\"\"Compute GT bouding box.\"\"\"\n",
        "        imgsize = (objframes[0].w, objframes[0].h)\n",
        "        objpos, winsize, zrange = get_crop_params(pose_uvd[:, 0, :],\n",
        "                imgsize, cam.f, dconf['scale'])\n",
        "\n",
        "        objpos += dconf['scale'] * np.array([dconf['transx'], dconf['transy']])\n",
        "        frames = np.empty((len(objframes),) + self.dataconf.input_shape)\n",
        "        pose = np.empty((len(objframes), self.poselayout.num_joints,\n",
        "            self.poselayout.dim))\n",
        "\n",
        "        for i in range(len(objframes)):\n",
        "            image = 'images/%s/%05d.jpg' % (seq.name, objframes[i].f)\n",
        "            imgt = T(Image.open(os.path.join(self.dataset_path, image)))\n",
        "\n",
        "            imgt.rotate_crop(dconf['angle'], objpos, winsize)\n",
        "            if dconf['hflip'] == 1:\n",
        "                imgt.horizontal_flip()\n",
        "\n",
        "            imgt.resize(self.dataconf.crop_resolution)\n",
        "            imgt.normalize_affinemap()\n",
        "            frames[i, :, :, :] = normalize_channels(imgt.asarray(),\n",
        "                    channel_power=dconf['chpower'])\n",
        "\n",
        "            pose[i, :, 0:2] = transform_2d_points(imgt.afmat,\n",
        "                    pose_uvd[i, :,0:2], transpose=True)\n",
        "            pose[i, :, 2] = \\\n",
        "                    (pose_uvd[i, :, 2] - zrange[0]) / (zrange[1] - zrange[0])\n",
        "\n",
        "            if imgt.hflip:\n",
        "                pose[i, :, :] = pose[i, self.poselayout.map_hflip, :]\n",
        "\n",
        "        \"\"\"Set outsider body joints to invalid (-1e9).\"\"\"\n",
        "        pose = np.reshape(pose, (-1, self.poselayout.dim))\n",
        "        pose[np.isnan(pose)] = -1e9\n",
        "        v = np.expand_dims(get_visible_joints(pose[:,0:2]), axis=-1)\n",
        "        pose[(v==0)[:,0],:] = -1e9\n",
        "        pose = np.reshape(pose, (len(objframes), self.poselayout.num_joints,\n",
        "            self.poselayout.dim))\n",
        "        v = np.reshape(v, (len(objframes), self.poselayout.num_joints, 1))\n",
        "\n",
        "        pose = np.concatenate((pose, v), axis=-1)\n",
        "        if self.topology != 'sequences':\n",
        "            pose_w = np.squeeze(pose_w, axis=0)\n",
        "            pose_uvd = np.squeeze(pose_uvd, axis=0)\n",
        "            pose = np.squeeze(pose, axis=0)\n",
        "            frames = np.squeeze(frames, axis=0)\n",
        "\n",
        "        output['camera'] = cam.serialize()\n",
        "        output['action'] = int(seq.name[1:3]) - 1\n",
        "        output['pose_w'] = pose_w\n",
        "        output['pose_uvd'] = pose_uvd\n",
        "        output['pose'] = pose\n",
        "        output['frame'] = frames\n",
        "\n",
        "        \"\"\"Take the last transformation matrix, it should not change\"\"\"\n",
        "        output['afmat'] = imgt.afmat.copy()\n",
        "\n",
        "        return output\n",
        "\n",
        "    def load_pose_annot(self, frames):\n",
        "        p = np.empty((len(frames), self.poselayout.num_joints,\n",
        "            self.poselayout.dim))\n",
        "        for i in range(len(frames)):\n",
        "            p[i,:] = frames[i].pose3d.T[self.poselayout.map_from_h36m,\n",
        "                    0:self.poselayout.dim].copy()\n",
        "\n",
        "        return p\n",
        "\n",
        "    def clip_length(self):\n",
        "        if self.topology == 'sequences':\n",
        "            return self.clip_size\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def clip_shape(self):\n",
        "        if self.topology == 'sequences':\n",
        "            return (self.clip_size,)\n",
        "        else:\n",
        "            return ()\n",
        "\n",
        "    def get_shape(self, dictkey):\n",
        "        if dictkey == 'frame':\n",
        "            return self.clip_shape() + self.dataconf.input_shape\n",
        "        if dictkey == 'pose':\n",
        "            return self.clip_shape() \\\n",
        "                    + (self.poselayout.num_joints, self.poselayout.dim+1)\n",
        "        if dictkey == 'pose_w':\n",
        "            return self.clip_shape() \\\n",
        "                    + (self.poselayout.num_joints, self.poselayout.dim)\n",
        "        if dictkey == 'pose_uvd':\n",
        "            return self.clip_shape() \\\n",
        "                    + (self.poselayout.num_joints, self.poselayout.dim)\n",
        "        if dictkey == 'action':\n",
        "            return (1,)\n",
        "        if dictkey == 'camera':\n",
        "            return (21,)\n",
        "        if dictkey == 'afmat':\n",
        "            return (3, 3)\n",
        "        raise Exception('Invalid dictkey on get_shape!')\n",
        "\n",
        "    def get_length(self, mode):\n",
        "        if self.topology == 'sequences':\n",
        "            return len(self.sequences[mode])\n",
        "        else:\n",
        "            return len(self.frame_idx[mode])"
      ],
      "metadata": {
        "id": "NoCxxGtPUcEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchLoader(Sequence):\n",
        "    \"\"\"Loader class for generic datasets, based on the Sequence class from\n",
        "    Keras.\n",
        "\n",
        "    One (or more) object(s) implementing a dataset should be provided.\n",
        "    The required functions are 'get_length(self, mode)' and\n",
        "    'get_data(self, key, mode)'. The first returns an integer, and the last\n",
        "    returns a dictionary containing the data for a given pair of (key, mode).\n",
        "\n",
        "    # Arguments\n",
        "        dataset: A dataset object, or a list of dataset objects (for multiple\n",
        "            datasets), which are merged by this class.\n",
        "        x_dictkeys: Key names (strings) to constitute the baches of X data\n",
        "            (input).\n",
        "        y_dictkeys: Identical to x_dictkeys, but for Y data (labels).\n",
        "            All given datasets must provide those keys.\n",
        "        batch_size: Number of samples in each batch. If multiple datasets, it\n",
        "            can be a list with the same length of 'dataset', where each value\n",
        "            corresponds to the number of samples from the respective dataset,\n",
        "            or it can be a single value, which corresponds to the number of\n",
        "            samples from *each* dataset.\n",
        "        num_predictions: number of predictions (y) that should be repeated for\n",
        "            training.\n",
        "        mode: TRAIN_MODE, TEST_MODE, or VALID_MODE.\n",
        "        shuffle: boolean to shuffle *samples* (not batches!) or not.\n",
        "        custom_dummy_dictkey: Allows to generate dummy outputs for each batch.\n",
        "            Should be defined as a list of tuples, each with three values:\n",
        "            (dictkey, shape, value). It is useful to include an action label for\n",
        "            a sequence poses from pose-only datasets, e.g., when mixturing MPII\n",
        "            and Human3.6M for training with action recognition at the same time\n",
        "            (to further mergning with an action dataset).\n",
        "    \"\"\"\n",
        "    BATCH_HOLD = 4\n",
        "\n",
        "    def __init__(self, dataset, x_dictkeys, y_dictkeys, mode,\n",
        "            batch_size=24, num_predictions=1, shuffle=True,\n",
        "            custom_dummy_dictkey=[]):\n",
        "\n",
        "        if not isinstance(dataset, list):\n",
        "            dataset = [dataset]\n",
        "        self.datasets = dataset\n",
        "        self.x_dictkeys = x_dictkeys\n",
        "        self.y_dictkeys = y_dictkeys\n",
        "        self.allkeys = x_dictkeys + y_dictkeys\n",
        "\n",
        "        \"\"\"Include custom dictkeys into the output list.\"\"\"\n",
        "        self.custom_dummy_dictkey = custom_dummy_dictkey\n",
        "        self.custom_dictkeys = []\n",
        "        for dummyout in self.custom_dummy_dictkey:\n",
        "            assert dummyout[0] not in self.y_dictkeys, \\\n",
        "                    'dummy key {} already in y_dictkeys!'.format(dummyout[0])\n",
        "            self.custom_dictkeys.append(dummyout[0])\n",
        "        self.y_dictkeys += self.custom_dictkeys\n",
        "\n",
        "        \"\"\"Make sure that all datasets have the same shapes for all dictkeys\"\"\"\n",
        "        for dkey in self.allkeys:\n",
        "            for i in range(1, len(self.datasets)):\n",
        "                assert self.datasets[i].get_shape(dkey) == \\\n",
        "                        self.datasets[i-1].get_shape(dkey), \\\n",
        "                        'Incompatible dataset shape for dictkey {}'.format(dkey)\n",
        "\n",
        "        self.batch_sizes = batch_size\n",
        "        if not isinstance(self.batch_sizes, list):\n",
        "            self.batch_sizes = len(self.datasets)*[self.batch_sizes]\n",
        "\n",
        "        assert len(self.datasets) == len(self.batch_sizes), \\\n",
        "                'dataset and batch_size should be lists with the same length.'\n",
        "\n",
        "        if isinstance(num_predictions, int):\n",
        "            self.num_predictions = len(self.y_dictkeys)*[num_predictions]\n",
        "        elif isinstance(num_predictions, list):\n",
        "            self.num_predictions = num_predictions\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                'Invalid num_predictions ({})'.format(num_predictions))\n",
        "\n",
        "        assert len(self.num_predictions) == len(self.y_dictkeys), \\\n",
        "                'num_predictions and y_dictkeys not matching'\n",
        "\n",
        "        self.mode = mode\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        \"\"\"Create one lock object for each dataset in case of data shuffle.\"\"\"\n",
        "        if self.shuffle:\n",
        "            self.qkey = []\n",
        "            self.lock = []\n",
        "            for d in range(self.num_datasets):\n",
        "                maxsize = self.datasets[d].get_length(self.mode) \\\n",
        "                        + BatchLoader.BATCH_HOLD*self.batch_sizes[d]\n",
        "                self.qkey.append(Queue(maxsize=maxsize))\n",
        "                self.lock.append(threading.Lock())\n",
        "\n",
        "    def __len__(self):\n",
        "        dataset_len = []\n",
        "        for d in range(self.num_datasets):\n",
        "            dataset_len.append(\n",
        "                    int(np.ceil(self.datasets[d].get_length(self.mode) /\n",
        "                        float(self.batch_sizes[d]))))\n",
        "\n",
        "        return max(dataset_len)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_dict = self.get_data(idx, self.mode)\n",
        "\n",
        "        \"\"\"Convert the dictionary of samples to a list for x and y.\"\"\"\n",
        "        x_batch = []\n",
        "        for dkey in self.x_dictkeys:\n",
        "            x_batch.append(data_dict[dkey])\n",
        "\n",
        "        y_batch = []\n",
        "        for i, dkey in enumerate(self.y_dictkeys):\n",
        "            for _ in range(self.num_predictions[i]):\n",
        "                y_batch.append(data_dict[dkey])\n",
        "\n",
        "        return x_batch, y_batch\n",
        "\n",
        "    def get_batch_size(self):\n",
        "        return sum(self.batch_sizes)\n",
        "\n",
        "    def get_data(self, idx, mode):\n",
        "        \"\"\"Get the required data by mergning all the datasets as specified\n",
        "        by the object's parameters.\"\"\"\n",
        "        data_dict = {}\n",
        "        for dkey in self.allkeys:\n",
        "            data_dict[dkey] = np.empty((sum(self.batch_sizes),) \\\n",
        "                    + self.datasets[0].get_shape(dkey))\n",
        "\n",
        "        \"\"\"Add custom dummy data.\"\"\"\n",
        "        for dummyout in self.custom_dummy_dictkey:\n",
        "            dkey, dshape, dvalue = dummyout\n",
        "            data_dict[dkey] = dvalue * np.ones(dshape)\n",
        "\n",
        "        batch_cnt = 0\n",
        "        for d in range(len(self.datasets)):\n",
        "            for i in range(self.batch_sizes[d]):\n",
        "                if self.shuffle:\n",
        "                    key = self.get_shuffled_key(d)\n",
        "                else:\n",
        "                    key = idx*self.batch_sizes[d] + i\n",
        "                    if key >= self.datasets[d].get_length(mode):\n",
        "                        key -= self.datasets[d].get_length(mode)\n",
        "\n",
        "                data = self.datasets[d].get_data(key, mode)\n",
        "                for dkey in self.allkeys:\n",
        "                    data_dict[dkey][batch_cnt, :] = data[dkey]\n",
        "\n",
        "                batch_cnt += 1\n",
        "\n",
        "        return data_dict\n",
        "\n",
        "    def get_shape(self, dictkey):\n",
        "        \"\"\"Inception of get_shape method.\n",
        "        First check if it is a custom key.\n",
        "        \"\"\"\n",
        "        for dummyout in self.custom_dummy_dictkey:\n",
        "            if dictkey == dummyout[0]:\n",
        "                return dummyout[1]\n",
        "        return (sum(self.batch_sizes),) + self.datasets[0].get_shape(dictkey)\n",
        "\n",
        "    def get_length(self, mode):\n",
        "        assert mode == self.mode, \\\n",
        "                'You are mixturing modes! {} with {}'.format(mode, self.mode)\n",
        "        return len(self)\n",
        "\n",
        "    def get_shuffled_key(self, dataset_idx):\n",
        "        assert self.shuffle, \\\n",
        "                'There is not sense in calling this function if shuffle=False!'\n",
        "\n",
        "        key = None\n",
        "        with self.lock[dataset_idx]:\n",
        "            min_samples = BatchLoader.BATCH_HOLD*self.batch_sizes[dataset_idx]\n",
        "            if self.qkey[dataset_idx].qsize() <= min_samples:\n",
        "                \"\"\"Need to fill that list.\"\"\"\n",
        "                num_samples = self.datasets[dataset_idx].get_length(self.mode)\n",
        "                newlist = list(range(num_samples))\n",
        "                random.shuffle(newlist)\n",
        "                try:\n",
        "                    for j in newlist:\n",
        "                        self.qkey[dataset_idx].put(j, False)\n",
        "                except queue.Full:\n",
        "                    pass\n",
        "            key = self.qkey[dataset_idx].get()\n",
        "\n",
        "        return key\n",
        "\n",
        "    @property\n",
        "    def num_datasets(self):\n",
        "        return len(self.datasets)\n",
        "\n"
      ],
      "metadata": {
        "id": "uEYhhLf9UyPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mpii_mat_annotation(filename):\n",
        "    mat = sio.loadmat(filename)\n",
        "    annot_tr = mat['annot_tr']\n",
        "    annot_val = mat['annot_val']\n",
        "\n",
        "    # Respect the order of TEST (0), TRAIN (1), and VALID (2)\n",
        "    rectidxs = [None, annot_tr[0,:], annot_val[0,:]]\n",
        "    images = [None, annot_tr[1,:], annot_val[1,:]]\n",
        "    annorect = [None, annot_tr[2,:], annot_val[2,:]]\n",
        "\n",
        "    return rectidxs, images, annorect\n",
        "\n",
        "\n",
        "def serialize_annorect(rectidxs, annorect):\n",
        "    assert len(rectidxs) == len(annorect)\n",
        "\n",
        "    sample_list = []\n",
        "    for i in range(len(rectidxs)):\n",
        "        rec = rectidxs[i]\n",
        "        for j in range(rec.size):\n",
        "            idx = rec[j,0]-1 # Convert idx from Matlab\n",
        "            ann = annorect[i][idx,0]\n",
        "            annot = {}\n",
        "            annot['head'] = ann['head'][0,0][0]\n",
        "            annot['objpos'] = ann['objpos'][0,0][0]\n",
        "            annot['scale'] = ann['scale'][0,0][0,0]\n",
        "            annot['pose'] = ann['pose'][0,0]\n",
        "            annot['imgidx'] = i\n",
        "            sample_list.append(annot)\n",
        "\n",
        "    return sample_list\n",
        "\n",
        "\n",
        "def calc_head_size(head_annot):\n",
        "    head = np.array([float(head_annot[0]), float(head_annot[1]),\n",
        "        float(head_annot[2]), float(head_annot[3])])\n",
        "    return 0.6 * np.linalg.norm(head[0:2] - head[2:4])\n",
        "\n",
        "\n",
        "class MpiiSinglePerson(object):\n",
        "    \"\"\"Implementation of the MPII dataset for single person.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path, dataconf,\n",
        "            poselayout=pa16j2d,\n",
        "            remove_outer_joints=True):\n",
        "\n",
        "        self.dataset_path = dataset_path\n",
        "        self.dataconf = dataconf\n",
        "        self.poselayout = poselayout\n",
        "        self.remove_outer_joints = remove_outer_joints\n",
        "        self.load_annotations(os.path.join(dataset_path, 'annotations.mat'))\n",
        "\n",
        "    def load_annotations(self, filename):\n",
        "        try:\n",
        "            #rectidxs, images, annorect = load_mpii_mat_annotation(filename)\n",
        "            rectidxs, images, annorect = load_mpii_mat_annotation(filename)\n",
        "            self.samples = {}\n",
        "            self.samples[TEST_MODE] = [] # No samples for test\n",
        "            self.samples[TRAIN_MODE] = serialize_annorect(\n",
        "                    rectidxs[TRAIN_MODE], annorect[TRAIN_MODE])\n",
        "            self.samples[VALID_MODE] = serialize_annorect(\n",
        "                    rectidxs[VALID_MODE], annorect[VALID_MODE])\n",
        "            self.images = images\n",
        "\n",
        "        except:\n",
        "            warning('Error loading the MPII dataset!')\n",
        "            raise\n",
        "\n",
        "    def load_image(self, key, mode):\n",
        "        try:\n",
        "            annot = self.samples[mode][key]\n",
        "            image = self.images[mode][annot['imgidx']][0]\n",
        "            imgt = T(Image.open(os.path.join(\n",
        "                self.dataset_path, 'images', image)))\n",
        "        except:\n",
        "            warning('Error loading sample key/mode: %d/%d' % (key, mode))\n",
        "            raise\n",
        "\n",
        "        return imgt\n",
        "\n",
        "    def get_data(self, key, mode, fast_crop=False):\n",
        "        output = {}\n",
        "\n",
        "        if mode == TRAIN_MODE:\n",
        "            dconf = self.dataconf.random_data_generator()\n",
        "        else:\n",
        "            dconf = self.dataconf.get_fixed_config()\n",
        "\n",
        "        imgt = self.load_image(key, mode)\n",
        "        annot = self.samples[mode][key]\n",
        "\n",
        "        scale = 1.25*annot['scale']\n",
        "        objpos = np.array([annot['objpos'][0], annot['objpos'][1] + 12*scale])\n",
        "        objpos += scale * np.array([dconf['transx'], dconf['transy']])\n",
        "        winsize = 200 * dconf['scale'] * scale\n",
        "        winsize = (winsize, winsize)\n",
        "        output['bbox'] = objposwin_to_bbox(objpos, winsize)\n",
        "\n",
        "        if fast_crop:\n",
        "            \"\"\"Slightly faster method, but gives lower precision.\"\"\"\n",
        "            imgt.crop_resize_rotate(objpos, winsize,\n",
        "                    self.dataconf.crop_resolution, dconf['angle'])\n",
        "        else:\n",
        "            imgt.rotate_crop(dconf['angle'], objpos, winsize)\n",
        "            imgt.resize(self.dataconf.crop_resolution)\n",
        "\n",
        "        if dconf['hflip'] == 1:\n",
        "            imgt.horizontal_flip()\n",
        "\n",
        "        imgt.normalize_affinemap()\n",
        "        output['frame'] = normalize_channels(imgt.asarray(),\n",
        "                channel_power=dconf['chpower'])\n",
        "\n",
        "        p = np.empty((self.poselayout.num_joints, self.poselayout.dim))\n",
        "        p[:] = np.nan\n",
        "\n",
        "        head = annot['head']\n",
        "        p[self.poselayout.map_to_mpii, 0:2] = \\\n",
        "                transform_2d_points(imgt.afmat, annot['pose'].T, transpose=True)\n",
        "        if imgt.hflip:\n",
        "            p = p[self.poselayout.map_hflip, :]\n",
        "\n",
        "        # Set invalid joints and NaN values as an invalid value\n",
        "        p[np.isnan(p)] = -1e9\n",
        "        v = np.expand_dims(get_visible_joints(p[:,0:2]), axis=-1)\n",
        "        if self.remove_outer_joints:\n",
        "            p[(v==0)[:,0],:] = -1e9\n",
        "\n",
        "        output['pose'] = np.concatenate((p, v), axis=-1)\n",
        "        output['headsize'] = calc_head_size(annot['head'])\n",
        "        output['afmat'] = imgt.afmat.copy()\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_shape(self, dictkey):\n",
        "        if dictkey == 'frame':\n",
        "            return self.dataconf.input_shape\n",
        "        if dictkey == 'pose':\n",
        "            return (self.poselayout.num_joints, self.poselayout.dim+1)\n",
        "        if dictkey == 'headsize':\n",
        "            return (1,)\n",
        "        if dictkey == 'afmat':\n",
        "            return (3, 3)\n",
        "        raise Exception('Invalid dictkey on get_shape!')\n",
        "\n",
        "    def get_length(self, mode):\n",
        "        return len(self.samples[mode])\n"
      ],
      "metadata": {
        "id": "ZQPH4OrQU35Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "use_small_images = True\n",
        "image_prefix = 'images-small' if use_small_images else 'images'\n",
        "video_subsample = 2\n",
        "\n",
        "ACTION_LABELS = ['drink water', 'eat meal/snack', 'brushing teeth',\n",
        "        'brushing hair', 'drop', 'pickup', 'throw', 'sitting down',\n",
        "        'standing up (from sitting position)', 'clapping', 'reading',\n",
        "        'writing', 'tear up paper', 'wear jacket', 'take off jacket',\n",
        "        'wear a shoe', 'take off a shoe', 'wear on glasses',\n",
        "        'take off glasses', 'put on a hat/cap', 'take off a hat/cap',\n",
        "        'cheer up', 'hand waving', 'kicking something',\n",
        "        'put something inside pocket / take out something from pocket',\n",
        "        'hopping (one foot jumping)', 'jump up',\n",
        "        'make a phone call/answer phone', 'playing with phone/tablet',\n",
        "        'typing on a keyboard', 'pointing to something with finger',\n",
        "        'taking a selfie', 'check time (from watch)', 'rub two hands together',\n",
        "        'nod head/bow', 'shake head', 'wipe face', 'salute',\n",
        "        'put the palms together', 'cross hands in front (say stop)',\n",
        "        'sneeze/cough', 'staggering', 'falling', 'touch head (headache)',\n",
        "        'touch chest (stomachache/heart pain)', 'touch back (backache)',\n",
        "        'touch neck (neckache)', 'nausea or vomiting condition',\n",
        "        'use a fan (with hand or paper)/feeling warm',\n",
        "        'punching/slapping other person', 'kicking other person',\n",
        "        'pushing other person', 'pat on back of other person',\n",
        "        'point finger at the other person', 'hugging other person',\n",
        "        'giving something to other person', 'touch other person s pocket',\n",
        "        'handshaking', 'walking towards each other',\n",
        "        'walking apart from each other']\n",
        "\n",
        "JOINT_LABELS = ['base of the spine', 'middle of the spine', 'neck', 'head',\n",
        "        'left shoulder', 'left elbow', 'left wrist', 'left hand',\n",
        "        'right shoulder', 'right elbow', 'right wrist', 'right hand',\n",
        "        'left hip', 'left knee', 'left ankle', 'left foot', 'right hip',\n",
        "        'right knee', 'right ankle', 'right foot', 'spine',\n",
        "        'tip of the left hand', 'left thumb', 'tip of the right hand',\n",
        "        'right thumb']\n",
        "\n",
        "VIEWPOINT_LABELS = ['cam1', 'cam2', 'cam3']\n",
        "\n",
        "\n",
        "def serialize_index_sequences(sequences):\n",
        "    frame_idx = []\n",
        "    for s in range(len(sequences)):\n",
        "        for f in range(len(sequences[s])):\n",
        "            frame_idx.append((s, f))\n",
        "\n",
        "    return frame_idx\n",
        "\n",
        "\n",
        "def ntu_load_annotations(dataset_path, eval_mode='cs',\n",
        "        num_S=17, num_C=3, num_P=40, num_R=2, num_A=60):\n",
        "\n",
        "    # Saniry checks\n",
        "    assert eval_mode in ['cs', 'cv'], \\\n",
        "        'Invalid evaluation mode {}'.format(eval_mode)\n",
        "\n",
        "    ntud_numpy_dir = os.path.join(dataset_path, 'nturgb+d_numpy')\n",
        "    ntud_images_dir = os.path.join(dataset_path, image_prefix)\n",
        "    for d in [ntud_numpy_dir, ntud_images_dir]:\n",
        "        assert os.path.isdir(d), \\\n",
        "            f'Error: check your NTU dataset! `{d}` not found!'\n",
        "\n",
        "    min_num_frames = np.inf\n",
        "    max_num_frames = -np.inf\n",
        "    num_videos = [0, 0, 0]\n",
        "\n",
        "    cs_train = [1, 2, 4, 5, 8, 9, 13, 14, 15, 16, 17, 18, 19,\n",
        "            25, 27, 28, 31, 34, 35, 38]\n",
        "    cv_train = [2, 3]\n",
        "\n",
        "    sequences = [[], [], []]\n",
        "    seq_ids = [[], [], []]\n",
        "    actions = [[], [], []]\n",
        "\n",
        "    for s in range(1,num_S+1):\n",
        "        for c in range(1,num_C+1):\n",
        "            for p in range(1,num_P+1):\n",
        "                for r in range(1,num_R+1):\n",
        "                    for a in range(1,num_A+1):\n",
        "\n",
        "                        sequence_id = \\\n",
        "                                'S%03dC%03dP%03dR%03dA%03d' % (s, c, p, r, a)\n",
        "                        filename = os.path.join(ntud_numpy_dir,\n",
        "                                sequence_id + '.npy')\n",
        "                        if not os.path.isfile(filename):\n",
        "                            continue # Ignore missing annotation files\n",
        "\n",
        "                        if eval_mode == 'cs':\n",
        "                            mode = TRAIN_MODE if p in cs_train else TEST_MODE\n",
        "                        else:\n",
        "                            mode = TRAIN_MODE if c in cv_train else TEST_MODE\n",
        "\n",
        "                        data = np.load(filename)\n",
        "                        if video_subsample is not None:\n",
        "                            data = data[0::video_subsample, :]\n",
        "\n",
        "                        \"\"\"Compute some stats about the dataset.\"\"\"\n",
        "                        if len(data) < min_num_frames:\n",
        "                            min_num_frames = len(data)\n",
        "                        if len(data) > max_num_frames:\n",
        "                            max_num_frames = len(data)\n",
        "                        num_videos[mode] += 1\n",
        "\n",
        "                        sequences[mode].append(data)\n",
        "                        seq_ids[mode].append(sequence_id)\n",
        "                        actions[mode].append(a)\n",
        "\n",
        "    frame_idx = [serialize_index_sequences(sequences[0]),\n",
        "            serialize_index_sequences(sequences[1]), []]\n",
        "\n",
        "    printcn('', 'Max/Min number of frames: {}/{}'.format(\n",
        "        max_num_frames, min_num_frames))\n",
        "    printcn('', 'Number of videos: {}'.format(num_videos))\n",
        "\n",
        "    return sequences, frame_idx, seq_ids, actions\n",
        "\n",
        "\n",
        "class Ntu(object):\n",
        "    def __init__(self, dataset_path, dataconf, poselayout=pa20j3d,\n",
        "            topology='sequence', use_gt_bbox=False, remove_outer_joints=True,\n",
        "            clip_size=16, pose_only=False, num_S=17):\n",
        "\n",
        "        self.dataset_path = dataset_path\n",
        "        self.dataconf = dataconf\n",
        "        self.poselayout = poselayout\n",
        "        self.topology = topology\n",
        "        self.use_gt_bbox = use_gt_bbox\n",
        "        self.clip_size = clip_size\n",
        "        self.remove_outer_joints = remove_outer_joints\n",
        "        self.pose_only = pose_only\n",
        "        self.action_labels = ACTION_LABELS\n",
        "        self.joint_labels = JOINT_LABELS\n",
        "\n",
        "        try:\n",
        "            self.sequences, self.frame_idx, self.seq_ids, self.actions = \\\n",
        "                    ntu_load_annotations(dataset_path, num_S=num_S)\n",
        "        except:\n",
        "            warning('Error loading NTU RGB+D dataset!')\n",
        "            raise\n",
        "\n",
        "    def get_data(self, key, mode, frame_list=None, bbox=None):\n",
        "        \"\"\"Method to load NTU samples specified by mode and key,\n",
        "        do data augmentation and bounding box cropping.\n",
        "        \"\"\"\n",
        "        output = {}\n",
        "\n",
        "        if mode == TRAIN_MODE:\n",
        "            dconf = self.dataconf.random_data_generator()\n",
        "            random_clip = True\n",
        "        else:\n",
        "            dconf = self.dataconf.get_fixed_config()\n",
        "            random_clip = False\n",
        "\n",
        "        if self.topology == 'sequences':\n",
        "            seq_idx = key\n",
        "            seq = self.sequences[mode][seq_idx]\n",
        "            seq_id = self.seq_ids[mode][seq_idx]\n",
        "            act = self.actions[mode][seq_idx]\n",
        "            if frame_list == None:\n",
        "                frame_list = get_clip_frame_index(len(seq), dconf['subspl'],self.clip_size, random_clip=random_clip)\n",
        "        else:\n",
        "            seq_idx, frame_idx = self.frame_idx[mode][key]\n",
        "            seq = self.sequences[mode][seq_idx]\n",
        "            seq_id = self.seq_ids[mode][seq_idx]\n",
        "            act = self.actions[mode][seq_idx]\n",
        "            frame_list = [frame_idx]\n",
        "\n",
        "        objframes = seq[frame_list]\n",
        "\n",
        "        \"\"\"Load pose annotation\"\"\"\n",
        "        pose, visible = self.get_pose_annot(objframes)\n",
        "\n",
        "        if use_small_images:\n",
        "            w, h = (int(1920/2), int(1080/2))\n",
        "        else:\n",
        "            w, h = (1920, 1080)\n",
        "\n",
        "        \"\"\"Compute the ground truth bounding box, if not given\"\"\"\n",
        "        if bbox is None:\n",
        "            if self.use_gt_bbox:\n",
        "                bbox = get_gt_bbox(pose[:, :, 0:2], visible, (w, h),\n",
        "                        scale=dconf['scale'], logkey=key)\n",
        "            else:\n",
        "                bbox = objposwin_to_bbox(np.array([w / 2, h / 2]),\n",
        "                        (dconf['scale']*max(w, h), dconf['scale']*max(w, h)))\n",
        "\n",
        "        rootz = np.nanmean(pose[:, 0, 2])\n",
        "        if np.isnan(rootz):\n",
        "            rootz = np.nanmean(pose[:, :, 2], axis=(0, 1))\n",
        "\n",
        "        zrange = np.array([rootz - dconf['scale']*1000,\n",
        "            rootz + dconf['scale']*1000])\n",
        "\n",
        "        objpos, winsize = bbox_to_objposwin(bbox)\n",
        "        if min(winsize) < 32:\n",
        "            winsize = (32, 32)\n",
        "        objpos += dconf['scale'] * np.array([dconf['transx'], dconf['transy']])\n",
        "\n",
        "        \"\"\"Pre-process data for each frame\"\"\"\n",
        "        if self.pose_only:\n",
        "            frames = None\n",
        "        else:\n",
        "            frames = np.zeros((len(objframes),) + self.dataconf.input_shape)\n",
        "\n",
        "        for i in range(len(objframes)):\n",
        "            if self.pose_only:\n",
        "                imgt = T(None, img_size=(w, h))\n",
        "            else:\n",
        "                imagepath = os.path.join(self.dataset_path, image_prefix,\n",
        "                        seq_id, '%05d.jpg' % objframes[i][0])\n",
        "                imgt = T(Image.open(imagepath))\n",
        "\n",
        "            imgt.rotate_crop(dconf['angle'], objpos, winsize)\n",
        "            imgt.resize(self.dataconf.crop_resolution)\n",
        "\n",
        "            if dconf['hflip'] == 1:\n",
        "                imgt.horizontal_flip()\n",
        "\n",
        "            imgt.normalize_affinemap()\n",
        "            if not self.pose_only:\n",
        "                frames[i, :, :, :] = normalize_channels(imgt.asarray(),\n",
        "                        channel_power=dconf['chpower'])\n",
        "\n",
        "            pose[i, :, 0:2] = transform_2d_points(imgt.afmat, pose[i, :, 0:2],\n",
        "                    transpose=True)\n",
        "            pose[i, :, 2] = (pose[i, :, 2] -zrange[0]) / (zrange[1] -zrange[0])\n",
        "\n",
        "            if imgt.hflip:\n",
        "                pose[i, :, :] = pose[i, self.poselayout.map_hflip, :]\n",
        "\n",
        "        \"\"\"Set outsider body joints to invalid (-1e9).\"\"\"\n",
        "        pose = np.reshape(pose, (-1, self.poselayout.dim))\n",
        "        pose[np.isnan(pose)] = -1e9\n",
        "        v = np.expand_dims(get_visible_joints(pose[:,0:2]), axis=-1)\n",
        "        pose[(v==0)[:,0],:] = -1e9\n",
        "        pose = np.reshape(pose, (len(objframes), self.poselayout.num_joints,\n",
        "            self.poselayout.dim))\n",
        "        v = np.reshape(v, (len(objframes), self.poselayout.num_joints, 1))\n",
        "\n",
        "        pose = np.concatenate((pose, v), axis=-1)\n",
        "        if self.topology != 'sequences':\n",
        "            pose = np.squeeze(pose, axis=0)\n",
        "            if not self.pose_only:\n",
        "                frames = np.squeeze(frames, axis=0)\n",
        "\n",
        "        action = np.zeros(self.get_shape('ntuaction'))\n",
        "        action[act - 1] = 1.\n",
        "\n",
        "        output['seq_idx'] = seq_idx\n",
        "        output['frame_list'] = frame_list\n",
        "        output['ntuaction'] = action\n",
        "        output['pennaction'] = np.zeros((15,))\n",
        "        output['pose'] = pose\n",
        "        output['frame'] = frames\n",
        "\n",
        "        \"\"\"Take the last transformation matrix, it should not change\"\"\"\n",
        "        output['afmat'] = imgt.afmat.copy()\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def get_pose_annot(self, frames):\n",
        "\n",
        "        num_joints = len(JOINT_LABELS)\n",
        "        pose = frames[:, 1+3*num_joints:]\n",
        "\n",
        "        p = np.zeros((len(frames), num_joints, self.poselayout.dim))\n",
        "\n",
        "        if use_small_images:\n",
        "            p[:, :, 0] = pose[:, 0:num_joints] / 2.\n",
        "            p[:, :, 1] = pose[:, num_joints:2*num_joints] / 2.\n",
        "        else:\n",
        "            p[:, :, 0] = pose[:, 0:num_joints]\n",
        "            p[:, :, 1] = pose[:, num_joints:2*num_joints]\n",
        "\n",
        "        if self.poselayout.dim == 3:\n",
        "            p[:, :, 2] = pose[:, 2*num_joints:]\n",
        "\n",
        "        p = p[:, self.poselayout.map_from_ntu, :].copy()\n",
        "        v = np.apply_along_axis(lambda x: 1 if x.all() else 0,\n",
        "                axis=2, arr=(p > 0))\n",
        "        p[v==0, :] = np.nan\n",
        "\n",
        "        return p, v\n",
        "\n",
        "    def get_clip_index(self, key, mode, subsamples=[2]):\n",
        "        assert self.topology == 'sequences', 'Topology not supported'\n",
        "\n",
        "        seq = self.sequences[mode][key]\n",
        "        index_list = []\n",
        "        for sub in subsamples:\n",
        "            start_frame = 0\n",
        "            while True:\n",
        "                last_frame = start_frame + self.clip_size * sub\n",
        "                if last_frame > len(seq):\n",
        "                    break\n",
        "                index_list.append(range(start_frame, last_frame, sub))\n",
        "                start_frame += int(self.clip_size / 2) + (sub - 1)\n",
        "\n",
        "        return index_list\n",
        "\n",
        "\n",
        "    def clip_length(self):\n",
        "        if self.topology == 'sequences':\n",
        "            return self.clip_size\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def clip_shape(self):\n",
        "        if self.topology == 'sequences':\n",
        "            return (self.clip_size,)\n",
        "        else:\n",
        "            return ()\n",
        "\n",
        "    def get_shape(self, dictkey):\n",
        "        if dictkey == 'frame':\n",
        "            return self.clip_shape() + self.dataconf.input_shape\n",
        "        if dictkey == 'pose':\n",
        "            return self.clip_shape() \\\n",
        "                    + (self.poselayout.num_joints, self.poselayout.dim+1)\n",
        "        if dictkey == 'ntuaction':\n",
        "            return (len(self.action_labels),)\n",
        "        if dictkey == 'pennaction':\n",
        "            return (15,)\n",
        "        if dictkey == 'afmat':\n",
        "            return (3, 3)\n",
        "        raise Exception('Invalid dictkey on get_shape!')\n",
        "\n",
        "    def get_length(self, mode):\n",
        "        if self.topology == 'sequences':\n",
        "            return len(self.sequences[mode])\n",
        "        else:\n",
        "            return len(self.frame_idx[mode])\n"
      ],
      "metadata": {
        "id": "8tRy0OjYU7Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ACTION_LABELS = None\n",
        "\n",
        "def load_pennaction_mat_annotation(filename):\n",
        "    mat = sio.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
        "\n",
        "    # Respect the order of TEST (0), TRAIN (1). No validation set.\n",
        "    sequences = [mat['sequences_te'], mat['sequences_tr'], []]\n",
        "    action_labels = mat['action_labels']\n",
        "    joint_labels = mat['joint_labels']\n",
        "\n",
        "    return sequences, action_labels, joint_labels\n",
        "\n",
        "\n",
        "def serialize_index_sequences(sequences):\n",
        "    frame_idx = []\n",
        "    for s in range(len(sequences)):\n",
        "        for f in range(len(sequences[s].frames)):\n",
        "            frame_idx.append((s, f))\n",
        "\n",
        "    return frame_idx\n",
        "\n",
        "\n",
        "def compute_clip_bbox(bbox_dict, seq_idx, frame_list):\n",
        "    x1 = y1 = np.inf\n",
        "    x2 = y2 = -np.inf\n",
        "\n",
        "    for f in frame_list:\n",
        "        b = bbox_dict['%d.%d' % (seq_idx, f)]\n",
        "        x1 = min(x1, b[0])\n",
        "        y1 = min(y1, b[1])\n",
        "        x2 = max(x2, b[2])\n",
        "        y2 = max(y2, b[3])\n",
        "\n",
        "    return np.array([x1, y1, x2, y2])\n",
        "\n",
        "\n",
        "class PennAction(object):\n",
        "    def __init__(self, dataset_path, dataconf, poselayout=pa16j2d,\n",
        "            topology='sequence', use_gt_bbox=False, remove_outer_joints=True,\n",
        "            clip_size=16, pose_only=False, output_fullframe=False,\n",
        "            pred_bboxes_file=None):\n",
        "\n",
        "        assert topology in ['sequences', 'frames'], \\\n",
        "                'Invalid topology ({})'.format(topology)\n",
        "\n",
        "        self.dataset_path = dataset_path\n",
        "        self.dataconf = dataconf\n",
        "        self.poselayout = poselayout\n",
        "        self.topology = topology\n",
        "        self.use_gt_bbox = use_gt_bbox\n",
        "        self.remove_outer_joints = remove_outer_joints\n",
        "        self.clip_size = clip_size\n",
        "        self.pose_only = pose_only\n",
        "        self.output_fullframe = output_fullframe\n",
        "        self.load_annotations(os.path.join(dataset_path, 'annotations.mat'))\n",
        "        if pred_bboxes_file:\n",
        "            filepath = os.path.join(dataset_path, pred_bboxes_file)\n",
        "            with open(filepath, 'r') as fid:\n",
        "                self.pred_bboxes = json.load(fid)\n",
        "        else:\n",
        "            self.pred_bboxes = None\n",
        "\n",
        "    def load_annotations(self, filename):\n",
        "        try:\n",
        "            self.sequences, self.action_labels, self.joint_labels = \\\n",
        "                    load_pennaction_mat_annotation(filename)\n",
        "            self.frame_idx = [serialize_index_sequences(self.sequences[0]),\n",
        "                    serialize_index_sequences(self.sequences[1]), []]\n",
        "\n",
        "            global ACTION_LABELS\n",
        "            ACTION_LABELS = self.action_labels\n",
        "\n",
        "        except:\n",
        "            warning('Error loading PennAction dataset!')\n",
        "            raise\n",
        "\n",
        "    def get_data(self, key, mode, frame_list=None, bbox=None):\n",
        "        \"\"\"Method to load Penn Action samples specified by mode and key,\n",
        "        do data augmentation and bounding box cropping.\n",
        "        \"\"\"\n",
        "        output = {}\n",
        "\n",
        "        if mode == TRAIN_MODE:\n",
        "            dconf = self.dataconf.random_data_generator()\n",
        "            random_clip = True\n",
        "        else:\n",
        "            dconf = self.dataconf.get_fixed_config()\n",
        "            random_clip = False\n",
        "\n",
        "        if self.topology == 'sequences':\n",
        "            seq_idx = key\n",
        "            seq = self.sequences[mode][seq_idx]\n",
        "            if frame_list == None:\n",
        "                frame_list = get_clip_frame_index(len(seq.frames),\n",
        "                        dconf['subspl'], self.clip_size,\n",
        "                        random_clip=random_clip)\n",
        "        else:\n",
        "            seq_idx, frame_idx = self.frame_idx[mode][key]\n",
        "            seq = self.sequences[mode][seq_idx]\n",
        "            frame_list = [frame_idx]\n",
        "\n",
        "        objframes = seq.frames[frame_list]\n",
        "\n",
        "        \"\"\"Load pose annotation\"\"\"\n",
        "        pose, visible = self.get_pose_annot(objframes)\n",
        "        w, h = (objframes[0].w, objframes[0].h)\n",
        "\n",
        "        \"\"\"Compute cropping bounding box, if not given.\"\"\"\n",
        "        if bbox is None:\n",
        "\n",
        "            if self.use_gt_bbox:\n",
        "                bbox = get_gt_bbox(pose[:, :, 0:2], visible, (w, h),\n",
        "                        scale=dconf['scale'], logkey=key)\n",
        "\n",
        "            elif self.pred_bboxes:\n",
        "                bbox = compute_clip_bbox(\n",
        "                        self.pred_bboxes[mode], seq_idx, frame_list)\n",
        "\n",
        "            else:\n",
        "                bbox = objposwin_to_bbox(np.array([w / 2, h / 2]),\n",
        "                        (dconf['scale']*max(w, h), dconf['scale']*max(w, h)))\n",
        "\n",
        "        objpos, winsize = bbox_to_objposwin(bbox)\n",
        "        if min(winsize) < 32:\n",
        "            winsize = (32, 32)\n",
        "        objpos += dconf['scale'] * np.array([dconf['transx'], dconf['transy']])\n",
        "\n",
        "        \"\"\"Pre-process data for each frame\"\"\"\n",
        "        if self.pose_only:\n",
        "            frames = None\n",
        "        else:\n",
        "            frames = np.zeros((len(objframes),) + self.dataconf.input_shape)\n",
        "            if self.output_fullframe:\n",
        "                fullframes = np.zeros((len(objframes), h, w,\n",
        "                    self.dataconf.input_shape[-1]))\n",
        "\n",
        "        for i in range(len(objframes)):\n",
        "            if self.pose_only:\n",
        "                imgt = T(None, img_size=(w, h))\n",
        "            else:\n",
        "                image = 'frames/%04d/%06d.jpg' % (seq.idx, objframes[i].f)\n",
        "                imgt = T(Image.open(os.path.join(self.dataset_path, image)))\n",
        "                if self.output_fullframe:\n",
        "                    fullframes[i, :, :, :] = normalize_channels(imgt.asarray(),\n",
        "                            channel_power=dconf['chpower'])\n",
        "\n",
        "            imgt.rotate_crop(dconf['angle'], objpos, winsize)\n",
        "            imgt.resize(self.dataconf.crop_resolution)\n",
        "\n",
        "            if dconf['hflip'] == 1:\n",
        "                imgt.horizontal_flip()\n",
        "\n",
        "            imgt.normalize_affinemap()\n",
        "            if not self.pose_only:\n",
        "                frames[i, :, :, :] = normalize_channels(imgt.asarray(),\n",
        "                        channel_power=dconf['chpower'])\n",
        "\n",
        "            pose[i, :, 0:2] = transform_2d_points(imgt.afmat, pose[i, :, 0:2],\n",
        "                    transpose=True)\n",
        "            if imgt.hflip:\n",
        "                pose[i, :, :] = pose[i, self.poselayout.map_hflip, :]\n",
        "\n",
        "        \"\"\"Set outsider body joints to invalid (-1e9).\"\"\"\n",
        "        pose = np.reshape(pose, (-1, self.poselayout.dim))\n",
        "        pose[np.isnan(pose)] = -1e9\n",
        "        v = np.expand_dims(get_visible_joints(pose[:,0:2]), axis=-1)\n",
        "        pose[(v==0)[:,0],:] = -1e9\n",
        "        pose = np.reshape(pose, (len(objframes), self.poselayout.num_joints,\n",
        "            self.poselayout.dim))\n",
        "        v = np.reshape(v, (len(objframes), self.poselayout.num_joints, 1))\n",
        "\n",
        "        pose = np.concatenate((pose, v), axis=-1)\n",
        "        if self.topology != 'sequences':\n",
        "            pose = np.squeeze(pose, axis=0)\n",
        "            if not self.pose_only:\n",
        "                frames = np.squeeze(frames, axis=0)\n",
        "\n",
        "        action = np.zeros(self.get_shape('pennaction'))\n",
        "        action[seq.action_id - 1] = 1.\n",
        "\n",
        "        output['seq_idx'] = seq_idx\n",
        "        output['frame_list'] = frame_list\n",
        "        output['pennaction'] = action\n",
        "        output['ntuaction'] = np.zeros((60,))\n",
        "        output['pose'] = pose\n",
        "        output['frame'] = frames\n",
        "        if self.output_fullframe and not self.pose_only:\n",
        "            output['fullframe'] = fullframes\n",
        "\n",
        "        output['bbox'] = bbox\n",
        "\n",
        "        \"\"\"Take the last transformation matrix, it should not change\"\"\"\n",
        "        output['afmat'] = imgt.afmat.copy()\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_clip_index(self, key, mode, subsamples=[2]):\n",
        "        assert self.topology == 'sequences', 'Topology not supported'\n",
        "\n",
        "        seq = self.sequences[mode][key]\n",
        "        index_list = []\n",
        "        for sub in subsamples:\n",
        "            start_frame = 0\n",
        "            while True:\n",
        "                last_frame = start_frame + self.clip_size * sub\n",
        "                if last_frame > len(seq.frames):\n",
        "                    break\n",
        "                index_list.append(range(start_frame, last_frame, sub))\n",
        "                start_frame += int(self.clip_size / 2) + (sub - 1)\n",
        "\n",
        "        return index_list\n",
        "\n",
        "\n",
        "    def get_pose_annot(self, frames):\n",
        "        p = np.nan * np.ones((len(frames), self.poselayout.num_joints,\n",
        "            self.poselayout.dim))\n",
        "        v = np.zeros((len(frames), self.poselayout.num_joints))\n",
        "        for i in range(len(frames)):\n",
        "            p[i, self.poselayout.map_to_pa13j, 0:2] = frames[i].pose.copy().T\n",
        "            v[i, self.poselayout.map_to_pa13j] = frames[i].visible.copy()\n",
        "            p[i, v[i] == 0, :] = np.nan\n",
        "            p[i, p[i] == 0] = np.nan\n",
        "\n",
        "        return p, v\n",
        "\n",
        "    def clip_length(self):\n",
        "        if self.topology == 'sequences':\n",
        "            return self.clip_size\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def clip_shape(self):\n",
        "        if self.topology == 'sequences':\n",
        "            return (self.clip_size,)\n",
        "        else:\n",
        "            return ()\n",
        "\n",
        "    def get_shape(self, dictkey):\n",
        "        if dictkey == 'frame':\n",
        "            return self.clip_shape() + self.dataconf.input_shape\n",
        "        if dictkey == 'pose':\n",
        "            return self.clip_shape() \\\n",
        "                    + (self.poselayout.num_joints, self.poselayout.dim+1)\n",
        "        if dictkey == 'pennaction':\n",
        "            return (len(self.action_labels),)\n",
        "        if dictkey == 'ntuaction':\n",
        "            return (60,)\n",
        "        if dictkey == 'afmat':\n",
        "            return (3, 3)\n",
        "        raise Exception('Invalid dictkey ({}) on get_shape!'.format(dictkey))\n",
        "\n",
        "    def get_length(self, mode):\n",
        "        if self.topology == 'sequences':\n",
        "            return len(self.sequences[mode])\n",
        "        else:\n",
        "            return len(self.frame_idx[mode])\n"
      ],
      "metadata": {
        "id": "Ky7pStfuU-al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic Models before training**"
      ],
      "metadata": {
        "id": "QAxUb7CeVFs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def channel_softmax_1d():\n",
        "\n",
        "    def _channel_softmax_1d(x):\n",
        "        ndim = K.ndim(x)\n",
        "        if ndim == 3:\n",
        "            e = K.exp(x - K.max(x, axis=(1,), keepdims=True))\n",
        "            s = K.sum(e, axis=(1,), keepdims=True)\n",
        "            return e / s\n",
        "        else:\n",
        "            raise ValueError('This function is specific for 3D tensors. '\n",
        "                    'Here, ndim=' + str(ndim))\n",
        "\n",
        "    return _channel_softmax_1d\n",
        "\n",
        "\n",
        "\n",
        "def relu(x, leakyrelu=False, name=None):\n",
        "    if leakyrelu:\n",
        "        return LeakyReLU(alpha=0.1)(x)\n",
        "    else:\n",
        "        return Activation('relu', name=name)(x)\n",
        "\n",
        "\n",
        "def localconv1d(x, filters, kernel_size, strides=1, use_bias=True, name=None):\n",
        "    \"\"\"LocallyConnected1D possibly wrapped by a TimeDistributed layer.\"\"\"\n",
        "    f = LocallyConnected1D(filters, kernel_size, strides=strides,\n",
        "            use_bias=use_bias, name=name)\n",
        "\n",
        "    return TimeDistributed(f, name=name)(x) if K.ndim(x) == 4 else f(x)\n",
        "\n",
        "\n",
        "def conv2d(x, filters, kernel_size, strides=(1, 1), padding='same', name=None):\n",
        "    \"\"\"Conv2D possibly wrapped by a TimeDistributed layer.\"\"\"\n",
        "    f = Conv2D(filters, kernel_size, strides=strides, padding=padding,\n",
        "            use_bias=False, name=name)\n",
        "\n",
        "    return TimeDistributed(f, name=name)(x) if K.ndim(x) == 5 else f(x)\n",
        "\n",
        "\n",
        "def sepconv2d(x, filters, kernel_size, strides=(1, 1), padding='same',\n",
        "        name=None):\n",
        "    \"\"\"SeparableConv2D possibly wrapped by a TimeDistributed layer.\"\"\"\n",
        "    f = SeparableConv2D(filters, kernel_size, strides=strides, padding=padding,\n",
        "            use_bias=False, name=name)\n",
        "\n",
        "    return TimeDistributed(f, name=name)(x) if K.ndim(x) == 5 else f(x)\n",
        "\n",
        "\n",
        "def conv2dtranspose(x, filters, kernel_size, strides=(1, 1), padding='same',\n",
        "        name=None):\n",
        "    \"\"\"Conv2DTranspose possibly wrapped by a TimeDistributed layer.\"\"\"\n",
        "    f = Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding,\n",
        "            use_bias=False, name=name)\n",
        "\n",
        "    return TimeDistributed(f, name=name)(x) if K.ndim(x) == 5 else f(x)\n",
        "\n",
        "\n",
        "def maxpooling2d(x, kernel_size=(2, 2), strides=(2, 2), padding='same',\n",
        "        name=None):\n",
        "    \"\"\"MaxPooling2D possibly wrapped by a TimeDistributed layer.\"\"\"\n",
        "    f = MaxPooling2D(kernel_size, strides=strides, padding=padding, name=name)\n",
        "\n",
        "    return TimeDistributed(f, name=name)(x) if K.ndim(x) == 5 else f(x)\n",
        "\n",
        "\n",
        "def upsampling2d(x, kernel_size=(2, 2), name=None):\n",
        "    \"\"\"UpSampling2D possibly wrapped by a TimeDistributed layer.\"\"\"\n",
        "    f = UpSampling2D(kernel_size, name=name)\n",
        "\n",
        "    return TimeDistributed(f, name=name)(x) if K.ndim(x) == 5 else f(x)\n",
        "\n",
        "\n",
        "def keypoint_confidence(x, name=None):\n",
        "    \"\"\"Implements the keypoint (body joint) confidence, given a set of\n",
        "    probability maps as input. No parameters required.\n",
        "    \"\"\"\n",
        "    def _keypoint_confidence(x):\n",
        "        x = 4 * AveragePooling2D((2, 2), strides=(1, 1))(x)\n",
        "        x = K.expand_dims(GlobalMaxPooling2D()(x), axis=-1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    f = Lambda(_keypoint_confidence, name=name)\n",
        "\n",
        "    return TimeDistributed(f, name=name)(x) if K.ndim(x) == 5 else f(x)\n",
        "\n",
        "\n",
        "def softargmax2d(x, limits=(0, 0, 1, 1), name=None):\n",
        "    x_x = lin_interpolation_2d(x, axis=0, vmin=limits[0], vmax=limits[2],\n",
        "            name=appstr(name, '_x'))\n",
        "    x_y = lin_interpolation_2d(x, axis=1, vmin=limits[1], vmax=limits[3],\n",
        "            name=appstr(name, '_y'))\n",
        "    x = concatenate([x_x, x_y], name=name)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def lin_interpolation_1d(inp):\n",
        "\n",
        "    depth, num_filters = K.int_shape(inp)[1:]\n",
        "    conv = Conv1D(num_filters, depth, use_bias=False)\n",
        "    x = conv(inp)\n",
        "\n",
        "    w = conv.get_weights()\n",
        "    w[0].fill(0)\n",
        "\n",
        "    start = 1/(2*depth)\n",
        "    end = 1 - start\n",
        "    linspace = np.linspace(start, end, num=depth)\n",
        "\n",
        "    for i in range(num_filters):\n",
        "        w[0][:, i, i] = linspace[:]\n",
        "\n",
        "    conv.set_weights(w)\n",
        "    conv.trainable = False\n",
        "\n",
        "    def _traspose(x):\n",
        "       x = K.squeeze(x, axis=-2)\n",
        "       x = K.expand_dims(x, axis=-1)\n",
        "       return x\n",
        "    x = Lambda(_traspose)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def lin_interpolation_2d(x, axis, vmin=0., vmax=1., name=None):\n",
        "    \"\"\"Implements a 2D linear interpolation using a depth size separable\n",
        "    convolution (non trainable).\n",
        "    \"\"\"\n",
        "    assert K.ndim(x) in [4, 5], \\\n",
        "            'Input tensor must have ndim 4 or 5 ({})'.format(K.ndim(x))\n",
        "\n",
        "    if 'global_sam_cnt' not in globals():\n",
        "        global global_sam_cnt\n",
        "        global_sam_cnt = 0\n",
        "\n",
        "    if name is None:\n",
        "        name = 'custom_sam_%d' % global_sam_cnt\n",
        "        global_sam_cnt += 1\n",
        "\n",
        "    if K.ndim(x) == 4:\n",
        "        num_rows, num_cols, num_filters = K.int_shape(x)[1:]\n",
        "    else:\n",
        "        num_rows, num_cols, num_filters = K.int_shape(x)[2:]\n",
        "\n",
        "    f = SeparableConv2D(num_filters, (num_rows, num_cols), use_bias=False,\n",
        "            name=name)\n",
        "    x = TimeDistributed(f, name=name)(x) if K.ndim(x) == 5 else f(x)\n",
        "\n",
        "    w = f.get_weights()\n",
        "    w[0].fill(0)\n",
        "    w[1].fill(0)\n",
        "    linspace = linspace_2d(num_rows, num_cols, dim=axis)\n",
        "\n",
        "    for i in range(num_filters):\n",
        "        w[0][:,:, i, 0] = linspace[:,:]\n",
        "        w[1][0, 0, i, i] = 1.\n",
        "\n",
        "    f.set_weights(w)\n",
        "    f.trainable = False\n",
        "\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=-2))(x)\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=-2))(x)\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=-1))(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def conv_bn(x, filters, size, strides=(1, 1), padding='same', name=None):\n",
        "    if name is not None:\n",
        "        conv_name = name + '_conv'\n",
        "    else:\n",
        "        conv_name = None\n",
        "\n",
        "    x = conv(x, filters, size, strides, padding, conv_name)\n",
        "    x = BatchNormalization(axis=-1, scale=False, name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def deconv(x, filters, size, strides=(1, 1), padding='same', name=None):\n",
        "    x = Conv2DTranspose(filters, size, strides=strides, padding=padding,\n",
        "            data_format=K.image_data_format(), use_bias=False, name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def conv_act(x, filters, size, strides=(1, 1), padding='same', name=None):\n",
        "    if name is not None:\n",
        "        conv_name = name + '_conv'\n",
        "    else:\n",
        "        conv_name = None\n",
        "\n",
        "    x = conv(x, filters, size, strides, padding, conv_name)\n",
        "    x = Activation('relu', name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def conv_bn_act(x, filters, size, strides=(1, 1), padding='same', name=None):\n",
        "    if name is not None:\n",
        "        conv_name = name + '_conv'\n",
        "        bn_name = name + '_bn'\n",
        "    else:\n",
        "        conv_name = None\n",
        "        bn_name = None\n",
        "\n",
        "    x = conv(x, filters, size, strides, padding, conv_name)\n",
        "    x = BatchNormalization(axis=-1, scale=False, name=bn_name)(x)\n",
        "    x = Activation('relu', name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def bn_act_conv(x, filters, size, strides=(1, 1), padding='same', name=None):\n",
        "    if name is not None:\n",
        "        bn_name = name + '_bn'\n",
        "        act_name = name + '_act'\n",
        "    else:\n",
        "        bn_name = None\n",
        "        act_name = None\n",
        "\n",
        "    x = BatchNormalization(axis=-1, scale=False, name=bn_name)(x)\n",
        "    x = Activation('relu', name=act_name)(x)\n",
        "    x = conv(x, filters, size, strides, padding, name)\n",
        "    return x\n",
        "\n",
        "\n",
        "def act_conv_bn(x, filters, size, strides=(1, 1), padding='same', name=None):\n",
        "    if name is not None:\n",
        "        conv_name = name + '_conv'\n",
        "        act_name = name + '_act'\n",
        "    else:\n",
        "        conv_name = None\n",
        "        act_name = None\n",
        "\n",
        "    x = Activation('relu', name=act_name)(x)\n",
        "    x = conv(x, filters, size, strides, padding, conv_name)\n",
        "    x = BatchNormalization(axis=-1, scale=False, name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def separable_conv_bn_act(x, filters, size, strides=(1, 1), padding='same',\n",
        "        name=None):\n",
        "    if name is not None:\n",
        "        conv_name = name + '_conv'\n",
        "        bn_name = name + '_bn'\n",
        "    else:\n",
        "        conv_name = None\n",
        "        bn_name = None\n",
        "\n",
        "    x = SeparableConv2D(filters, size, strides=strides, padding=padding,\n",
        "            use_bias=False, name=conv_name)(x)\n",
        "    x = BatchNormalization(axis=-1, scale=False, name=bn_name)(x)\n",
        "    x = Activation('relu', name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def separable_act_conv_bn(x, filters, size, strides=(1, 1), padding='same',\n",
        "        name=None):\n",
        "    if name is not None:\n",
        "        conv_name = name + '_conv'\n",
        "        act_name = name + '_act'\n",
        "    else:\n",
        "        conv_name = None\n",
        "        act_name = None\n",
        "\n",
        "    x = Activation('relu', name=act_name)(x)\n",
        "    x = SeparableConv2D(filters, size, strides=strides, padding=padding,\n",
        "            use_bias=False, name=conv_name)(x)\n",
        "    x = BatchNormalization(axis=-1, scale=False, name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def separable_conv_bn(x, filters, size, strides=(1, 1), padding='same',\n",
        "        name=None):\n",
        "    if name is not None:\n",
        "        conv_name = name + '_conv'\n",
        "    else:\n",
        "        conv_name = None\n",
        "\n",
        "    x = SeparableConv2D(filters, size, strides=strides, padding=padding,\n",
        "            use_bias=False, name=conv_name)(x)\n",
        "    x = BatchNormalization(axis=-1, scale=False, name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def act_conv(x, filters, size, strides=(1, 1), padding='same', name=None):\n",
        "    if name is not None:\n",
        "        act_name = name + '_act'\n",
        "    else:\n",
        "        act_name = None\n",
        "\n",
        "    x = Activation('relu', name=act_name)(x)\n",
        "    x = conv(x, filters, size, strides, padding, name)\n",
        "    return x\n",
        "\n",
        "def bn_act_conv3d(x, filters, size, strides=(1, 1, 1), padding='same',\n",
        "        name=None):\n",
        "\n",
        "    if name is not None:\n",
        "        bn_name = name + '_bn'\n",
        "        act_name = name + '_act'\n",
        "    else:\n",
        "        bn_name = None\n",
        "        act_name = None\n",
        "\n",
        "    x = BatchNormalization(axis=-1, scale=False, name=bn_name)(x)\n",
        "    x = Activation('relu', name=act_name)(x)\n",
        "    x = Conv3D(filters, size, strides=strides, padding=padding,\n",
        "            use_bias=False, name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def dense(x, filters, name=None):\n",
        "    x = Dense(filters, kernel_regularizer=l1(0.001), name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def bn_act_dense(x, filters, name=None):\n",
        "    if name is not None:\n",
        "        bn_name = name + '_bn'\n",
        "        act_name = name + '_act'\n",
        "    else:\n",
        "        bn_name = None\n",
        "        act_name = None\n",
        "\n",
        "    x = BatchNormalization(axis=-1, scale=False, name=bn_name)(x)\n",
        "    x = Activation('relu', name=act_name)(x)\n",
        "    x = Dense(filters, kernel_regularizer=l1(0.001), name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def act_channel_softmax(x, name=None):\n",
        "    x = Activation(channel_softmax_2d(), name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def act_depth_softmax(x, name=None):\n",
        "    x = Activation(channel_softmax_1d(), name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def aggregate_position_probability(inp):\n",
        "    y,p = inp\n",
        "\n",
        "    p = concatenate([p, p], axis=-1)\n",
        "    yp = p * y\n",
        "    yn = (1 - p) * y\n",
        "    y = concatenate([yp, yn], axis=-1)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def fc_aggregation_block(y, p, name=None):\n",
        "    dim = K.int_shape(y)[-1]\n",
        "\n",
        "    x = Lambda(aggregate_position_probability, name=name)([y, p])\n",
        "    x = Dense(2*dim, use_bias=False, kernel_regularizer=l1(0.0002),\n",
        "            name=name + '_fc1')(x)\n",
        "    x = Activation('relu', name=name + '_act')(x)\n",
        "    x = Dense(dim, kernel_regularizer=l1(0.0002), name=name + '_fc2')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def sparse_fc_mapping(x, input_idxs):\n",
        "\n",
        "    num_units = len(input_idxs)\n",
        "    d = Dense(num_units, use_bias=False)\n",
        "    d.trainable = False\n",
        "    x = d(x)\n",
        "\n",
        "    w = d.get_weights()\n",
        "    w[0].fill(0)\n",
        "    for i in range(num_units):\n",
        "        w[0][input_idxs[i], i] = 1.\n",
        "    d.set_weights(w)\n",
        "\n",
        "    return x\n",
        "\n",
        "def max_min_pooling(x, strides=(2, 2), padding='same', name=None):\n",
        "    if 'max_min_pool_cnt' not in globals():\n",
        "        global max_min_pool_cnt\n",
        "        max_min_pool_cnt = 0\n",
        "\n",
        "    if name is None:\n",
        "        name = 'MaxMinPooling2D_%d' % max_min_pool_cnt\n",
        "        max_min_pool_cnt += 1\n",
        "\n",
        "    def _max_plus_min(x):\n",
        "        x1 = MaxPooling2D(strides, padding=padding)(x)\n",
        "        x2 = MaxPooling2D(strides, padding=padding)(-x)\n",
        "        return x1 - x2\n",
        "\n",
        "    return Lambda(_max_plus_min, name=name)(x)\n",
        "\n",
        "\n",
        "def global_max_min_pooling(x, name=None):\n",
        "    if 'global_max_min_pool_cnt' not in globals():\n",
        "        global global_max_min_pool_cnt\n",
        "        global_max_min_pool_cnt = 0\n",
        "\n",
        "    if name is None:\n",
        "        name = 'GlobalMaxMinPooling2D_%d' % global_max_min_pool_cnt\n",
        "        global_max_min_pool_cnt += 1\n",
        "\n",
        "    def _global_max_plus_min(x):\n",
        "        x1 = GlobalMaxPooling2D()(x)\n",
        "        x2 = GlobalMaxPooling2D()(-x)\n",
        "        return x1 - x2\n",
        "\n",
        "    return Lambda(_global_max_plus_min, name=name)(x)\n",
        "\n",
        "\n",
        "def kl_divergence_regularizer(x, rho=0.01):\n",
        "\n",
        "    def _kl_regularizer(y_pred):\n",
        "        _, rows, cols, _ = K.int_shape(y_pred)\n",
        "        vmax = K.max(y_pred, axis=(1, 2))\n",
        "        vmax = K.expand_dims(vmax, axis=(1))\n",
        "        vmax = K.expand_dims(vmax, axis=(1))\n",
        "        vmax = K.tile(vmax, [1, rows, cols, 1])\n",
        "        y_delta = K.cast(K.greater_equal(y_pred, vmax), 'float32')\n",
        "        return rho * K.sum(y_pred *\n",
        "                (K.log(K.clip(y_pred, K.epsilon(), 1.))\n",
        "                - K.log(K.clip(y_delta, K.epsilon(), 1.))) / (rows * cols)\n",
        "            )\n",
        "\n",
        "    # Build an auxiliary non trainable layer, just to use the activity reg.\n",
        "    num_filters = K.int_shape(x)[-1]\n",
        "    aux_conv = Conv2D(num_filters, (1, 1), use_bias=False,\n",
        "            activity_regularizer=_kl_regularizer)\n",
        "    aux_conv.trainable = False\n",
        "    x = aux_conv(x)\n",
        "\n",
        "    # Set identity weights\n",
        "    w = aux_conv.get_weights()\n",
        "    w[0].fill(0)\n",
        "\n",
        "    for i in range(num_filters):\n",
        "        w[0][0,0,i,i] = 1.\n",
        "\n",
        "    aux_conv.set_weights(w)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def kronecker_prod(h, f, name='Kronecker_prod'):\n",
        "    \"\"\" # Inputs: inp[0] (heatmaps) and inp[1] (visual features)\n",
        "    \"\"\"\n",
        "    inp = [h, f]\n",
        "    def _combine_heatmaps_visual(inp):\n",
        "        hm = inp[0]\n",
        "        x = inp[1]\n",
        "        nj = K.int_shape(hm)[-1]\n",
        "        nf = K.int_shape(x)[-1]\n",
        "        hm = K.expand_dims(hm, axis=-1)\n",
        "        if len(K.int_shape(hm)) == 6:\n",
        "            hm = K.tile(hm, [1, 1, 1, 1, 1, nf])\n",
        "        elif len(K.int_shape(hm)) == 5:\n",
        "            hm = K.tile(hm, [1, 1, 1, 1, nf])\n",
        "        else:\n",
        "            raise ValueError(f'Invalid heatmap shape {hm}')\n",
        "\n",
        "        x = K.expand_dims(x, axis=-2)\n",
        "        if len(K.int_shape(x)) == 6:\n",
        "            x = K.tile(x, [1, 1, 1, 1, nj, 1])\n",
        "        elif len(K.int_shape(x)) == 5:\n",
        "            x = K.tile(x, [1, 1, 1, nj, 1])\n",
        "        else:\n",
        "            raise ValueError(f'Invalid featuremap shape {x}')\n",
        "\n",
        "        x = hm * x\n",
        "        x = K.sum(x, axis=(2, 3))\n",
        "\n",
        "        return x\n",
        "\n",
        "    return Lambda(_combine_heatmaps_visual, name=name)(inp)\n",
        "\n",
        "\n",
        "# Aliases.\n",
        "conv = conv2d"
      ],
      "metadata": {
        "id": "J2ZtJIXZVBpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def channel_softmax_2d(alpha=1):\n",
        "\n",
        "    def _channel_softmax_2d(x):\n",
        "        assert K.ndim(x) in [4, 5], \\\n",
        "                'Input tensor must have ndim 4 or 5 ({})'.format(K.ndim(x))\n",
        "\n",
        "        if alpha != 1:\n",
        "            x = alpha * x\n",
        "        e = K.exp(x - K.max(x, axis=(-3, -2), keepdims=True))\n",
        "        s = K.clip(K.sum(e, axis=(-3, -2), keepdims=True), K.epsilon(), None)\n",
        "\n",
        "        return e / s\n",
        "\n",
        "    return _channel_softmax_2d\n",
        "\n",
        "\n",
        "def build_context_aggregation(num_joints, num_context, alpha,\n",
        "        num_frames=1, name=None):\n",
        "\n",
        "    inp = Input(shape=(num_joints * num_context, 1))\n",
        "    d = Dense(num_joints, use_bias=False)\n",
        "\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=-1))(inp)\n",
        "    x = d(x)\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=-1))(x)\n",
        "\n",
        "    w = d.get_weights()\n",
        "    w[0].fill(0)\n",
        "    for j in range(num_joints):\n",
        "        start = j*num_context\n",
        "        w[0][j * num_context : (j + 1) * num_context, j] = 1.\n",
        "    d.set_weights(w)\n",
        "    d.trainable = False\n",
        "\n",
        "    ctx_sum = Model(inputs=inp, outputs=x)\n",
        "    ctx_sum.trainable = False\n",
        "    if num_frames > 1:\n",
        "        ctx_sum = TimeDistributed(ctx_sum,\n",
        "                input_shape=(num_frames,) + K.int_shape(inp)[1:])\n",
        "\n",
        "    # Define auxiliary layers.\n",
        "    mul_alpha = Lambda(lambda x: alpha * x)\n",
        "    mul_1alpha = Lambda(lambda x: (1 - alpha) * x)\n",
        "\n",
        "    # This depends on TensorFlow because keras does not implement divide.\n",
        "    tf_div = Lambda(lambda x: tf.divide(x[0], x[1]))\n",
        "\n",
        "    if num_frames == 1:\n",
        "        # Define inputs\n",
        "        ys = Input(shape=(num_joints, 2))\n",
        "        yc = Input(shape=(num_joints * num_context, 2))\n",
        "        pc = Input(shape=(num_joints * num_context, 1))\n",
        "\n",
        "        # Split contextual predictions in x and y and do computations separately\n",
        "        xi = Lambda(lambda x: x[:,:, 0:1])(yc)\n",
        "        yi = Lambda(lambda x: x[:,:, 1:2])(yc)\n",
        "    else:\n",
        "        ys = Input(shape=(num_frames, num_joints, 2))\n",
        "        yc = Input(shape=(num_frames, num_joints * num_context, 2))\n",
        "        pc = Input(shape=(num_frames, num_joints * num_context, 1))\n",
        "\n",
        "        # Split contextual predictions in x and y and do computations separately\n",
        "        xi = Lambda(lambda x: x[:,:,:, 0:1])(yc)\n",
        "        yi = Lambda(lambda x: x[:,:,:, 1:2])(yc)\n",
        "\n",
        "    pxi = multiply([xi, pc])\n",
        "    pyi = multiply([yi, pc])\n",
        "\n",
        "    pc_sum = ctx_sum(pc)\n",
        "    pxi_sum = ctx_sum(pxi)\n",
        "    pyi_sum = ctx_sum(pyi)\n",
        "    pc_div = Lambda(lambda x: x / num_context)(pc_sum)\n",
        "    pxi_div = tf_div([pxi_sum, pc_sum])\n",
        "    pyi_div = tf_div([pyi_sum, pc_sum])\n",
        "    yc_div = concatenate([pxi_div, pyi_div])\n",
        "\n",
        "    ys_alpha = mul_alpha(ys)\n",
        "    yc_div_1alpha = mul_1alpha(yc_div)\n",
        "\n",
        "    y = add([ys_alpha, yc_div_1alpha])\n",
        "\n",
        "    model = Model(inputs=[ys, yc, pc], outputs=y, name=name)\n",
        "    model.trainable = False\n",
        "\n",
        "    return model\n",
        "\n",
        "def _reset_invalid_joints(y_true, y_pred):\n",
        "    \"\"\"Reset (set to zero) invalid joints, according to y_true, and compute the\n",
        "    number of valid joints.\n",
        "    \"\"\"\n",
        "    idx = K.cast(K.greater(y_true, 0.), 'float32')\n",
        "    y_true = idx * y_true\n",
        "    y_pred = idx * y_pred\n",
        "    num_joints = K.clip(K.sum(idx, axis=(-1, -2)), 1, None)\n",
        "    return y_true, y_pred, num_joints\n",
        "\n",
        "\n",
        "def elasticnet_loss_on_valid_joints(y_true, y_pred):\n",
        "    y_true, y_pred, num_joints = _reset_invalid_joints(y_true, y_pred)\n",
        "    l1 = K.sum(K.abs(y_pred - y_true), axis=(-1, -2)) / num_joints\n",
        "    l2 = K.sum(K.square(y_pred - y_true), axis=(-1, -2)) / num_joints\n",
        "    return l1 + l2\n",
        "\n",
        "def action_top(x, name=None):\n",
        "    x = global_max_min_pooling(x)\n",
        "    x = Activation('softmax', name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_act_pred_block(x, num_out, name=None, last=False, include_top=True):\n",
        "\n",
        "    num_features = K.int_shape(x)[-1]\n",
        "\n",
        "    ident = x\n",
        "    x = act_conv_bn(x, int(num_features/2), (1, 1))\n",
        "    x = act_conv_bn(x, num_features, (3, 3))\n",
        "    x = add([ident, x])\n",
        "\n",
        "    ident = x\n",
        "    x1 = act_conv_bn(x, num_features, (3, 3))\n",
        "    x = max_min_pooling(x1, (2, 2))\n",
        "    action_hm = act_conv(x, num_out, (3, 3))\n",
        "    y = action_hm\n",
        "    if include_top:\n",
        "        y = action_top(y)\n",
        "\n",
        "    if not last:\n",
        "        action_hm = UpSampling2D((2, 2))(action_hm)\n",
        "        action_hm = act_conv_bn(action_hm, num_features, (3, 3))\n",
        "        x = add([ident, x1, action_hm])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def build_pose_model(num_joints, num_actions, num_temp_frames=None, pose_dim=2,\n",
        "        name=None, include_top=True, network_version='v1'):\n",
        "\n",
        "    y = Input(shape=(num_temp_frames, num_joints, pose_dim))\n",
        "    p = Input(shape=(num_temp_frames, num_joints, 1))\n",
        "\n",
        "    ## Pose information\n",
        "    mask = Lambda(lambda x: K.tile(x, [1, 1, 1, pose_dim]))(p)\n",
        "    x = Lambda(lambda x: x[0] * x[1])([y, mask])\n",
        "\n",
        "    if network_version == 'v1':\n",
        "        a = conv_bn_act(x, 8, (3, 1))\n",
        "        b = conv_bn_act(x, 16, (3, 3))\n",
        "        c = conv_bn_act(x, 24, (3, 5))\n",
        "        x = concatenate([a, b, c])\n",
        "        a = conv_bn(x, 56, (3, 3))\n",
        "        b = conv_bn(x, 32, (1, 1))\n",
        "        b = conv_bn(b, 56, (3, 3))\n",
        "        x = concatenate([a, b])\n",
        "        x = max_min_pooling(x, (2, 2))\n",
        "    elif network_version == 'v2':\n",
        "        a = conv_bn_act(x, 12, (3, 1))\n",
        "        b = conv_bn_act(x, 24, (3, 3))\n",
        "        c = conv_bn_act(x, 36, (3, 5))\n",
        "        x = concatenate([a, b, c])\n",
        "        a = conv_bn(x, 112, (3, 3))\n",
        "        b = conv_bn(x, 64, (1, 1))\n",
        "        b = conv_bn(b, 112, (3, 3))\n",
        "        x = concatenate([a, b])\n",
        "        x = max_min_pooling(x, (2, 2))\n",
        "    else:\n",
        "        raise Exception('Unkown network version \"{}\"'.format(network_version))\n",
        "\n",
        "    x, y1 = build_act_pred_block(x, num_actions, name='y1',\n",
        "            include_top=include_top)\n",
        "    x, y2 = build_act_pred_block(x, num_actions, name='y2',\n",
        "            include_top=include_top)\n",
        "    x, y3 = build_act_pred_block(x, num_actions, name='y3',\n",
        "            include_top=include_top)\n",
        "    _, y4 = build_act_pred_block(x, num_actions, name='y4',\n",
        "            include_top=include_top, last=True)\n",
        "    x = [y1, y2, y3, y4]\n",
        "\n",
        "    model = Model(inputs=[y, p], outputs=x, name=name)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_visual_model(num_joints, num_actions, num_features,\n",
        "        num_temp_frames=None, name=None, include_top=True):\n",
        "\n",
        "    inp = Input(shape=(num_temp_frames, num_joints, num_features))\n",
        "    x = conv_bn(inp, 256, (1, 1))\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x, y1 = build_act_pred_block(x, num_actions, name='y1',\n",
        "            include_top=include_top)\n",
        "    x, y2 = build_act_pred_block(x, num_actions, name='y2',\n",
        "            include_top=include_top)\n",
        "    x, y3 = build_act_pred_block(x, num_actions, name='y3',\n",
        "            include_top=include_top)\n",
        "    _, y4 = build_act_pred_block(x, num_actions, name='y4',\n",
        "            include_top=include_top, last=True)\n",
        "    model = Model(inp, [y1, y2, y3, y4], name=name)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _get_2d_pose_estimation_from_model(inp, model_pe, num_joints, num_blocks,\n",
        "        num_context_per_joint, full_trainable=False):\n",
        "\n",
        "    num_frames = K.int_shape(inp)[1]\n",
        "\n",
        "    stem = model_pe.get_layer('Stem')\n",
        "    stem.trainable = full_trainable\n",
        "\n",
        "    i = 1\n",
        "    recep_block = model_pe.get_layer('rBlock%d' % i)\n",
        "    recep_block.trainable = full_trainable\n",
        "\n",
        "    x1 = TimeDistributed(stem, name='td_%s' % stem.name)(inp)\n",
        "    xb1 = TimeDistributed(recep_block, name='td_%s' % recep_block.name)(x1)\n",
        "\n",
        "    inp_pe = Input(shape=K.int_shape(xb1)[2:])\n",
        "    sep_conv = model_pe.get_layer('SepConv%d' % i)\n",
        "    reg_map = model_pe.get_layer('RegMap%d' % i)\n",
        "    fre_map = model_pe.get_layer('fReMap%d' % i)\n",
        "    x2 = sep_conv(inp_pe)\n",
        "    x3 = fre_map(reg_map(x2))\n",
        "    x = add([inp_pe, x2, x3])\n",
        "\n",
        "    for i in range(2, num_blocks):\n",
        "        recep_block = model_pe.get_layer('rBlock%d' % i)\n",
        "        sep_conv = model_pe.get_layer('SepConv%d' % i)\n",
        "        reg_map = model_pe.get_layer('RegMap%d' % i)\n",
        "        fre_map = model_pe.get_layer('fReMap%d' % i)\n",
        "        x1 = recep_block(x)\n",
        "        x2 = sep_conv(x1)\n",
        "        x3 = fre_map(reg_map(x2))\n",
        "        x = add([x1, x2, x3])\n",
        "\n",
        "    recep_block = model_pe.get_layer('rBlock%d' % num_blocks)\n",
        "    sep_conv = model_pe.get_layer('SepConv%d' % num_blocks)\n",
        "    reg_map = model_pe.get_layer('RegMap%d' % num_blocks)\n",
        "    x = recep_block(x)\n",
        "    x = sep_conv(x)\n",
        "    x = reg_map(x)\n",
        "\n",
        "    model1 = Model(inp_pe, x, name='PoseReg')\n",
        "    model1.trainable = full_trainable\n",
        "\n",
        "    num_heatmaps = (num_context_per_joint + 1) * num_joints\n",
        "    num_rows = K.int_shape(model1.output)[1]\n",
        "    num_cols = K.int_shape(model1.output)[2]\n",
        "\n",
        "    sams_input_shape = (num_frames, num_rows, num_cols, num_joints)\n",
        "    samc_input_shape = \\\n",
        "            (num_frames, num_rows, num_cols, num_heatmaps - num_joints)\n",
        "\n",
        "    # Build the time distributed models\n",
        "    model_pe.get_layer('sSAM').trainable = full_trainable\n",
        "    sam_s_model = TimeDistributed(model_pe.get_layer('sSAM'),\n",
        "            input_shape=sams_input_shape, name='sSAM')\n",
        "\n",
        "    if num_context_per_joint > 0:\n",
        "        model_pe.get_layer('cSAM').trainable = full_trainable\n",
        "        sam_c_model = TimeDistributed(model_pe.get_layer('cSAM'),\n",
        "                input_shape=samc_input_shape, name='cSAM')\n",
        "\n",
        "    model_pe.get_layer('sjProb').trainable = False\n",
        "    jprob_s_model = TimeDistributed(model_pe.get_layer('sjProb'),\n",
        "            input_shape=sams_input_shape, name='sjProb')\n",
        "\n",
        "    if num_context_per_joint > 0:\n",
        "        model_pe.get_layer('cjProb').trainable = False\n",
        "        jprob_c_model = TimeDistributed(model_pe.get_layer('cjProb'),\n",
        "                input_shape=samc_input_shape, name='cjProb')\n",
        "\n",
        "    agg_model = build_context_aggregation(num_joints,\n",
        "            num_context_per_joint, 0.8, num_frames=num_frames, name='Agg')\n",
        "\n",
        "    h = TimeDistributed(model1, name='td_Model1')(xb1)\n",
        "    if num_context_per_joint > 0:\n",
        "        hs = Lambda(lambda x: x[:,:,:,:, :num_joints])(h)\n",
        "        hc = Lambda(lambda x: x[:,:,:,:, num_joints:])(h)\n",
        "    else:\n",
        "        hs = h\n",
        "\n",
        "    ys = sam_s_model(hs)\n",
        "    if num_context_per_joint > 0:\n",
        "        yc = sam_c_model(hc)\n",
        "        pc = jprob_c_model(hc)\n",
        "        y = agg_model([ys, yc, pc])\n",
        "    else:\n",
        "        y = ys\n",
        "\n",
        "    p = jprob_s_model(Lambda(lambda x: 4*x)(hs))\n",
        "\n",
        "    hs = TimeDistributed(Activation(channel_softmax_2d()),\n",
        "            name='td_ChannelSoftmax')(hs)\n",
        "\n",
        "    return y, p, hs, xb1\n",
        "\n",
        "\n",
        "def _get_3d_pose_estimation_from_model(inp, model_pe, num_joints, num_blocks,\n",
        "        depth_maps, full_trainable=False):\n",
        "\n",
        "    num_frames = K.int_shape(inp)[1]\n",
        "\n",
        "    model_pe.summary()\n",
        "\n",
        "    stem = model_pe.get_layer('Stem')\n",
        "    stem.trainable = full_trainable\n",
        "\n",
        "    i = 1\n",
        "    recep_block = model_pe.get_layer('rBlock%d' % i)\n",
        "    recep_block.trainable = full_trainable\n",
        "\n",
        "    x1 = TimeDistributed(stem, name='td_%s' % stem.name)(inp)\n",
        "    xb1 = TimeDistributed(recep_block, name='td_%s' % recep_block.name)(x1)\n",
        "\n",
        "    inp_pe = Input(shape=K.int_shape(xb1)[2:])\n",
        "    sep_conv = model_pe.get_layer('SepConv%d' % i)\n",
        "    reg_map = model_pe.get_layer('RegMap%d' % i)\n",
        "    fre_map = model_pe.get_layer('fReMap%d' % i)\n",
        "    x2 = sep_conv(inp_pe)\n",
        "    x3 = fre_map(reg_map(x2))\n",
        "    x = add([inp_pe, x2, x3])\n",
        "\n",
        "    for i in range(2, num_blocks):\n",
        "        recep_block = model_pe.get_layer('rBlock%d' % i)\n",
        "        sep_conv = model_pe.get_layer('SepConv%d' % i)\n",
        "        reg_map = model_pe.get_layer('RegMap%d' % i)\n",
        "        fre_map = model_pe.get_layer('fReMap%d' % i)\n",
        "        x1 = recep_block(x)\n",
        "        x2 = sep_conv(x1)\n",
        "        x3 = fre_map(reg_map(x2))\n",
        "        x = add([x1, x2, x3])\n",
        "\n",
        "    recep_block = model_pe.get_layer('rBlock%d' % num_blocks)\n",
        "    sep_conv = model_pe.get_layer('SepConv%d' % num_blocks)\n",
        "    reg_map = model_pe.get_layer('RegMap%d' % num_blocks)\n",
        "    x = recep_block(x)\n",
        "    x = sep_conv(x)\n",
        "    x = reg_map(x)\n",
        "\n",
        "    model1 = Model(inp_pe, x, name='PoseReg')\n",
        "    model1.trainable = full_trainable\n",
        "\n",
        "    num_rows = K.int_shape(model1.output)[1]\n",
        "    num_cols = K.int_shape(model1.output)[2]\n",
        "\n",
        "    sams_input_shape = (num_frames, num_rows, num_cols, num_joints)\n",
        "    samz_input_shape = (num_frames, depth_maps, num_joints)\n",
        "\n",
        "    # Build the time distributed models\n",
        "    model_pe.get_layer('sSAM').trainable = full_trainable\n",
        "    sam_s_model = TimeDistributed(model_pe.get_layer('sSAM'),\n",
        "            input_shape=sams_input_shape, name='sSAM')\n",
        "\n",
        "    model_pe.get_layer('zSAM').trainable = full_trainable\n",
        "    sam_z_model = TimeDistributed(model_pe.get_layer('zSAM'),\n",
        "            input_shape=samz_input_shape, name='zSAM')\n",
        "\n",
        "    h = TimeDistributed(model1, name='td_Model1')(xb1)\n",
        "    assert K.int_shape(h)[-1] == depth_maps * num_joints\n",
        "\n",
        "    def _reshape_heatmaps(x):\n",
        "        x = K.expand_dims(x, axis=-1)\n",
        "        x = K.reshape(x, (-1, K.int_shape(x)[1], K.int_shape(x)[2],\n",
        "            K.int_shape(x)[3], depth_maps, num_joints))\n",
        "\n",
        "        return x\n",
        "\n",
        "    h = Lambda(_reshape_heatmaps)(h)\n",
        "    hxy = Lambda(lambda x: K.mean(x, axis=4))(h)\n",
        "    hz = Lambda(lambda x: K.mean(x, axis=(2, 3)))(h)\n",
        "\n",
        "    pxy = sam_s_model(hxy)\n",
        "    pz = sam_z_model(hz)\n",
        "    pose = concatenate([pxy, pz])\n",
        "\n",
        "    vxy = TimeDistributed(GlobalMaxPooling2D(), name='td_GlobalMaxPooling2D',\n",
        "            input_shape=K.int_shape(hxy)[1:])(hxy)\n",
        "    vz = TimeDistributed(GlobalMaxPooling1D(), name='td_GlobalMaxPooling1D',\n",
        "            input_shape=K.int_shape(hz)[1:])(hz)\n",
        "    v = add([vxy, vz])\n",
        "    v = Lambda(lambda x: 2*K.expand_dims(x, axis=-1))(v)\n",
        "    visible = Activation('sigmoid')(v)\n",
        "\n",
        "    hxy = TimeDistributed(Activation(channel_softmax_2d()),\n",
        "            name='td_ChannelSoftmax')(hxy)\n",
        "\n",
        "    return pose, visible, hxy, xb1\n",
        "\n",
        "\n",
        "def build_guided_visual_model(model_pe, num_actions, input_shape, num_frames,\n",
        "        num_joints, num_blocks, num_context_per_joint=2):\n",
        "\n",
        "    inp = Input(shape=(num_frames,) + input_shape)\n",
        "    _, _, hs, xb1 = _get_2d_pose_estimation_from_model(inp, model_pe, num_joints,\n",
        "            num_blocks, num_context_per_joint,\n",
        "            num_context_per_joint=num_context_per_joint)\n",
        "\n",
        "    f = kronecker_prod(hs, xb1)\n",
        "    num_features = K.int_shape(f)[-1]\n",
        "    model_ar = build_visual_model(num_joints, num_actions, num_features,\n",
        "            num_temp_frames=num_frames, name='GuidedVisAR')\n",
        "\n",
        "    x = model_ar(f)\n",
        "    model = Model(inp, x)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_merge_model(model_pe,\n",
        "        num_actions,\n",
        "        input_shape,\n",
        "        num_frames,\n",
        "        num_joints,\n",
        "        num_blocks,\n",
        "        pose_dim=2,\n",
        "        depth_maps=8,\n",
        "        num_context_per_joint=2,\n",
        "        pose_net_version='v1',\n",
        "        output_poses=False,\n",
        "        weighted_merge=True,\n",
        "        ar_pose_weights=None,\n",
        "        ar_visual_weights=None,\n",
        "        full_trainable=False):\n",
        "\n",
        "    inp = Input(shape=(num_frames,) + input_shape)\n",
        "    outputs = []\n",
        "\n",
        "    if pose_dim == 2:\n",
        "        y, p, hs, xb1 = _get_2d_pose_estimation_from_model(inp, model_pe,\n",
        "                num_joints, num_blocks, num_context_per_joint,\n",
        "                full_trainable=full_trainable)\n",
        "    elif pose_dim == 3:\n",
        "        y, p, hs, xb1 = _get_3d_pose_estimation_from_model(inp, model_pe,\n",
        "                num_joints, num_blocks, depth_maps,\n",
        "                full_trainable=full_trainable)\n",
        "\n",
        "    if output_poses:\n",
        "        outputs.append(y)\n",
        "        outputs.append(p)\n",
        "\n",
        "    model_pose = build_pose_model(num_joints, num_actions, num_frames,\n",
        "            pose_dim=pose_dim, include_top=False, name='PoseAR',\n",
        "            network_version=pose_net_version)\n",
        "    # model_pose.trainable = False\n",
        "    if ar_pose_weights is not None:\n",
        "        model_pose.load_weights(ar_pose_weights)\n",
        "    out_pose = model_pose([y, p])\n",
        "\n",
        "    f = kronecker_prod(hs, xb1)\n",
        "    num_features = K.int_shape(f)[-1]\n",
        "    model_vis = build_visual_model(num_joints, num_actions, num_features,\n",
        "            num_temp_frames=num_frames, include_top=False, name='GuidedVisAR')\n",
        "    # model_vis.trainable = False\n",
        "    if ar_visual_weights is not None:\n",
        "        model_vis.load_weights(ar_visual_weights)\n",
        "    out_vis = model_vis(f)\n",
        "\n",
        "    for i in range(len(out_pose)):\n",
        "        outputs.append(action_top(out_pose[i], name='p%d' % (i+1)))\n",
        "\n",
        "    for i in range(len(out_vis)):\n",
        "        outputs.append(action_top(out_vis[i], name='v%d' % (i+1)))\n",
        "\n",
        "    p = out_pose[-1]\n",
        "    v = out_vis[-1]\n",
        "\n",
        "    def _heatmap_weighting(inp):\n",
        "        num_filters = K.int_shape(inp)[-1]\n",
        "        conv = SeparableConv2D(num_filters, (1, 1),\n",
        "                use_bias=False)\n",
        "        x = conv(inp)\n",
        "        w = conv.get_weights()\n",
        "        w[0].fill(1.)\n",
        "        w[1].fill(0)\n",
        "        for i in range(num_filters):\n",
        "            w[1][0, 0, i, i] = 1.\n",
        "        conv.set_weights(w)\n",
        "\n",
        "        return x\n",
        "\n",
        "    if weighted_merge:\n",
        "        p = _heatmap_weighting(p)\n",
        "        v = _heatmap_weighting(v)\n",
        "\n",
        "    m = add([p, v])\n",
        "    outputs.append(action_top(m, name='m'))\n",
        "\n",
        "    model = Model(inp, outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def compile(model, lr=0.001, momentum=0.95, loss_weights=None,\n",
        "        pose_predicted=False):\n",
        "\n",
        "    if pose_predicted:\n",
        "        losses = []\n",
        "        losses.append(elasticnet_loss_on_valid_joints)\n",
        "        losses.append('binary_crossentropy')\n",
        "        for i in range(len(model.output) - 2):\n",
        "            losses.append('categorical_crossentropy')\n",
        "\n",
        "        model.compile(loss=losses,\n",
        "                optimizer=SGD(lr=lr, momentum=momentum, nesterov=True),\n",
        "                loss_weights=loss_weights)\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=SGD(lr=lr, momentum=momentum, nesterov=True),\n",
        "                metrics=['acc'], loss_weights=loss_weights)\n",
        "\n"
      ],
      "metadata": {
        "id": "aoJlq2sWVQTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def channel_softmax_2d(alpha=1):\n",
        "\n",
        "    def _channel_softmax_2d(x):\n",
        "        assert K.ndim(x) in [4, 5], \\\n",
        "                'Input tensor must have ndim 4 or 5 ({})'.format(K.ndim(x))\n",
        "\n",
        "        if alpha != 1:\n",
        "            x = alpha * x\n",
        "        e = K.exp(x - K.max(x, axis=(-3, -2), keepdims=True))\n",
        "        s = K.clip(K.sum(e, axis=(-3, -2), keepdims=True), K.epsilon(), None)\n",
        "\n",
        "        return e / s\n",
        "\n",
        "    return _channel_softmax_2d\n",
        "\n",
        "\n",
        "def build_context_aggregation(num_joints, num_context, alpha,\n",
        "        num_frames=1, name=None):\n",
        "\n",
        "    inp = Input(shape=(num_joints * num_context, 1))\n",
        "    d = Dense(num_joints, use_bias=False)\n",
        "\n",
        "    x = Lambda(lambda x: K.squeeze(x, axis=-1))(inp)\n",
        "    x = d(x)\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=-1))(x)\n",
        "\n",
        "    w = d.get_weights()\n",
        "    w[0].fill(0)\n",
        "    for j in range(num_joints):\n",
        "        start = j*num_context\n",
        "        w[0][j * num_context : (j + 1) * num_context, j] = 1.\n",
        "    d.set_weights(w)\n",
        "    d.trainable = False\n",
        "\n",
        "    ctx_sum = Model(inputs=inp, outputs=x)\n",
        "    ctx_sum.trainable = False\n",
        "    if num_frames > 1:\n",
        "        ctx_sum = TimeDistributed(ctx_sum,\n",
        "                input_shape=(num_frames,) + K.int_shape(inp)[1:])\n",
        "\n",
        "    # Define auxiliary layers.\n",
        "    mul_alpha = Lambda(lambda x: alpha * x)\n",
        "    mul_1alpha = Lambda(lambda x: (1 - alpha) * x)\n",
        "\n",
        "    # This depends on TensorFlow because keras does not implement divide.\n",
        "    tf_div = Lambda(lambda x: tf.divide(x[0], x[1]))\n",
        "\n",
        "    if num_frames == 1:\n",
        "        # Define inputs\n",
        "        ys = Input(shape=(num_joints, 2))\n",
        "        yc = Input(shape=(num_joints * num_context, 2))\n",
        "        pc = Input(shape=(num_joints * num_context, 1))\n",
        "\n",
        "        # Split contextual predictions in x and y and do computations separately\n",
        "        xi = Lambda(lambda x: x[:,:, 0:1])(yc)\n",
        "        yi = Lambda(lambda x: x[:,:, 1:2])(yc)\n",
        "    else:\n",
        "        ys = Input(shape=(num_frames, num_joints, 2))\n",
        "        yc = Input(shape=(num_frames, num_joints * num_context, 2))\n",
        "        pc = Input(shape=(num_frames, num_joints * num_context, 1))\n",
        "\n",
        "        # Split contextual predictions in x and y and do computations separately\n",
        "        xi = Lambda(lambda x: x[:,:,:, 0:1])(yc)\n",
        "        yi = Lambda(lambda x: x[:,:,:, 1:2])(yc)\n",
        "\n",
        "    pxi = multiply([xi, pc])\n",
        "    pyi = multiply([yi, pc])\n",
        "\n",
        "    pc_sum = ctx_sum(pc)\n",
        "    pxi_sum = ctx_sum(pxi)\n",
        "    pyi_sum = ctx_sum(pyi)\n",
        "    pc_div = Lambda(lambda x: x / num_context)(pc_sum)\n",
        "    pxi_div = tf_div([pxi_sum, pc_sum])\n",
        "    pyi_div = tf_div([pyi_sum, pc_sum])\n",
        "    yc_div = concatenate([pxi_div, pyi_div])\n",
        "\n",
        "    ys_alpha = mul_alpha(ys)\n",
        "    yc_div_1alpha = mul_1alpha(yc_div)\n",
        "\n",
        "    y = add([ys_alpha, yc_div_1alpha])\n",
        "\n",
        "    model = Model(inputs=[ys, yc, pc], outputs=y, name=name)\n",
        "    model.trainable = False\n",
        "\n",
        "    return model\n",
        "\n",
        "def _reset_invalid_joints(y_true, y_pred):\n",
        "    \"\"\"Reset (set to zero) invalid joints, according to y_true, and compute the\n",
        "    number of valid joints.\n",
        "    \"\"\"\n",
        "    idx = K.cast(K.greater(y_true, 0.), 'float32')\n",
        "    y_true = idx * y_true\n",
        "    y_pred = idx * y_pred\n",
        "    num_joints = K.clip(K.sum(idx, axis=(-1, -2)), 1, None)\n",
        "    return y_true, y_pred, num_joints\n",
        "\n",
        "\n",
        "def elasticnet_loss_on_valid_joints(y_true, y_pred):\n",
        "    y_true, y_pred, num_joints = _reset_invalid_joints(y_true, y_pred)\n",
        "    l1 = K.sum(K.abs(y_pred - y_true), axis=(-1, -2)) / num_joints\n",
        "    l2 = K.sum(K.square(y_pred - y_true), axis=(-1, -2)) / num_joints\n",
        "    return l1 + l2\n",
        "\n",
        "def action_top(x, name=None):\n",
        "    x = global_max_min_pooling(x)\n",
        "    x = Activation('softmax', name=name)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_act_pred_block(x, num_out, name=None, last=False, include_top=True):\n",
        "\n",
        "    num_features = K.int_shape(x)[-1]\n",
        "\n",
        "    ident = x\n",
        "    x = act_conv_bn(x, int(num_features/2), (1, 1))\n",
        "    x = act_conv_bn(x, num_features, (3, 3))\n",
        "    x = add([ident, x])\n",
        "\n",
        "    ident = x\n",
        "    x1 = act_conv_bn(x, num_features, (3, 3))\n",
        "    x = max_min_pooling(x1, (2, 2))\n",
        "    action_hm = act_conv(x, num_out, (3, 3))\n",
        "    y = action_hm\n",
        "    if include_top:\n",
        "        y = action_top(y)\n",
        "\n",
        "    if not last:\n",
        "        action_hm = UpSampling2D((2, 2))(action_hm)\n",
        "        action_hm = act_conv_bn(action_hm, num_features, (3, 3))\n",
        "        x = add([ident, x1, action_hm])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def build_pose_model(num_joints, num_actions, num_temp_frames=None, pose_dim=2,\n",
        "        name=None, include_top=True, network_version='v1'):\n",
        "\n",
        "    y = Input(shape=(num_temp_frames, num_joints, pose_dim))\n",
        "    p = Input(shape=(num_temp_frames, num_joints, 1))\n",
        "\n",
        "    ## Pose information\n",
        "    mask = Lambda(lambda x: K.tile(x, [1, 1, 1, pose_dim]))(p)\n",
        "    x = Lambda(lambda x: x[0] * x[1])([y, mask])\n",
        "\n",
        "    if network_version == 'v1':\n",
        "        a = conv_bn_act(x, 8, (3, 1))\n",
        "        b = conv_bn_act(x, 16, (3, 3))\n",
        "        c = conv_bn_act(x, 24, (3, 5))\n",
        "        x = concatenate([a, b, c])\n",
        "        a = conv_bn(x, 56, (3, 3))\n",
        "        b = conv_bn(x, 32, (1, 1))\n",
        "        b = conv_bn(b, 56, (3, 3))\n",
        "        x = concatenate([a, b])\n",
        "        x = max_min_pooling(x, (2, 2))\n",
        "    elif network_version == 'v2':\n",
        "        a = conv_bn_act(x, 12, (3, 1))\n",
        "        b = conv_bn_act(x, 24, (3, 3))\n",
        "        c = conv_bn_act(x, 36, (3, 5))\n",
        "        x = concatenate([a, b, c])\n",
        "        a = conv_bn(x, 112, (3, 3))\n",
        "        b = conv_bn(x, 64, (1, 1))\n",
        "        b = conv_bn(b, 112, (3, 3))\n",
        "        x = concatenate([a, b])\n",
        "        x = max_min_pooling(x, (2, 2))\n",
        "    else:\n",
        "        raise Exception('Unkown network version \"{}\"'.format(network_version))\n",
        "\n",
        "    x, y1 = build_act_pred_block(x, num_actions, name='y1',\n",
        "            include_top=include_top)\n",
        "    x, y2 = build_act_pred_block(x, num_actions, name='y2',\n",
        "            include_top=include_top)\n",
        "    x, y3 = build_act_pred_block(x, num_actions, name='y3',\n",
        "            include_top=include_top)\n",
        "    _, y4 = build_act_pred_block(x, num_actions, name='y4',\n",
        "            include_top=include_top, last=True)\n",
        "    x = [y1, y2, y3, y4]\n",
        "\n",
        "    model = Model(inputs=[y, p], outputs=x, name=name)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_visual_model(num_joints, num_actions, num_features,\n",
        "        num_temp_frames=None, name=None, include_top=True):\n",
        "\n",
        "    inp = Input(shape=(num_temp_frames, num_joints, num_features))\n",
        "    x = conv_bn(inp, 256, (1, 1))\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x, y1 = build_act_pred_block(x, num_actions, name='y1',\n",
        "            include_top=include_top)\n",
        "    x, y2 = build_act_pred_block(x, num_actions, name='y2',\n",
        "            include_top=include_top)\n",
        "    x, y3 = build_act_pred_block(x, num_actions, name='y3',\n",
        "            include_top=include_top)\n",
        "    _, y4 = build_act_pred_block(x, num_actions, name='y4',\n",
        "            include_top=include_top, last=True)\n",
        "    model = Model(inp, [y1, y2, y3, y4], name=name)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _get_2d_pose_estimation_from_model(inp, model_pe, num_joints, num_blocks,\n",
        "        num_context_per_joint, full_trainable=False):\n",
        "\n",
        "    num_frames = K.int_shape(inp)[1]\n",
        "\n",
        "    stem = model_pe.get_layer('Stem')\n",
        "    stem.trainable = full_trainable\n",
        "\n",
        "    i = 1\n",
        "    recep_block = model_pe.get_layer('rBlock%d' % i)\n",
        "    recep_block.trainable = full_trainable\n",
        "\n",
        "    x1 = TimeDistributed(stem, name='td_%s' % stem.name)(inp)\n",
        "    xb1 = TimeDistributed(recep_block, name='td_%s' % recep_block.name)(x1)\n",
        "\n",
        "    inp_pe = Input(shape=K.int_shape(xb1)[2:])\n",
        "    sep_conv = model_pe.get_layer('SepConv%d' % i)\n",
        "    reg_map = model_pe.get_layer('RegMap%d' % i)\n",
        "    fre_map = model_pe.get_layer('fReMap%d' % i)\n",
        "    x2 = sep_conv(inp_pe)\n",
        "    x3 = fre_map(reg_map(x2))\n",
        "    x = add([inp_pe, x2, x3])\n",
        "\n",
        "    for i in range(2, num_blocks):\n",
        "        recep_block = model_pe.get_layer('rBlock%d' % i)\n",
        "        sep_conv = model_pe.get_layer('SepConv%d' % i)\n",
        "        reg_map = model_pe.get_layer('RegMap%d' % i)\n",
        "        fre_map = model_pe.get_layer('fReMap%d' % i)\n",
        "        x1 = recep_block(x)\n",
        "        x2 = sep_conv(x1)\n",
        "        x3 = fre_map(reg_map(x2))\n",
        "        x = add([x1, x2, x3])\n",
        "\n",
        "    recep_block = model_pe.get_layer('rBlock%d' % num_blocks)\n",
        "    sep_conv = model_pe.get_layer('SepConv%d' % num_blocks)\n",
        "    reg_map = model_pe.get_layer('RegMap%d' % num_blocks)\n",
        "    x = recep_block(x)\n",
        "    x = sep_conv(x)\n",
        "    x = reg_map(x)\n",
        "\n",
        "    model1 = Model(inp_pe, x, name='PoseReg')\n",
        "    model1.trainable = full_trainable\n",
        "\n",
        "    num_heatmaps = (num_context_per_joint + 1) * num_joints\n",
        "    num_rows = K.int_shape(model1.output)[1]\n",
        "    num_cols = K.int_shape(model1.output)[2]\n",
        "\n",
        "    sams_input_shape = (num_frames, num_rows, num_cols, num_joints)\n",
        "    samc_input_shape = \\\n",
        "            (num_frames, num_rows, num_cols, num_heatmaps - num_joints)\n",
        "\n",
        "    # Build the time distributed models\n",
        "    model_pe.get_layer('sSAM').trainable = full_trainable\n",
        "    sam_s_model = TimeDistributed(model_pe.get_layer('sSAM'),\n",
        "            input_shape=sams_input_shape, name='sSAM')\n",
        "\n",
        "    if num_context_per_joint > 0:\n",
        "        model_pe.get_layer('cSAM').trainable = full_trainable\n",
        "        sam_c_model = TimeDistributed(model_pe.get_layer('cSAM'),\n",
        "                input_shape=samc_input_shape, name='cSAM')\n",
        "\n",
        "    model_pe.get_layer('sjProb').trainable = False\n",
        "    jprob_s_model = TimeDistributed(model_pe.get_layer('sjProb'),\n",
        "            input_shape=sams_input_shape, name='sjProb')\n",
        "\n",
        "    if num_context_per_joint > 0:\n",
        "        model_pe.get_layer('cjProb').trainable = False\n",
        "        jprob_c_model = TimeDistributed(model_pe.get_layer('cjProb'),\n",
        "                input_shape=samc_input_shape, name='cjProb')\n",
        "\n",
        "    agg_model = build_context_aggregation(num_joints,\n",
        "            num_context_per_joint, 0.8, num_frames=num_frames, name='Agg')\n",
        "\n",
        "    h = TimeDistributed(model1, name='td_Model1')(xb1)\n",
        "    if num_context_per_joint > 0:\n",
        "        hs = Lambda(lambda x: x[:,:,:,:, :num_joints])(h)\n",
        "        hc = Lambda(lambda x: x[:,:,:,:, num_joints:])(h)\n",
        "    else:\n",
        "        hs = h\n",
        "\n",
        "    ys = sam_s_model(hs)\n",
        "    if num_context_per_joint > 0:\n",
        "        yc = sam_c_model(hc)\n",
        "        pc = jprob_c_model(hc)\n",
        "        y = agg_model([ys, yc, pc])\n",
        "    else:\n",
        "        y = ys\n",
        "\n",
        "    p = jprob_s_model(Lambda(lambda x: 4*x)(hs))\n",
        "\n",
        "    hs = TimeDistributed(Activation(channel_softmax_2d()),\n",
        "            name='td_ChannelSoftmax')(hs)\n",
        "\n",
        "    return y, p, hs, xb1\n",
        "\n",
        "\n",
        "def _get_3d_pose_estimation_from_model(inp, model_pe, num_joints, num_blocks,\n",
        "        depth_maps, full_trainable=False):\n",
        "\n",
        "    num_frames = K.int_shape(inp)[1]\n",
        "\n",
        "    model_pe.summary()\n",
        "\n",
        "    stem = model_pe.get_layer('Stem')\n",
        "    stem.trainable = full_trainable\n",
        "\n",
        "    i = 1\n",
        "    recep_block = model_pe.get_layer('rBlock%d' % i)\n",
        "    recep_block.trainable = full_trainable\n",
        "\n",
        "    x1 = TimeDistributed(stem, name='td_%s' % stem.name)(inp)\n",
        "    xb1 = TimeDistributed(recep_block, name='td_%s' % recep_block.name)(x1)\n",
        "\n",
        "    inp_pe = Input(shape=K.int_shape(xb1)[2:])\n",
        "    sep_conv = model_pe.get_layer('SepConv%d' % i)\n",
        "    reg_map = model_pe.get_layer('RegMap%d' % i)\n",
        "    fre_map = model_pe.get_layer('fReMap%d' % i)\n",
        "    x2 = sep_conv(inp_pe)\n",
        "    x3 = fre_map(reg_map(x2))\n",
        "    x = add([inp_pe, x2, x3])\n",
        "\n",
        "    for i in range(2, num_blocks):\n",
        "        recep_block = model_pe.get_layer('rBlock%d' % i)\n",
        "        sep_conv = model_pe.get_layer('SepConv%d' % i)\n",
        "        reg_map = model_pe.get_layer('RegMap%d' % i)\n",
        "        fre_map = model_pe.get_layer('fReMap%d' % i)\n",
        "        x1 = recep_block(x)\n",
        "        x2 = sep_conv(x1)\n",
        "        x3 = fre_map(reg_map(x2))\n",
        "        x = add([x1, x2, x3])\n",
        "\n",
        "    recep_block = model_pe.get_layer('rBlock%d' % num_blocks)\n",
        "    sep_conv = model_pe.get_layer('SepConv%d' % num_blocks)\n",
        "    reg_map = model_pe.get_layer('RegMap%d' % num_blocks)\n",
        "    x = recep_block(x)\n",
        "    x = sep_conv(x)\n",
        "    x = reg_map(x)\n",
        "\n",
        "    model1 = Model(inp_pe, x, name='PoseReg')\n",
        "    model1.trainable = full_trainable\n",
        "\n",
        "    num_rows = K.int_shape(model1.output)[1]\n",
        "    num_cols = K.int_shape(model1.output)[2]\n",
        "\n",
        "    sams_input_shape = (num_frames, num_rows, num_cols, num_joints)\n",
        "    samz_input_shape = (num_frames, depth_maps, num_joints)\n",
        "\n",
        "    # Build the time distributed models\n",
        "    model_pe.get_layer('sSAM').trainable = full_trainable\n",
        "    sam_s_model = TimeDistributed(model_pe.get_layer('sSAM'),\n",
        "            input_shape=sams_input_shape, name='sSAM')\n",
        "\n",
        "    model_pe.get_layer('zSAM').trainable = full_trainable\n",
        "    sam_z_model = TimeDistributed(model_pe.get_layer('zSAM'),\n",
        "            input_shape=samz_input_shape, name='zSAM')\n",
        "\n",
        "    h = TimeDistributed(model1, name='td_Model1')(xb1)\n",
        "    assert K.int_shape(h)[-1] == depth_maps * num_joints\n",
        "\n",
        "    def _reshape_heatmaps(x):\n",
        "        x = K.expand_dims(x, axis=-1)\n",
        "        x = K.reshape(x, (-1, K.int_shape(x)[1], K.int_shape(x)[2],\n",
        "            K.int_shape(x)[3], depth_maps, num_joints))\n",
        "\n",
        "        return x\n",
        "\n",
        "    h = Lambda(_reshape_heatmaps)(h)\n",
        "    hxy = Lambda(lambda x: K.mean(x, axis=4))(h)\n",
        "    hz = Lambda(lambda x: K.mean(x, axis=(2, 3)))(h)\n",
        "\n",
        "    pxy = sam_s_model(hxy)\n",
        "    pz = sam_z_model(hz)\n",
        "    pose = concatenate([pxy, pz])\n",
        "\n",
        "    vxy = TimeDistributed(GlobalMaxPooling2D(), name='td_GlobalMaxPooling2D',\n",
        "            input_shape=K.int_shape(hxy)[1:])(hxy)\n",
        "    vz = TimeDistributed(GlobalMaxPooling1D(), name='td_GlobalMaxPooling1D',\n",
        "            input_shape=K.int_shape(hz)[1:])(hz)\n",
        "    v = add([vxy, vz])\n",
        "    v = Lambda(lambda x: 2*K.expand_dims(x, axis=-1))(v)\n",
        "    visible = Activation('sigmoid')(v)\n",
        "\n",
        "    hxy = TimeDistributed(Activation(channel_softmax_2d()),\n",
        "            name='td_ChannelSoftmax')(hxy)\n",
        "\n",
        "    return pose, visible, hxy, xb1\n",
        "\n",
        "\n",
        "def build_guided_visual_model(model_pe, num_actions, input_shape, num_frames,\n",
        "        num_joints, num_blocks, num_context_per_joint=2):\n",
        "\n",
        "    inp = Input(shape=(num_frames,) + input_shape)\n",
        "    _, _, hs, xb1 = _get_2d_pose_estimation_from_model(inp, model_pe, num_joints,\n",
        "            num_blocks, num_context_per_joint,\n",
        "            num_context_per_joint=num_context_per_joint)\n",
        "\n",
        "    f = kronecker_prod(hs, xb1)\n",
        "    num_features = K.int_shape(f)[-1]\n",
        "    model_ar = build_visual_model(num_joints, num_actions, num_features,\n",
        "            num_temp_frames=num_frames, name='GuidedVisAR')\n",
        "\n",
        "    x = model_ar(f)\n",
        "    model = Model(inp, x)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_merge_model(model_pe,\n",
        "        num_actions,\n",
        "        input_shape,\n",
        "        num_frames,\n",
        "        num_joints,\n",
        "        num_blocks,\n",
        "        pose_dim=2,\n",
        "        depth_maps=8,\n",
        "        num_context_per_joint=2,\n",
        "        pose_net_version='v1',\n",
        "        output_poses=False,\n",
        "        weighted_merge=True,\n",
        "        ar_pose_weights=None,\n",
        "        ar_visual_weights=None,\n",
        "        full_trainable=False):\n",
        "\n",
        "    inp = Input(shape=(num_frames,) + input_shape)\n",
        "    outputs = []\n",
        "\n",
        "    if pose_dim == 2:\n",
        "        y, p, hs, xb1 = _get_2d_pose_estimation_from_model(inp, model_pe,\n",
        "                num_joints, num_blocks, num_context_per_joint,\n",
        "                full_trainable=full_trainable)\n",
        "    elif pose_dim == 3:\n",
        "        y, p, hs, xb1 = _get_3d_pose_estimation_from_model(inp, model_pe,\n",
        "                num_joints, num_blocks, depth_maps,\n",
        "                full_trainable=full_trainable)\n",
        "\n",
        "    if output_poses:\n",
        "        outputs.append(y)\n",
        "        outputs.append(p)\n",
        "\n",
        "    model_pose = build_pose_model(num_joints, num_actions, num_frames,\n",
        "            pose_dim=pose_dim, include_top=False, name='PoseAR',\n",
        "            network_version=pose_net_version)\n",
        "    # model_pose.trainable = False\n",
        "    if ar_pose_weights is not None:\n",
        "        model_pose.load_weights(ar_pose_weights)\n",
        "    out_pose = model_pose([y, p])\n",
        "\n",
        "    f = kronecker_prod(hs, xb1)\n",
        "    num_features = K.int_shape(f)[-1]\n",
        "    model_vis = build_visual_model(num_joints, num_actions, num_features,\n",
        "            num_temp_frames=num_frames, include_top=False, name='GuidedVisAR')\n",
        "    # model_vis.trainable = False\n",
        "    if ar_visual_weights is not None:\n",
        "        model_vis.load_weights(ar_visual_weights)\n",
        "    out_vis = model_vis(f)\n",
        "\n",
        "    for i in range(len(out_pose)):\n",
        "        outputs.append(action_top(out_pose[i], name='p%d' % (i+1)))\n",
        "\n",
        "    for i in range(len(out_vis)):\n",
        "        outputs.append(action_top(out_vis[i], name='v%d' % (i+1)))\n",
        "\n",
        "    p = out_pose[-1]\n",
        "    v = out_vis[-1]\n",
        "\n",
        "    def _heatmap_weighting(inp):\n",
        "        num_filters = K.int_shape(inp)[-1]\n",
        "        conv = SeparableConv2D(num_filters, (1, 1),\n",
        "                use_bias=False)\n",
        "        x = conv(inp)\n",
        "        w = conv.get_weights()\n",
        "        w[0].fill(1.)\n",
        "        w[1].fill(0)\n",
        "        for i in range(num_filters):\n",
        "            w[1][0, 0, i, i] = 1.\n",
        "        conv.set_weights(w)\n",
        "\n",
        "        return x\n",
        "\n",
        "    if weighted_merge:\n",
        "        p = _heatmap_weighting(p)\n",
        "        v = _heatmap_weighting(v)\n",
        "\n",
        "    m = add([p, v])\n",
        "    outputs.append(action_top(m, name='m'))\n",
        "\n",
        "    model = Model(inp, outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def compile(model, lr=0.001, momentum=0.95, loss_weights=None,\n",
        "        pose_predicted=False):\n",
        "\n",
        "    if pose_predicted:\n",
        "        losses = []\n",
        "        losses.append(elasticnet_loss_on_valid_joints)\n",
        "        losses.append('binary_crossentropy')\n",
        "        for i in range(len(model.output) - 2):\n",
        "            losses.append('categorical_crossentropy')\n",
        "\n",
        "        model.compile(loss=losses,\n",
        "                optimizer=SGD(lr=lr, momentum=momentum, nesterov=True),\n",
        "                loss_weights=loss_weights)\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=SGD(lr=lr, momentum=momentum, nesterov=True),\n",
        "                metrics=['acc'], loss_weights=loss_weights)\n",
        "\n"
      ],
      "metadata": {
        "id": "6ZEw9g6EVTi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(inp, kernel_size, filters, last_act=True):\n",
        "\n",
        "    filters1, filters2, filters3 = filters\n",
        "\n",
        "    x = conv_bn_act(inp, filters1, (1, 1))\n",
        "    x = conv_bn_act(x, filters2, kernel_size)\n",
        "    x = conv_bn(x, filters3, (1, 1))\n",
        "\n",
        "    shortcut = conv_bn(inp, filters3, (1, 1))\n",
        "    x = add([x, shortcut])\n",
        "    if last_act:\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def identity_block(inp, kernel_size, filters, last_act=True):\n",
        "\n",
        "    filters1, filters2, filters3 = filters\n",
        "\n",
        "    x = conv_bn_act(inp, filters1, (1, 1))\n",
        "    x = conv_bn_act(x, filters2, kernel_size)\n",
        "    x = conv_bn(x, filters3, (1, 1))\n",
        "\n",
        "    x = add([x, inp])\n",
        "    if last_act:\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def stem_inception_v4(x, image_div=8):\n",
        "    \"\"\"Entry-flow network (stem) *based* on Inception_v4.\"\"\"\n",
        "\n",
        "    assert image_div in [4, 8, 16, 32], \\\n",
        "            'Invalid image_div ({}).'.format(image_div)\n",
        "\n",
        "    x = conv_bn_act(x, 32, (3, 3), strides=(2, 2))\n",
        "    x = conv_bn_act(x, 32, (3, 3))\n",
        "    if image_div is 32:\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "    x = conv_bn_act(x, 64, (3, 3))\n",
        "\n",
        "    a = conv_bn_act(x, 96, (3, 3), strides=(2, 2))\n",
        "    b = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = concatenate([a, b])\n",
        "\n",
        "    a = conv_bn_act(x, 64, (1, 1))\n",
        "    a = conv(a, 96, (3, 3))\n",
        "    b = conv_bn_act(x, 64, (1, 1))\n",
        "    b = conv_bn_act(b, 64, (5, 1))\n",
        "    b = conv_bn_act(b, 64, (1, 5))\n",
        "    b = conv(b, 96, (3, 3))\n",
        "    x = concatenate([a, b])\n",
        "    x = BatchNormalization(axis=-1, scale=False)(x)\n",
        "\n",
        "    if image_div != 4:\n",
        "        a = act_conv_bn(x, 192, (3, 3), strides=(2, 2))\n",
        "        b = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "        x = concatenate([a, b])\n",
        "\n",
        "    if image_div in [16, 32]:\n",
        "        a = act_conv_bn(x, 192, (3, 3), strides=(2, 2))\n",
        "        b = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "        x = concatenate([a, b])\n",
        "\n",
        "    if image_div ==4:\n",
        "        x = residual(x, int_size=112, out_size=2*192+64, convtype='normal',\n",
        "                name='residual0')\n",
        "    else:\n",
        "        x = residual(x, int_size=144, out_size=3*192, convtype='normal',\n",
        "                name='residual0')\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def stem_residual_eccv(x, image_div=8):\n",
        "    \"\"\"Entry-flow network (stem) *based* on ResNet ('residual' option).\"\"\"\n",
        "\n",
        "    assert image_div in [4, 8, 16, 32], \\\n",
        "            'Invalid image_div ({}).'.format(image_div)\n",
        "\n",
        "    x = conv_bn_act(x, 64, (7, 7), strides=(2, 2), padding='same')\n",
        "    a = conv_bn_act(x, 128, (3, 3), padding='same')\n",
        "    b = conv_bn_act(x, 128, (1, 1), padding='same')\n",
        "    x = add([a, b])\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = residual(x, int_size=128, out_size=256, convtype='normal', name='rn0')\n",
        "    x = residual(x, int_size=128, out_size=256, convtype='normal', name='rn1')\n",
        "\n",
        "    if image_div is 4:\n",
        "        x = residual(x, out_size=256, convtype='normal', name='rn3')\n",
        "\n",
        "    else:\n",
        "        x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "        x = residual(x, int_size=192, out_size=384, convtype='normal',\n",
        "                name='rn3')\n",
        "        x = residual(x, int_size=192, out_size=384, convtype='normal',\n",
        "                name='rn4')\n",
        "\n",
        "        if image_div in [16, 32]:\n",
        "            x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "            x = residual(x, int_size=256, out_size=512, convtype='normal',\n",
        "                    name='rn5')\n",
        "            x = residual(x, int_size=256, out_size=512, convtype='normal',\n",
        "                    name='rn6')\n",
        "\n",
        "            if image_div is 32:\n",
        "                x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def reception_block(x, num_levels, kernel_size, int_size=None,\n",
        "        convtype='depthwise', name=None):\n",
        "\n",
        "    def hourglass(x, n):\n",
        "        up1 = residual(x, kernel_size=kernel_size, int_size=int_size,\n",
        "                convtype=convtype)\n",
        "\n",
        "        low = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "        if n == num_levels:\n",
        "            low = act_conv_bn(low, int(K.int_shape(x)[-1] / 2), (1, 1))\n",
        "        low = residual(low, kernel_size=kernel_size, int_size=int_size,\n",
        "                convtype=convtype)\n",
        "\n",
        "        if n > 2:\n",
        "            low = hourglass(low, n-1)\n",
        "        else:\n",
        "            low = residual(low, kernel_size=kernel_size,\n",
        "                    int_size=int_size,\n",
        "                    convtype=convtype)\n",
        "\n",
        "        if n == num_levels:\n",
        "            low = residual(low, kernel_size=kernel_size,\n",
        "                    out_size=K.int_shape(x)[-1], int_size=int_size,\n",
        "                    convtype=convtype)\n",
        "        else:\n",
        "            low = residual(low, kernel_size=kernel_size,\n",
        "                    int_size=int_size, convtype=convtype)\n",
        "\n",
        "        up2 = UpSampling2D((2, 2))(low)\n",
        "\n",
        "        x = add([up1, up2])\n",
        "\n",
        "        return x\n",
        "\n",
        "    x = hourglass(x, num_levels)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_keypoints_regressor(input_shape, dim, num_maps, sam_model, prob_model,\n",
        "        name=None, verbose=0):\n",
        "\n",
        "    assert num_maps >= 1, \\\n",
        "            'The number of maps should be at least 1 (%d given)' % num_maps\n",
        "\n",
        "    inputs = []\n",
        "    inputs3d = []\n",
        "    p_concat = []\n",
        "    v_concat = []\n",
        "\n",
        "    # Auxiliary functions\n",
        "    v_tile = Lambda(lambda x: K.tile(x, (1, 1, dim)))\n",
        "    # This depends on TensorFlow because keras does not implement divide.\n",
        "    tf_div = Lambda(lambda x: tf.divide(x[0], x[1]))\n",
        "\n",
        "    for i in range(num_maps):\n",
        "        h = Input(shape=input_shape)\n",
        "        inputs.append(h)\n",
        "        h_s = act_channel_softmax(h)\n",
        "        p = sam_model(h_s)\n",
        "        v = prob_model(h_s)\n",
        "\n",
        "        if dim == 3:\n",
        "            d = Input(shape=input_shape)\n",
        "            inputs3d.append(d)\n",
        "            d_s = Activation('sigmoid')(d)\n",
        "            dm = multiply([d_s, h_s])\n",
        "            z = Lambda(lambda x: K.sum(x, axis=(1, 2)))(dm)\n",
        "            z = Lambda(lambda x: K.expand_dims(x, axis=-1))(z)\n",
        "            p = concatenate([p, z])\n",
        "\n",
        "        if num_maps > 1:\n",
        "            t = v_tile(v)\n",
        "            p = multiply([p, v_tile(v)])\n",
        "\n",
        "        p_concat.append(p)\n",
        "        v_concat.append(v)\n",
        "\n",
        "    if num_maps > 1:\n",
        "        p = add(p_concat)\n",
        "        v_sum = add(v_concat)\n",
        "        p = tf_div([p, v_tile(v_sum)])\n",
        "        v = maximum(v_concat)\n",
        "    else:\n",
        "        p = p_concat[0]\n",
        "        v = v_concat[0]\n",
        "\n",
        "    model = Model(inputs+inputs3d, [p, v], name=name)\n",
        "    if verbose:\n",
        "        model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_softargmax_1d(input_shape, name=None):\n",
        "\n",
        "    if name is None:\n",
        "        name_sm = None\n",
        "    else:\n",
        "        name_sm = name + '_softmax'\n",
        "\n",
        "    inp = Input(shape=input_shape)\n",
        "    x = act_depth_softmax(inp, name=name_sm)\n",
        "\n",
        "    x = lin_interpolation_1d(x)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=x, name=name)\n",
        "    model.trainable = False\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_softargmax_2d(input_shape, rho=0., name=None):\n",
        "\n",
        "    if name is None:\n",
        "        name_sm = None\n",
        "    else:\n",
        "        name_sm = name + '_softmax'\n",
        "\n",
        "    inp = Input(shape=input_shape)\n",
        "    x = act_channel_softmax(inp, name=name_sm)\n",
        "    if rho > 0:\n",
        "        x = kl_divergence_regularizer(x, rho=rho)\n",
        "\n",
        "    x_x = lin_interpolation_2d(x, axis=0)\n",
        "    x_y = lin_interpolation_2d(x, axis=1)\n",
        "    x = concatenate([x_x, x_y])\n",
        "\n",
        "    model = Model(inputs=inp, outputs=x, name=name)\n",
        "    model.trainable = False\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_joints_probability(input_shape, name=None, verbose=0):\n",
        "\n",
        "    inp = Input(shape=input_shape)\n",
        "\n",
        "    x = inp\n",
        "    x = AveragePooling2D((2, 2), strides=(1, 1))(x)\n",
        "    x = Lambda(lambda x: 4*x)(x)\n",
        "    x = GlobalMaxPooling2D()(x)\n",
        "\n",
        "    x = Lambda(lambda x: K.expand_dims(x, axis=-1))(x)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=x, name=name)\n",
        "    if verbose:\n",
        "        model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "-KvR97a4VWb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def concat_tensorlist(t):\n",
        "    assert isinstance(t, list), 't should be a list, got ({})'.format(t)\n",
        "\n",
        "    if len(t) > 1:\n",
        "        return concatenate(t)\n",
        "    return t[0]\n",
        "\n",
        "\n",
        "def add_tensorlist(t):\n",
        "    assert isinstance(t, list), 't should be a list, got ({})'.format(t)\n",
        "\n",
        "    if len(t) > 1:\n",
        "        return add(t)\n",
        "    return t[0]\n",
        "\n",
        "\n",
        "def residual_unit(x, kernel_size, strides=(1, 1), out_size=None,\n",
        "        convtype='depthwise', shortcut_act=True,\n",
        "        features_div=2, name=None):\n",
        "    \"\"\"(Separable) Residual Unit implementation.\n",
        "    \"\"\"\n",
        "    assert convtype in ['depthwise', 'normal'], \\\n",
        "            'Invalid convtype ({}).'.format(convtype)\n",
        "\n",
        "    num_filters = K.int_shape(x)[-1]\n",
        "    if out_size is None:\n",
        "        out_size = num_filters\n",
        "\n",
        "    skip_conv = (num_filters != out_size) or (strides != (1, 1))\n",
        "\n",
        "    if skip_conv:\n",
        "        x = BatchNormalization(name=appstr(name, '_bn1'))(x)\n",
        "\n",
        "    shortcut = x\n",
        "    if skip_conv:\n",
        "        if shortcut_act:\n",
        "            shortcut = relu(shortcut, name=appstr(name, '_shortcut_act'))\n",
        "        shortcut = conv2d(shortcut, out_size, (1, 1), strides=strides,\n",
        "                name=appstr(name, '_shortcut_conv'))\n",
        "\n",
        "    if not skip_conv:\n",
        "        x = BatchNormalization(name=appstr(name, '_bn1'))(x)\n",
        "    x = relu(x, name=appstr(name, '_act1'))\n",
        "\n",
        "    if convtype == 'depthwise':\n",
        "        x = sepconv2d(x, out_size, kernel_size, strides=strides,\n",
        "                name=appstr(name, '_conv1'))\n",
        "    else:\n",
        "        x = conv2d(x, int(out_size / features_div), (1, 1),\n",
        "                name=appstr(name, '_conv1'))\n",
        "        middle_bn_name = appstr(name, '_bn2')\n",
        "        x = BatchNormalization(name=middle_bn_name)(x)\n",
        "        x = relu(x, name=appstr(name, '_act2'))\n",
        "        x = conv2d(x, out_size, kernel_size, strides=strides,\n",
        "                name=appstr(name, '_conv2'))\n",
        "\n",
        "    x = add([shortcut, x])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def downscaling_unit(x, cfg, out_size=None, name=None):\n",
        "    \"\"\"Downscaling Unit using depth wise separable convolutions\"\"\"\n",
        "\n",
        "    kernel_size = cfg.kernel_size\n",
        "    downsampling_type = cfg.downsampling_type\n",
        "\n",
        "    if out_size is None:\n",
        "        out_size = K.int_shape(x)[-1]\n",
        "\n",
        "    s1 = (2, 2) if downsampling_type == 'conv' else (1, 1)\n",
        "    if downsampling_type == 'maxpooling':\n",
        "        x = maxpooling2d(x, (2, 2))\n",
        "\n",
        "    x = residual_unit(x, kernel_size, out_size=out_size, strides=s1,\n",
        "            name=appstr(name, '_r0'))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def upscaling_unit(x, cfg, out_size=None, name=None):\n",
        "    \"\"\"Upscaling Unit using depth wise separable convolutions\"\"\"\n",
        "\n",
        "    kernel_size = cfg.kernel_size\n",
        "    downsampling_type = cfg.downsampling_type\n",
        "\n",
        "    if out_size is None:\n",
        "        out_size = K.int_shape(x)[-1]\n",
        "\n",
        "    if downsampling_type == 'maxpooling':\n",
        "        x = upsampling2d(x, (2, 2))\n",
        "        x = residual_unit(x, kernel_size, out_size=out_size,\n",
        "                name=appstr(name, '_r0'))\n",
        "    else:\n",
        "        x = BatchNormalization(name=appstr(name, '_bn1'))(x)\n",
        "        x = relu(x, name=appstr(name, '_act1'))\n",
        "        x = conv2dtranspose(x, out_size, (2, 2), strides=(2, 2),\n",
        "                name=appstr(name, '_convtrans1'))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def set_trainable_layers(model, keyword, pos_trainable, neg_trainable=None):\n",
        "\n",
        "    def trainable_flag(curr, newval):\n",
        "        return newval if newval in [True, False] else curr\n",
        "\n",
        "    for i in range(len(model.layers)):\n",
        "        name = model.layers[i].name\n",
        "        if '_xy_x' in name or '_xy_y' in name \\\n",
        "                or '_xy2_x' in name or '_xy2_y' in name:\n",
        "            warning('Unchanged layer {}'.format(name))\n",
        "            continue\n",
        "\n",
        "        if keyword in name:\n",
        "            model.layers[i].trainable = \\\n",
        "                    trainable_flag(model.layers[i].trainable, pos_trainable)\n",
        "        else:\n",
        "            model.layers[i].trainable = \\\n",
        "                    trainable_flag(model.layers[i].trainable, neg_trainable)\n",
        "\n",
        "\n",
        "def copy_replica_layers(model):\n",
        "    for i in range(len(model.layers)):\n",
        "        if '_replica' in model.layers[i].name:\n",
        "            rname = model.layers[i].name\n",
        "            lname = rname.split('_replica')[0]\n",
        "            worg = model.get_layer(lname).get_weights()\n",
        "            wrep = model.get_layer(rname).get_weights()\n",
        "            wrep[0][:] = worg[0][:]\n",
        "            model.get_layer(rname).set_weights(wrep)\n",
        "\n",
        "\n",
        "def compile_model(model, loss, optimizer, loss_weights=None):\n",
        "\n",
        "    nout = len(model.outputs)\n",
        "    if loss_weights is not None:\n",
        "        if isinstance(loss_weights, list):\n",
        "            assert len(loss_weights) == nout, \\\n",
        "                    'loss_weights incompatible with model'\n",
        "        else:\n",
        "            loss_weights = nout*[loss_weights]\n",
        "\n",
        "    if isinstance(loss, list):\n",
        "        assert nout == len(loss), 'loss not corresponding to the model outputs'\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer, loss_weights=loss_weights)\n",
        "\n",
        "\n",
        "# Aliases.\n",
        "residual = residual_unit\n",
        "downscaling = downscaling_unit\n",
        "upscaling = upscaling_unit"
      ],
      "metadata": {
        "id": "ZiVoRsoDVZrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Define the ReceptionNet for human pose estimation for Keras and TensorFlow.\n",
        "\n",
        "The network is defined as:\n",
        "\n",
        "-------   ------\n",
        "|Input|-->|Stem|--> [...],\n",
        "-------   ------\n",
        "\n",
        "end every block:\n",
        "\n",
        "                     -----------------------------------------------\n",
        "                     |             --------------------------------|\n",
        "           --------- |  ---------- |  ---------      ---------     |\n",
        "    [...]->|rBlockN|--->|SepConvN|--->|RegMapN|-(H)->|fReMapN|--->(+)-->[...]\n",
        "           ---------    ----------    ---------      ---------\n",
        "\n",
        "For dim = 2 (2D poses):\n",
        "\n",
        "                  |-->(sSAM)-------------------\n",
        "         |--(Hs)--|                           |\n",
        "         |        |-->(sjProp)--> *visible*   |\n",
        "    H -> |                                    |\n",
        "         |        |-->(cSAM)----------------(Agg)--> *pose*\n",
        "         |--(Hc)--|                           |\n",
        "                  |-->(cjProp)----------------|\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def _sepconv_residual(x, out_size, name, kernel_size=(3, 3)):\n",
        "    shortcut_name = name + '_shortcut'\n",
        "    reduce_name = name + '_reduce'\n",
        "\n",
        "    num_filters = K.int_shape(x)[-1]\n",
        "    if num_filters == out_size:\n",
        "        ident = x\n",
        "    else:\n",
        "        ident = act_conv_bn(x, out_size, (1, 1), name=shortcut_name)\n",
        "\n",
        "    if out_size < num_filters:\n",
        "        x = act_conv_bn(x, out_size, (1, 1), name=reduce_name)\n",
        "\n",
        "    x = separable_act_conv_bn(x, out_size, kernel_size, name=name)\n",
        "    x = add([ident, x])\n",
        "\n",
        "    return x\n",
        "\n",
        "def _stem(inp, old_model=False):\n",
        "\n",
        "    xi = Input(shape=K.int_shape(inp)[1:]) # 256 x 256 x 3\n",
        "\n",
        "    x = conv_bn_act(xi, 32, (3, 3), strides=(2, 2))\n",
        "    if not old_model:\n",
        "        x = conv_bn_act(x, 32, (3, 3))\n",
        "    x = conv_bn_act(x, 64, (3, 3))\n",
        "\n",
        "    if old_model:\n",
        "        a = conv_bn_act(x, 32, (3, 3), strides=(2, 2))\n",
        "    else:\n",
        "        a = conv_bn_act(x, 96, (3, 3), strides=(2, 2))\n",
        "    b = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = concatenate([a, b])\n",
        "\n",
        "    a = conv_bn_act(x, 64, (1, 1))\n",
        "    a = conv_bn(a, 96, (3, 3))\n",
        "    b = conv_bn_act(x, 64, (1, 1))\n",
        "    b = conv_bn_act(b, 64, (5, 1))\n",
        "    b = conv_bn_act(b, 64, (1, 5))\n",
        "    b = conv_bn(b, 96, (3, 3))\n",
        "    x = concatenate([a, b])\n",
        "\n",
        "    a = act_conv_bn(x, 192, (3, 3), strides=(2, 2))\n",
        "    b = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "    x = concatenate([a, b])\n",
        "\n",
        "    if not old_model:\n",
        "        x = _sepconv_residual(x, 3*192, name='sepconv1')\n",
        "\n",
        "    model = Model(xi, x, name='Stem')\n",
        "    x = model(inp)\n",
        "\n",
        "    if old_model:\n",
        "        x = _sepconv_residual(x, 512, name='sepconv1')\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_reception_block(inp, name, ksize=(3, 3)):\n",
        "    input_shape = K.int_shape(inp)[1:]\n",
        "    size = input_shape[-1]\n",
        "\n",
        "    xi = Input(shape=input_shape)\n",
        "    a = _sepconv_residual(xi, size, name='sepconv_l1', kernel_size=ksize)\n",
        "\n",
        "    low1 = MaxPooling2D((2, 2))(xi)\n",
        "    low1 = act_conv_bn(low1, int(size/2), (1, 1))\n",
        "    low1 = _sepconv_residual(low1, int(size/2), name='sepconv_l2_1',\n",
        "            kernel_size=ksize)\n",
        "    b = _sepconv_residual(low1, int(size/2), name='sepconv_l2_2',\n",
        "            kernel_size=ksize)\n",
        "\n",
        "    c = MaxPooling2D((2, 2))(low1)\n",
        "    c = _sepconv_residual(c, int(size/2), name='sepconv_l3_1',\n",
        "            kernel_size=ksize)\n",
        "    c = _sepconv_residual(c, int(size/2), name='sepconv_l3_2',\n",
        "            kernel_size=ksize)\n",
        "    c = _sepconv_residual(c, int(size/2), name='sepconv_l3_3',\n",
        "            kernel_size=ksize)\n",
        "    c = UpSampling2D((2, 2))(c)\n",
        "\n",
        "    b = add([b, c])\n",
        "    b = _sepconv_residual(b, size, name='sepconv_l2_3', kernel_size=ksize)\n",
        "    b = UpSampling2D((2, 2))(b)\n",
        "    x = add([a, b])\n",
        "\n",
        "    model = Model(inputs=xi, outputs=x, name=name)\n",
        "\n",
        "    return model(inp)\n",
        "\n",
        "\n",
        "def build_sconv_block(inp, name=None, ksize=(3, 3)):\n",
        "    input_shape = K.int_shape(inp)[1:]\n",
        "\n",
        "    xi = Input(shape=input_shape)\n",
        "    x = separable_act_conv_bn(xi, input_shape[-1], ksize)\n",
        "\n",
        "    model = Model(inputs=xi, outputs=x, name=name)\n",
        "\n",
        "    return model(inp)\n",
        "\n",
        "\n",
        "def build_regmap_block(inp, num_maps, name=None):\n",
        "    input_shape = K.int_shape(inp)[1:]\n",
        "\n",
        "    xi = Input(shape=input_shape)\n",
        "    x = act_conv(xi, num_maps, (1, 1))\n",
        "\n",
        "    model = Model(inputs=xi, outputs=x, name=name)\n",
        "\n",
        "    return model(inp)\n",
        "\n",
        "\n",
        "def build_fremap_block(inp, num_filters, name=None):\n",
        "    input_shape = K.int_shape(inp)[1:]\n",
        "\n",
        "    xi = Input(shape=input_shape)\n",
        "    x = act_conv_bn(xi, num_filters, (1, 1))\n",
        "\n",
        "    model = Model(inputs=xi, outputs=x, name=name)\n",
        "\n",
        "    return model(inp)\n",
        "\n",
        "\n",
        "def pose_regression_2d_context(h, num_joints, sam_s_model,\n",
        "        sam_c_model, jprob_c_model, agg_model, jprob_s_model):\n",
        "\n",
        "    # Split heatmaps for specialized and contextual information\n",
        "    hs = Lambda(lambda x: x[:,:,:,:num_joints])(h)\n",
        "    hc = Lambda(lambda x: x[:,:,:,num_joints:])(h)\n",
        "\n",
        "    # Soft-argmax and joint probability for each heatmap\n",
        "    ps = sam_s_model(hs)\n",
        "    pc = sam_c_model(hc)\n",
        "    vc = jprob_c_model(hc)\n",
        "\n",
        "    pose = agg_model([ps, pc, vc])\n",
        "    visible = jprob_s_model(hs)\n",
        "\n",
        "    return pose, visible, hs\n",
        "\n",
        "\n",
        "def pose_regression_2d(h, sam_s_model, jprob_s_model):\n",
        "\n",
        "    pose = sam_s_model(h)\n",
        "    visible = jprob_s_model(h)\n",
        "\n",
        "    return pose, visible, h\n",
        "\n",
        "\n",
        "def pose_regression_3d(h, num_joints, depth_maps, sam_s_model, sam_z_model):\n",
        "    assert K.int_shape(h)[-1] == depth_maps * num_joints\n",
        "\n",
        "    def _reshape_heatmaps(x):\n",
        "        x = K.expand_dims(x, axis=-1)\n",
        "        x = K.reshape(x, (-1, K.int_shape(x)[1], K.int_shape(x)[2],\n",
        "            depth_maps, num_joints))\n",
        "\n",
        "        return x\n",
        "\n",
        "    h = Lambda(_reshape_heatmaps)(h)\n",
        "    hxy = Lambda(lambda x: K.mean(x, axis=3))(h)\n",
        "    hz = Lambda(lambda x: K.mean(x, axis=(1, 2)))(h)\n",
        "    pxy = sam_s_model(hxy)\n",
        "    pz = sam_z_model(hz)\n",
        "    pose = concatenate([pxy, pz])\n",
        "\n",
        "    vxy = GlobalMaxPooling2D()(hxy)\n",
        "    vz = GlobalMaxPooling1D()(hz)\n",
        "    v = add([vxy, vz])\n",
        "    v = Lambda(lambda x: K.expand_dims(x, axis=-1))(v)\n",
        "    visible = Activation('sigmoid')(v)\n",
        "\n",
        "    return pose, visible, hxy\n",
        "\n",
        "\n",
        "def build(input_shape, num_joints, dim,\n",
        "        num_context_per_joint=None,\n",
        "        alpha=0.8,\n",
        "        num_blocks=4,\n",
        "        depth_maps=16,\n",
        "        ksize=(3, 3),\n",
        "        export_heatmaps=False,\n",
        "        export_vfeat_block=None,\n",
        "        old_model=False,\n",
        "        concat_pose_confidence=True):\n",
        "\n",
        "    if dim == 2:\n",
        "        if num_context_per_joint is None:\n",
        "            num_context_per_joint = 2\n",
        "\n",
        "        num_heatmaps = (num_context_per_joint + 1) * num_joints\n",
        "\n",
        "    elif dim == 3:\n",
        "        assert num_context_per_joint == None, \\\n",
        "                'For 3D pose estimation, contextual heat maps are not allowed.'\n",
        "        num_heatmaps = depth_maps * num_joints\n",
        "    else:\n",
        "        raise ValueError('\"dim\" must be 2 or 3 and not (%d)' % dim)\n",
        "\n",
        "    inp = Input(shape=input_shape)\n",
        "    outputs = []\n",
        "    vfeat = None\n",
        "\n",
        "    x = _stem(inp, old_model=old_model)\n",
        "\n",
        "    num_rows, num_cols, num_filters = K.int_shape(x)[1:]\n",
        "\n",
        "    # Build the soft-argmax models (no parameters) for specialized and\n",
        "    # contextual maps.\n",
        "    sams_input_shape = (num_rows, num_cols, num_joints)\n",
        "    sam_s_model = build_softargmax_2d(sams_input_shape, rho=0, name='sSAM')\n",
        "    jprob_s_model = build_joints_probability(sams_input_shape, name='sjProb')\n",
        "\n",
        "    # Build the aggregation model (no parameters)\n",
        "    if num_context_per_joint is not None:\n",
        "        samc_input_shape = (num_rows, num_cols, num_heatmaps - num_joints)\n",
        "        sam_c_model = build_softargmax_2d(samc_input_shape, rho=0,\n",
        "                name='cSAM')\n",
        "        jprob_c_model = build_joints_probability(samc_input_shape,\n",
        "                name='cjProb')\n",
        "        agg_model = build_context_aggregation(num_joints,\n",
        "                num_context_per_joint, alpha, name='Agg')\n",
        "\n",
        "    if dim == 3:\n",
        "        samz_input_shape = (depth_maps, num_joints)\n",
        "        sam_z_model = build_softargmax_1d(samz_input_shape, name='zSAM')\n",
        "\n",
        "    for bidx in range(num_blocks):\n",
        "        block_shape = K.int_shape(x)[1:]\n",
        "        x = build_reception_block(x, name='rBlock%d' % (bidx + 1), ksize=ksize)\n",
        "\n",
        "        if export_vfeat_block == (bidx+1):\n",
        "            vfeat = x\n",
        "\n",
        "        ident_map = x\n",
        "        x = build_sconv_block(x, name='SepConv%d' % (bidx + 1), ksize=ksize)\n",
        "        h = build_regmap_block(x, num_heatmaps, name='RegMap%d' % (bidx + 1))\n",
        "\n",
        "        if dim == 2:\n",
        "            if num_context_per_joint is not None:\n",
        "                pose, visible, hm = pose_regression_2d_context(h, num_joints,\n",
        "                        sam_s_model, sam_c_model, jprob_c_model, agg_model,\n",
        "                        jprob_s_model)\n",
        "            else:\n",
        "                pose, visible, hm = pose_regression_2d(h, sam_s_model,\n",
        "                        jprob_s_model)\n",
        "        else:\n",
        "            pose, visible, hm = pose_regression_3d(h, num_joints, depth_maps,\n",
        "                    sam_s_model, sam_z_model)\n",
        "\n",
        "        if concat_pose_confidence:\n",
        "            outputs.append(concatenate([pose, visible]))\n",
        "        else:\n",
        "            outputs.append(pose)\n",
        "            outputs.append(visible)\n",
        "\n",
        "        if export_heatmaps:\n",
        "            outputs.append(hm)\n",
        "\n",
        "        if bidx < num_blocks - 1:\n",
        "            h = build_fremap_block(h, block_shape[-1],\n",
        "                    name='fReMap%d' % (bidx + 1))\n",
        "            x = add([ident_map, x, h])\n",
        "\n",
        "    if vfeat is not None:\n",
        "        outputs.append(vfeat)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def compile(model, ptr, vtr, num_y_per_branch=1):\n",
        "    \"\"\"Create a list with ground truth, loss functions and loss weights.\n",
        "    \"\"\"\n",
        "    yholder_tr = []\n",
        "    losses = []\n",
        "    loss_weights = []\n",
        "    num_blocks = int(len(model.output) / (num_y_per_branch + 1))\n",
        "\n",
        "    printcn(OKBLUE,\n",
        "            'Compiling model with %d outputs per branch and %d branches.' %\n",
        "            (num_y_per_branch, num_blocks))\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "        for j in range(num_y_per_branch):\n",
        "            yholder_tr.append(ptr)\n",
        "            losses.append(elasticnet_loss_on_valid_joints)\n",
        "            loss_weights.append(1.)\n",
        "        yholder_tr.append(vtr)\n",
        "        losses.append('binary_crossentropy')\n",
        "        loss_weights.append(0.01)\n",
        "\n",
        "    printcn(OKBLUE, 'loss_weights: ' + str(loss_weights))\n",
        "    model.compile(loss=losses, optimizer=tf.keras.optimizers.legacy.RMSprop, loss_weights=loss_weights)\n",
        "\n",
        "    return yholder_tr"
      ],
      "metadata": {
        "id": "A2RSIIu7VdNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General filing**"
      ],
      "metadata": {
        "id": "Jzpix4O3VkEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SaveModel(Callback):\n",
        "\n",
        "    def __init__(self, filepath, model_to_save=None, save_best_only=False,\n",
        "            callback_to_monitor=None, verbose=1):\n",
        "\n",
        "        if save_best_only and callback_to_monitor is None:\n",
        "            warning('Cannot save the best model with no callback monitor')\n",
        "\n",
        "        self.filepath = filepath\n",
        "        self.model_to_save = model_to_save\n",
        "        self.save_best_only = save_best_only\n",
        "        self.callback_to_monitor = callback_to_monitor\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.model_to_save is not None:\n",
        "            model = self.model_to_save\n",
        "        else:\n",
        "            model = self.model\n",
        "\n",
        "        filename = self.filepath.format(epoch=epoch + 1)\n",
        "\n",
        "        if self.best_epoch == epoch + 1 or not self.save_best_only:\n",
        "            if self.verbose:\n",
        "                printnl('Saving model @epoch=%05d to %s' \\\n",
        "                        % (epoch + 1, filename))\n",
        "            model.save_weights(filename)\n",
        "\n",
        "    @property\n",
        "    def best_epoch(self):\n",
        "        if self.callback_to_monitor is not None:\n",
        "            return self.callback_to_monitor.best_epoch\n",
        "        else:\n",
        "            return None"
      ],
      "metadata": {
        "id": "h5zvUevlVgWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class DataConfig(object):\n",
        "    \"\"\"Input frame configuration and data augmentation setup.\"\"\"\n",
        "\n",
        "    def __init__(self, crop_resolution=(256, 256), image_channels=(3,),\n",
        "            angles=[0], fixed_angle=0,\n",
        "            scales=[1], fixed_scale=1,\n",
        "            trans_x=[0], fixed_trans_x=0,\n",
        "            trans_y=[0], fixed_trans_y=0,\n",
        "            hflips=[0, 1], fixed_hflip=0,\n",
        "            chpower=0.01*np.array(range(90, 110+1, 2)), fixed_chpower=1,\n",
        "            geoocclusion=None, fixed_geoocclusion=None,\n",
        "            subsampling=[1], fixed_subsampling=1):\n",
        "\n",
        "        self.crop_resolution = crop_resolution\n",
        "        self.image_channels = image_channels\n",
        "        if K.image_data_format() == 'channels_last':\n",
        "            self.input_shape = crop_resolution + image_channels\n",
        "        else:\n",
        "            self.input_shape = image_channels + crop_resolution\n",
        "        self.angles = angles\n",
        "        self.fixed_angle = fixed_angle\n",
        "        self.scales = scales\n",
        "        self.fixed_scale = fixed_scale\n",
        "        self.trans_x = trans_x\n",
        "        self.trans_y = trans_y\n",
        "        self.fixed_trans_x = fixed_trans_x\n",
        "        self.fixed_trans_y = fixed_trans_y\n",
        "        self.hflips = hflips\n",
        "        self.fixed_hflip = fixed_hflip\n",
        "        self.chpower = chpower\n",
        "        self.fixed_chpower = fixed_chpower\n",
        "        self.geoocclusion = geoocclusion\n",
        "        self.fixed_geoocclusion = fixed_geoocclusion\n",
        "        self.subsampling = subsampling\n",
        "        self.fixed_subsampling = fixed_subsampling\n",
        "\n",
        "    def get_fixed_config(self):\n",
        "        return {'angle': self.fixed_angle,\n",
        "                'scale': self.fixed_scale,\n",
        "                'transx': self.fixed_trans_x,\n",
        "                'transy': self.fixed_trans_y,\n",
        "                'hflip': self.fixed_hflip,\n",
        "                'chpower': self.fixed_chpower,\n",
        "                'geoocclusion': self.fixed_geoocclusion,\n",
        "                'subspl': self.fixed_subsampling}\n",
        "\n",
        "    def random_data_generator(self):\n",
        "        angle = DataConfig._getrand(self.angles)\n",
        "        scale = DataConfig._getrand(self.scales)\n",
        "        trans_x = DataConfig._getrand(self.trans_x)\n",
        "        trans_y = DataConfig._getrand(self.trans_y)\n",
        "        hflip = DataConfig._getrand(self.hflips)\n",
        "        chpower = (DataConfig._getrand(self.chpower),\n",
        "                DataConfig._getrand(self.chpower),\n",
        "                DataConfig._getrand(self.chpower))\n",
        "        geoocclusion = self.__get_random_geoocclusion()\n",
        "        subsampling = DataConfig._getrand(self.subsampling)\n",
        "\n",
        "        return {'angle': angle,\n",
        "                'scale': scale,\n",
        "                'transx': trans_x,\n",
        "                'transy': trans_y,\n",
        "                'hflip': hflip,\n",
        "                'chpower': chpower,\n",
        "                'geoocclusion': geoocclusion,\n",
        "                'subspl': subsampling}\n",
        "\n",
        "    def __get_random_geoocclusion(self):\n",
        "        if self.geoocclusion is not None:\n",
        "\n",
        "            w = int(DataConfig._getrand(self.geoocclusion) / 2)\n",
        "            h = int(DataConfig._getrand(self.geoocclusion) / 2)\n",
        "            xmin = w + 1\n",
        "            xmax = self.crop_resolution[0] - xmin\n",
        "            ymin = h + 1\n",
        "            ymax = self.crop_resolution[1] - ymin\n",
        "\n",
        "            x = DataConfig._getrand(range(xmin, xmax, 5))\n",
        "            y = DataConfig._getrand(range(ymin, ymax, 5))\n",
        "            bbox = (x-w, y-h, x+w, y+h)\n",
        "\n",
        "            return bbox\n",
        "\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def _getrand(x):\n",
        "        return x[np.random.randint(0, len(x))]\n",
        "\n",
        "\n",
        "# Data generation and configuration setup\n",
        "\n",
        "mpii_sp_dataconf = DataConfig(\n",
        "        crop_resolution=(256, 256),\n",
        "        angles=np.array(range(-40, 40+1, 5)),\n",
        "        scales=np.array([0.7, 1., 1.3]),\n",
        "        )\n",
        "\n",
        "pennaction_dataconf = DataConfig(\n",
        "        crop_resolution=(256, 256),\n",
        "        angles=np.array(range(-30, 30+1, 5)),\n",
        "        scales=np.array([0.7, 1.0, 1.3]),\n",
        "        trans_x=np.array(range(-40, 40+1, 5)),\n",
        "        trans_y=np.array(range(-10, 10+1, 5)),\n",
        "        subsampling=[4, 6, 8],\n",
        "        fixed_subsampling=6\n",
        "        )\n",
        "\n",
        "pennaction_pe_dataconf = DataConfig(\n",
        "        crop_resolution=(256, 256),\n",
        "        angles=np.array(range(-40, 40+1, 5)),\n",
        "        scales=np.array([0.7, 1.0, 1.3, 2.0]),\n",
        "        trans_x=np.array(range(-40, 40+1, 5)),\n",
        "        trans_y=np.array(range(-10, 10+1, 5)),\n",
        "        )\n",
        "\n",
        "human36m_dataconf = DataConfig(\n",
        "        crop_resolution=(256, 256),\n",
        "        angles=np.array(range(-10, 10+1, 5)),\n",
        "        scales=np.array([0.8, 1.0, 1.2]),\n",
        "        trans_x=np.array(range(-20, 20+1, 5)),\n",
        "        trans_y=np.array(range(-4, 4+1, 1)),\n",
        "        geoocclusion=np.array(range(20, 90)),\n",
        "        )\n",
        "\n",
        "ntu_dataconf = DataConfig(\n",
        "        crop_resolution=(256, 256),\n",
        "        angles=[0],\n",
        "        scales=np.array([0.7, 1.0, 1.3]),\n",
        "        trans_x=range(-40, 40+1, 5),\n",
        "        trans_y=range(-10, 10+1, 5),\n",
        "        subsampling=[3, 4, 5],\n",
        "        fixed_subsampling=4\n",
        "        )\n",
        "\n",
        "ntu_pe_dataconf = DataConfig(\n",
        "        crop_resolution=(256, 256),\n",
        "        angles=np.array(range(-10, 10+1, 5)),\n",
        "        scales=np.array([0.7, 1.0, 1.3, 2.0]),\n",
        "        trans_x=np.array(range(-40, 40+1, 5)),\n",
        "        trans_y=np.array(range(-10, 10+1, 5)),\n",
        "        )\n",
        "\n",
        "class ModelConfig(object):\n",
        "    \"\"\"Hyperparameters for models.\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, poselayout,\n",
        "            num_actions=[],\n",
        "            num_pyramids=8,\n",
        "            action_pyramids=[1, 2], # list of pyramids to perform AR\n",
        "            num_levels=4,\n",
        "            kernel_size=(5, 5),\n",
        "            growth=96,\n",
        "            image_div=8,\n",
        "            predict_rootz=False,\n",
        "            downsampling_type='maxpooling',\n",
        "            pose_replica=False,\n",
        "            num_pose_features=128,\n",
        "            num_visual_features=128,\n",
        "            sam_alpha=1,\n",
        "            dbg_decoupled_pose=False,\n",
        "            dbg_decoupled_h=False):\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "        self.num_joints = poselayout.num_joints\n",
        "        self.dim = poselayout.dim\n",
        "\n",
        "        assert type(num_actions) == list, 'num_actions should be a list'\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        self.num_pyramids = num_pyramids\n",
        "        self.action_pyramids = action_pyramids\n",
        "        self.num_levels = num_levels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.growth = growth\n",
        "        self.image_div = image_div\n",
        "        self.predict_rootz = predict_rootz\n",
        "        self.downsampling_type = downsampling_type\n",
        "        self.pose_replica = pose_replica\n",
        "        self.num_pose_features = num_pose_features\n",
        "        self.num_visual_features = num_visual_features\n",
        "        self.sam_alpha = sam_alpha\n",
        "\n",
        "        \"\"\"Debugging flags.\"\"\"\n",
        "        self.dbg_decoupled_pose = dbg_decoupled_pose\n",
        "        self.dbg_decoupled_h = dbg_decoupled_h\n",
        "\n",
        "# Aliases.\n",
        "mpii_dataconf = mpii_sp_dataconf"
      ],
      "metadata": {
        "id": "GJoZkXPGVoSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def elasticnet_bincross_loss_on_valid_joints(y_true, y_pred):\n",
        "    idx = K.cast(K.greater(y_true, 0.), 'float32')\n",
        "    num_joints = K.clip(K.sum(idx, axis=(-1, -2)), 1, None)\n",
        "\n",
        "    l1 = K.abs(y_pred - y_true)\n",
        "    l2 = K.square(y_pred - y_true)\n",
        "    bc = 0.01*K.binary_crossentropy(y_true, y_pred)\n",
        "    dummy = 0. * y_pred\n",
        "\n",
        "    return K.sum(tf.where(K.cast(idx, 'bool'), l1 + l2 + bc, dummy),\n",
        "            axis=(-1, -2)) / num_joints\n",
        "\n",
        "\n",
        "def l1_loss_on_valid_joints(y_true, y_pred):\n",
        "    y_true, y_pred, num_joints = _reset_invalid_joints(y_true, y_pred)\n",
        "    return K.sum(K.abs(y_pred - y_true), axis=(-1, -2)) / num_joints\n",
        "\n",
        "\n",
        "def l2_loss_on_valid_joints(y_true, y_pred):\n",
        "    y_true, y_pred, num_joints = _reset_invalid_joints(y_true, y_pred)\n",
        "    return K.sum(K.square(y_pred - y_true), axis=(-1, -2)) / num_joints\n",
        "\n",
        "\n",
        "def pose_regression_loss(pose_loss, visibility_weight):\n",
        "\n",
        "    def _pose_regression_loss(y_true, y_pred):\n",
        "        video_clip = K.ndim(y_true) == 4\n",
        "        if video_clip:\n",
        "            \"\"\"The model was time-distributed, so there is one additional\n",
        "            dimension.\n",
        "            \"\"\"\n",
        "            p_true = y_true[:, :, :, 0:-1]\n",
        "            p_pred = y_pred[:, :, :, 0:-1]\n",
        "            v_true = y_true[:, :, :, -1]\n",
        "            v_pred = y_pred[:, :, :, -1]\n",
        "        else:\n",
        "            p_true = y_true[:, :, 0:-1]\n",
        "            p_pred = y_pred[:, :, 0:-1]\n",
        "            v_true = y_true[:, :, -1]\n",
        "            v_pred = y_pred[:, :, -1]\n",
        "\n",
        "        if pose_loss == 'l1l2':\n",
        "            ploss = elasticnet_loss_on_valid_joints(p_true, p_pred)\n",
        "        elif pose_loss == 'l1':\n",
        "            ploss = l1_loss_on_valid_joints(p_true, p_pred)\n",
        "        elif pose_loss == 'l2':\n",
        "            ploss = l2_loss_on_valid_joints(p_true, p_pred)\n",
        "        elif pose_loss == 'l1l2bincross':\n",
        "            ploss = elasticnet_bincross_loss_on_valid_joints(p_true, p_pred)\n",
        "        else:\n",
        "            raise Exception('Invalid pose_loss option ({})'.format(pose_loss))\n",
        "\n",
        "        vloss = binary_crossentropy(v_true, v_pred)\n",
        "\n",
        "        if video_clip:\n",
        "            \"\"\"If time-distributed, average the error on video frames.\"\"\"\n",
        "            vloss = K.mean(vloss, axis=-1)\n",
        "            ploss = K.mean(ploss, axis=-1)\n",
        "\n",
        "        return ploss + visibility_weight*vloss\n",
        "\n",
        "    return _pose_regression_loss"
      ],
      "metadata": {
        "id": "z9qAtOGxVrCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _norm(x, axis=None):\n",
        "    return np.sqrt(np.sum(np.power(x, 2), axis=axis))\n",
        "\n",
        "def _valid_joints(y, min_valid=-1e6):\n",
        "    def and_all(x):\n",
        "        if x.all():\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    return np.apply_along_axis(and_all, axis=1, arr=(y > min_valid))\n",
        "\n",
        "def mean_distance_error(y_true, y_pred):\n",
        "    \"\"\"Compute the mean distance error on predicted samples, considering\n",
        "    only the valid joints from y_true.\n",
        "\n",
        "    # Arguments\n",
        "        y_true: [num_samples, nb_joints, dim]\n",
        "        y_pred: [num_samples, nb_joints, dim]\n",
        "\n",
        "    # Return\n",
        "        The mean absolute error on valid joints.\n",
        "    \"\"\"\n",
        "\n",
        "    assert y_true.shape == y_pred.shape\n",
        "    num_samples = len(y_true)\n",
        "\n",
        "    dist = np.zeros(y_true.shape[0:2])\n",
        "    valid = np.zeros(y_true.shape[0:2])\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        valid[i,:] = _valid_joints(y_true[i])\n",
        "        dist[i,:] = _norm(y_true[i] - y_pred[i], axis=1)\n",
        "\n",
        "    match = dist * valid\n",
        "    # print ('Maximum valid distance: {}'.format(match.max()))\n",
        "    # print ('Average valid distance: {}'.format(match.mean()))\n",
        "\n",
        "    return match.sum() / valid.sum()\n",
        "\n",
        "def pckh(y_true, y_pred, head_size, refp=0.5):\n",
        "    \"\"\"Compute the PCKh measure (using refp of the head size) on predicted\n",
        "    samples.\n",
        "\n",
        "    # Arguments\n",
        "        y_true: [num_samples, nb_joints, 2]\n",
        "        y_pred: [num_samples, nb_joints, 2]\n",
        "        head_size: [num_samples, 1]\n",
        "\n",
        "    # Return\n",
        "        The PCKh score.\n",
        "    \"\"\"\n",
        "\n",
        "    assert y_true.shape == y_pred.shape\n",
        "    assert len(y_true) == len(head_size)\n",
        "    num_samples = len(y_true)\n",
        "\n",
        "    # Ignore the joints 6 and 7 (pelvis and thorax respectively), according\n",
        "    # to the file 'annolist2matrix.m'\n",
        "    used_joints = [2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 8, 9]\n",
        "    y_true = y_true[:, used_joints, :]\n",
        "    y_pred = y_pred[:, used_joints, :]\n",
        "    dist = np.zeros((num_samples, len(used_joints)))\n",
        "    valid = np.zeros((num_samples, len(used_joints)))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        valid[i,:] = _valid_joints(y_true[i])\n",
        "        dist[i,:] = _norm(y_true[i] - y_pred[i], axis=1) / head_size[i]\n",
        "    match = (dist <= refp) * valid\n",
        "\n",
        "    return match.sum() / valid.sum()\n",
        "\n",
        "\n",
        "def pck3d(y_true, y_pred, refp=150):\n",
        "    \"\"\"Compute the PCK3D measure (using refp as the threshold) on predicted\n",
        "    samples.\n",
        "\n",
        "    # Arguments\n",
        "        y_true: [num_samples, nb_joints, 3]\n",
        "        y_pred: [num_samples, nb_joints, 3]\n",
        "\n",
        "    # Return\n",
        "        The PCKh score.\n",
        "    \"\"\"\n",
        "\n",
        "    assert y_true.shape == y_pred.shape\n",
        "    num_samples = len(y_true)\n",
        "\n",
        "    # Ignore the joints 6 and 7 (pelvis and thorax respectively), according\n",
        "    # to the file 'annolist2matrix.m'\n",
        "    used_joints = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
        "    y_true = y_true[:, used_joints, :]\n",
        "    y_pred = y_pred[:, used_joints, :]\n",
        "    dist = np.zeros((num_samples, len(used_joints)))\n",
        "    valid = np.zeros((num_samples, len(used_joints)))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        valid[i,:] = _valid_joints(y_true[i])\n",
        "        dist[i,:] = _norm(y_true[i] - y_pred[i], axis=1)\n",
        "    match = (dist <= refp) * valid\n",
        "\n",
        "    return match.sum() / valid.sum()\n",
        "\n",
        "\n",
        "def pckh_per_joint(y_true, y_pred, head_size, pose_layout, refp=0.5, verbose=1):\n",
        "    \"\"\"Compute the PCKh measure (using refp of the head size) on predicted\n",
        "    samples per joint and output the results.\n",
        "\n",
        "    # Arguments\n",
        "        y_true: [num_samples, nb_joints, 2]\n",
        "        y_pred: [num_samples, nb_joints, 2]\n",
        "        head_size: [num_samples, 1]\n",
        "        pose_layout: from deephar.utils.pose\n",
        "    \"\"\"\n",
        "\n",
        "    assert y_true.shape == y_pred.shape\n",
        "    assert len(y_true) == len(head_size)\n",
        "\n",
        "    num_samples = len(y_true)\n",
        "    num_joints = pose_layout.num_joints\n",
        "    dist = np.zeros((num_samples, num_joints))\n",
        "    valid = np.zeros((num_samples, num_joints))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        valid[i,:] = _valid_joints(y_true[i])\n",
        "        dist[i,:] = _norm(y_true[i] - y_pred[i], axis=1) / head_size[i]\n",
        "\n",
        "    for j in range(num_joints):\n",
        "        jname = pose_layout.joint_names[j]\n",
        "        space = 7*' '\n",
        "        ss = len(space) - len(jname)\n",
        "        if verbose:\n",
        "            printc(HEADER, jname + space[0:ss] + '| ')\n",
        "    if verbose:\n",
        "        print ('')\n",
        "\n",
        "    match = (dist <= refp) * valid\n",
        "    for j in range(num_joints):\n",
        "        pck = match[:, j].sum() / valid[:, j].sum()\n",
        "        if verbose:\n",
        "            printc(OKBLUE, ' %.2f | ' % (100 * pck))\n",
        "    if verbose:\n",
        "        print ('')\n",
        "\n",
        "\n",
        "def pck_torso(y_true, y_pred, refp=0.2):\n",
        "    \"\"\" Compute the PCK (using 0.2 of the torso size) on predicted samples.\n",
        "\n",
        "        Input:  y_true [nb_samples, nb_joints, 2]\n",
        "                y_pred [nb_samples, nb_joints, 2]\n",
        "\n",
        "        Return: The PCK score [1]\n",
        "    \"\"\"\n",
        "    assert y_true.shape == y_pred.shape\n",
        "    nb_samples, _, nb_joints = y_true.shape\n",
        "\n",
        "    dist = np.zeros((nb_samples, nb_joints))\n",
        "    valid = np.zeros((nb_samples, nb_joints))\n",
        "    torso = _norm(y_true[:,:,5] - y_true[:,:,10], axis=1)\n",
        "\n",
        "    for i in range(nb_samples):\n",
        "        valid[i,:] = _valid_joints(y_true[i])\n",
        "        dist[i,:] = _norm(y_true[i] - y_pred[i], axis=0) / torso[i]\n",
        "    match = (dist <= refp) * valid\n",
        "\n",
        "    return match.sum() / valid.sum()"
      ],
      "metadata": {
        "id": "y_kqZOAGVt9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def elasticnet_loss_on_valid_joints(y_true, y_pred):\n",
        "    idx = K.cast(K.greater(y_true, -1e6), 'float32')\n",
        "    y_true = idx * y_true\n",
        "    y_pred = idx * y_pred\n",
        "    l1 = K.sum(K.abs(y_pred - y_true), axis=(-2, -1))\n",
        "    l2 = K.sum(K.square(y_pred - y_true), axis=(-2, -1))\n",
        "    return l1 + l2\n"
      ],
      "metadata": {
        "id": "uNs6u3tNVw5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainerOnGenerator(object):\n",
        "    \"\"\"This class basically is a wrapper to the method 'fit_generator' from\n",
        "    Keras, despite that it also can configure user callbacks, tensorboard,\n",
        "    learning rate scheduler, and model saving.\n",
        "\n",
        "    The built-in learning rate scheduler depends on a validation callback\n",
        "    with an attribute 'best_epoch'.\n",
        "\n",
        "    # Arguments\n",
        "        logdir: Path to where all the logs and weights will be saved.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, gen_tr, gpu_model=None, steps_per_epoch=None,\n",
        "            initial_lr=1e-3, lr_factor=0.1, lr_patience=10, minimum_lr=1e-7,\n",
        "            epochs=1, verbose=1, workers=1, shuffle=True, initial_epoch=0,\n",
        "            validation_callbacks=None, custom_lr_scheduler=None,\n",
        "            save_tensor_board=False, weights_fname='weights.hdf5', logdir=None):\n",
        "\n",
        "        self.model = model\n",
        "        if gpu_model is not None:\n",
        "            self.gpu_model = gpu_model\n",
        "        else:\n",
        "            self.gpu_model = model\n",
        "\n",
        "        self.gen_tr = gen_tr\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "        self.initial_lr = initial_lr\n",
        "        self.lr_factor = lr_factor\n",
        "        self.lr_patience = lr_patience\n",
        "        self.lr_wait = 0\n",
        "        self.minimum_lr = minimum_lr\n",
        "\n",
        "        self.epochs = epochs\n",
        "        self.verbose = verbose\n",
        "        self.workers = workers\n",
        "        self.shuffle = shuffle\n",
        "        self.initial_epoch = initial_epoch\n",
        "\n",
        "        self.val_cb = validation_callbacks\n",
        "        self.callbacks = []\n",
        "        self.weights_fname = weights_fname\n",
        "        self.logdir = logdir\n",
        "\n",
        "        if self.val_cb is not None:\n",
        "            if not isinstance(self.val_cb, list):\n",
        "                self.val_cb = [self.val_cb]\n",
        "\n",
        "            self.callbacks += self.val_cb\n",
        "\n",
        "            if custom_lr_scheduler is None:\n",
        "                lrscheduler = LearningRateScheduler(\n",
        "                        self.learningrate_scheduler)\n",
        "                self.callbacks.append(lrscheduler)\n",
        "\n",
        "        if custom_lr_scheduler is not None:\n",
        "                lrscheduler = LearningRateScheduler(custom_lr_scheduler)\n",
        "                self.callbacks.append(lrscheduler)\n",
        "\n",
        "        if (self.logdir is not None) and save_tensor_board:\n",
        "            tensorboard = TensorBoard(log_dir=self.logdir)\n",
        "            self.callbacks.append(tensorboard)\n",
        "\n",
        "        if len(self.callbacks) == 0:\n",
        "            self.callbacks = None # Reset if not used\n",
        "\n",
        "\n",
        "    def learningrate_scheduler(self, epoch, lr):\n",
        "        best_epoch = self.val_cb[-1].best_epoch\n",
        "        if epoch == self.initial_epoch:\n",
        "            lr = self.initial_lr\n",
        "\n",
        "        elif best_epoch == epoch:\n",
        "            self.lr_wait = 0\n",
        "            if self.logdir is not None:\n",
        "                self.model.save_weights(\n",
        "                        os.path.join(self.logdir, self.weights_fname))\n",
        "        else:\n",
        "            \"\"\"Increase the waiting time if it was not the best epoch.\"\"\"\n",
        "            self.lr_wait += 1\n",
        "\n",
        "        if self.lr_wait >= self.lr_patience:\n",
        "            self.lr_wait = 0\n",
        "\n",
        "            \"\"\"Reduce the learning rate and (re)load the best model.\"\"\"\n",
        "            lr *= self.lr_factor\n",
        "\n",
        "            if self.logdir is not None:\n",
        "                printcn(OKGREEN,\n",
        "                        'Reloading weights from epoch %03d' % best_epoch)\n",
        "                self.model.load_weights(\n",
        "                        os.path.join(self.logdir, self.weights_fname))\n",
        "\n",
        "            if lr < self.minimum_lr:\n",
        "                printcn(FAIL, 'Minimum learning rate reached!')\n",
        "                self.gpu_model.stop_training = True\n",
        "            else:\n",
        "                printcn(OKGREEN, 'Setting learning rate to: %g' % lr)\n",
        "\n",
        "        return lr\n",
        "\n",
        "    def train(self):\n",
        "        self.gpu_model.fit_generator(self.gen_tr,\n",
        "                steps_per_epoch=self.steps_per_epoch,\n",
        "                epochs=self.epochs,\n",
        "                verbose=self.verbose,\n",
        "                callbacks=self.callbacks,\n",
        "                workers=self.workers,\n",
        "                use_multiprocessing=False,\n",
        "                shuffle=self.shuffle,\n",
        "                initial_epoch=self.initial_epoch)\n",
        "\n",
        "\n",
        "class MultiModelTrainer(object):\n",
        "    \"\"\"This class is much more than a wrapper to the method 'fit_generator'\n",
        "    from Keras. It is able to train a list o models, given a corresponding\n",
        "    list of data generator (one per model), by training each model with one\n",
        "    batch of its corresponding data. Actually, it is supposed that each model\n",
        "    here is a small part of a bigger model (the full model), which is actually\n",
        "    used for saveing weights.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, models, generators, workers=1, shuffle=True,\n",
        "            max_queue_size=10, print_full_losses=False):\n",
        "\n",
        "        assert len(models) == len(generators), \\\n",
        "                'ValueError: models and generators should be lists of same size'\n",
        "\n",
        "        if type(workers) is not list:\n",
        "            workers = len(models)*[workers]\n",
        "\n",
        "        self.models = models\n",
        "        self.output_generators = []\n",
        "        self.batch_logs = {}\n",
        "        self.print_full_losses = print_full_losses\n",
        "\n",
        "        metric_names = []\n",
        "\n",
        "        batch_size = 0\n",
        "        for i in range(len(models)):\n",
        "            assert isinstance(generators[i], BatchLoader), \\\n",
        "                    'Only BatchLoader class is supported'\n",
        "            batch_size += generators[i].get_batch_size()\n",
        "            enqueuer = OrderedEnqueuer(generators[i], shuffle=shuffle)\n",
        "            enqueuer.start(workers=workers[i], max_queue_size=max_queue_size)\n",
        "            self.output_generators.append(enqueuer.get())\n",
        "\n",
        "            metric_names.append('loss%d' % i)\n",
        "            if self.print_full_losses:\n",
        "                for out in models[i].outputs:\n",
        "                    metric_names.append(out.name.split('/')[0])\n",
        "\n",
        "        self.batch_logs['size'] = batch_size\n",
        "        self.metric_names = metric_names\n",
        "\n",
        "    def train(self, epochs, steps_per_epoch, initial_epoch=0,\n",
        "            end_of_epoch_callback=None, verbose=1):\n",
        "\n",
        "        epoch = initial_epoch\n",
        "\n",
        "        logger = ProgbarLogger(count_mode='steps')\n",
        "        logger.set_params({\n",
        "            'epochs': epochs,\n",
        "            'steps': steps_per_epoch,\n",
        "            'verbose': verbose,\n",
        "            'metrics': self.metric_names})\n",
        "        logger.on_train_begin()\n",
        "\n",
        "        while epoch < epochs:\n",
        "            step = 0\n",
        "            batch = 0\n",
        "\n",
        "            logger.on_epoch_begin(epoch)\n",
        "\n",
        "            while step < steps_per_epoch:\n",
        "\n",
        "                self.batch_logs['batch'] = batch\n",
        "                logger.on_batch_begin(batch, self.batch_logs)\n",
        "\n",
        "                for i in range(len(self.models)):\n",
        "                    x, y = next(self.output_generators[i])\n",
        "                    outs = self.models[i].train_on_batch(x, y)\n",
        "\n",
        "                    if not isinstance(outs, list):\n",
        "                        outs = [outs]\n",
        "                    if self.print_full_losses:\n",
        "                        for l, o in zip(self.metric_names, outs):\n",
        "                            self.batch_logs[l] = o\n",
        "                    else:\n",
        "                        self.batch_logs[self.metric_names[i]] = outs[0]\n",
        "\n",
        "                logger.on_batch_end(batch, self.batch_logs)\n",
        "\n",
        "                step += 1\n",
        "                batch += 1\n",
        "\n",
        "            logger.on_epoch_end(epoch)\n",
        "            if end_of_epoch_callback is not None:\n",
        "                end_of_epoch_callback(epoch)\n",
        "\n",
        "            epoch += 1"
      ],
      "metadata": {
        "id": "3YhkUVIMV0V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Action Recognition Task**"
      ],
      "metadata": {
        "id": "LvepKR9kV6k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_singleclip_gt_bbox(model, x_te, action_te, batch_size=1, verbose=1):\n",
        "\n",
        "    num_blocks = len(model.outputs)\n",
        "    start = time.time()\n",
        "\n",
        "    pred = model.predict(x_te, batch_size=batch_size, verbose=verbose)\n",
        "    dt = time.time() - start\n",
        "\n",
        "    if verbose:\n",
        "        printc(WARNING, 'PennAction, single-clip, action acc.%:')\n",
        "\n",
        "    scores = []\n",
        "    for b in range(num_blocks):\n",
        "\n",
        "        y_pred = pred[b]\n",
        "        correct = np.equal(np.argmax(action_te, axis=-1),\n",
        "                np.argmax(y_pred, axis=-1), dtype=np.float64)\n",
        "        scores.append(sum(correct) / len(correct))\n",
        "\n",
        "        if verbose:\n",
        "            printc(WARNING, ' %.1f' % (100*scores[-1]))\n",
        "\n",
        "    if verbose:\n",
        "        printcn('', '\\n%d samples in %.1f sec: %.1f clips per sec' \\\n",
        "                % (len(x_te), dt, len(x_te) / dt))\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def eval_singleclip_gt_bbox_generator(model, datagen, verbose=1, logdir=None):\n",
        "\n",
        "    num_blocks = len(model.outputs)\n",
        "    num_samples = len(datagen)\n",
        "    start = time.time()\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        [x], [y] = datagen[i]\n",
        "        if 'y_true' not in locals():\n",
        "            y_true = np.zeros((num_samples,) + y.shape[1:])\n",
        "            y_pred = np.zeros((num_samples, num_blocks) + y.shape[1:])\n",
        "\n",
        "        y_true[i, :] = y\n",
        "        pred = model.predict(x)\n",
        "        for b in range(num_blocks):\n",
        "            y_pred[i, b, :] = pred[b]\n",
        "\n",
        "    dt = time.time() - start\n",
        "    if verbose:\n",
        "        printc(WARNING, 'PennAction, single-clip, action acc.%:')\n",
        "\n",
        "    if logdir is not None:\n",
        "        logpath = os.path.join(logdir, 'single-clip')\n",
        "        mkdir(logpath)\n",
        "\n",
        "    scores = []\n",
        "    for b in range(num_blocks):\n",
        "        correct = np.equal(np.argmax(y_true, axis=-1), np.argmax(y_pred[:, b, :], axis=-1))\n",
        "        scores.append(sum(correct) / len(correct))\n",
        "        if verbose:\n",
        "            printc(WARNING, ' %.1f ' % (100*scores[-1]))\n",
        "\n",
        "        if logdir is not None:\n",
        "            np.save(logpath + '/%02d.npy' % b, correct)\n",
        "\n",
        "    if verbose:\n",
        "        printcn('', '\\n%d samples in %.1f sec: %.1f clips per sec' \\\n",
        "                % (num_samples, dt, num_samples / dt))\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def eval_multiclip_dataset(model, penn, subsampling, bboxes_file=None,\n",
        "        logdir=None, verbose=1):\n",
        "    \"\"\"If bboxes_file if not given, use ground truth bounding boxes.\"\"\"\n",
        "\n",
        "    num_samples = penn.get_length(TEST_MODE)\n",
        "    num_blocks = len(model.outputs)\n",
        "\n",
        "    \"\"\"Save and reset some original configs from the dataset.\"\"\"\n",
        "    org_hflip = penn.dataconf.fixed_hflip\n",
        "\n",
        "    cnt_corr = 0\n",
        "    cnt_total = 0\n",
        "\n",
        "    action_shape = (num_samples,) + penn.get_shape('pennaction')\n",
        "    a_true = np.zeros(action_shape)\n",
        "    a_pred = np.ones((num_blocks,) + action_shape)\n",
        "    missing_clips = {}\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        if verbose:\n",
        "            printc(OKBLUE, '%04d/%04d\\t' % (i, num_samples))\n",
        "\n",
        "        frame_list = penn.get_clip_index(i, TEST_MODE, subsamples=[subsampling])\n",
        "\n",
        "        \"\"\"Variable to hold all preditions for this sequence.\n",
        "        2x frame_list due to hflip.\n",
        "        \"\"\"\n",
        "        allpred = np.ones((num_blocks, 2*len(frame_list)) + action_shape[1:])\n",
        "\n",
        "        for f in range(len(frame_list)):\n",
        "            for hflip in range(2):\n",
        "                preds_clip = []\n",
        "                try:\n",
        "                    penn.dataconf.fixed_hflip = hflip # Force horizontal flip\n",
        "\n",
        "                    \"\"\"Load clip and predict action.\"\"\"\n",
        "                    data = penn.get_data(i, TEST_MODE, frame_list=frame_list[f])\n",
        "                    a_true[i, :] = data['pennaction']\n",
        "\n",
        "                    pred = model.predict(np.expand_dims(data['frame'], axis=0))\n",
        "                    for b in range(num_blocks):\n",
        "                        allpred[b, 2*f+hflip, :] = pred[b][0]\n",
        "                        a_pred[b, i, :] *= pred[b][0]\n",
        "\n",
        "                    if np.argmax(a_true[i]) != np.argmax(a_pred[-1, i]):\n",
        "                        missing_clips['%04d.%03d.%d' % (i, f, hflip)] = [\n",
        "                                int(np.argmax(a_true[i])),\n",
        "                                int(np.argmax(a_pred[-1, i]))]\n",
        "\n",
        "                except Exception as e:\n",
        "                    warning('eval_multiclip, exception on sample ' \\\n",
        "                            + str(i) + ' frame ' + str(f) + ': ' + str(e))\n",
        "\n",
        "        if verbose:\n",
        "            cor = int(np.argmax(a_true[i]) == np.argmax(a_pred[-1, i]))\n",
        "\n",
        "            cnt_total += 1\n",
        "            cnt_corr += cor\n",
        "            printnl('%d : %.1f' % (cor, 100 * cnt_corr / cnt_total))\n",
        "\n",
        "    if logdir is not None:\n",
        "        np.save('%s/allpred.npy' % logdir, allpred)\n",
        "        np.save('%s/a_true.npy' % logdir, a_true)\n",
        "        with open(os.path.join(logdir, 'missing-clips.json'), 'w') as fid:\n",
        "            json.dump(missing_clips, fid)\n",
        "\n",
        "    a_true = np.expand_dims(a_true, axis=0)\n",
        "    a_true = np.tile(a_true, (num_blocks, 1, 1))\n",
        "    correct = np.argmax(a_true, axis=-1) == np.argmax(a_pred, axis=-1)\n",
        "    scores = 100*np.sum(correct, axis=-1) / num_samples\n",
        "    if verbose:\n",
        "        printcn(WARNING, 'PennAction, multi-clip.\\n')\n",
        "        printcn(WARNING, np.array2string(np.array(scores), precision=2))\n",
        "        printcn(WARNING, 'PennAction best: %.2f' % max(scores))\n",
        "\n",
        "    penn.dataconf.fixed_hflip = org_hflip\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "class PennActionEvalCallback(Callback):\n",
        "\n",
        "    def __init__(self, data, batch_size=1, eval_model=None,\n",
        "            logdir=None):\n",
        "\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.eval_model = eval_model\n",
        "        self.scores = {}\n",
        "        self.logdir = logdir\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if self.eval_model is not None:\n",
        "            model = self.eval_model\n",
        "        else:\n",
        "            model = self.model\n",
        "\n",
        "        if type(self.data) == BatchLoader:\n",
        "            scores = eval_singleclip_gt_bbox_generator(model, self.data)\n",
        "        else:\n",
        "            scores = eval_singleclip_gt_bbox(model, self.data[0],\n",
        "                    self.data[1], batch_size=self.batch_size)\n",
        "\n",
        "        epoch += 1\n",
        "        if self.logdir is not None:\n",
        "            if not hasattr(self, 'logarray'):\n",
        "                self.logarray = {}\n",
        "            self.logarray[epoch] = scores\n",
        "            with open(os.path.join(self.logdir, 'penn_val.json'), 'w') as f:\n",
        "                json.dump(self.logarray, f)\n",
        "\n",
        "        cur_best = max(scores)\n",
        "        self.scores[epoch] = cur_best\n",
        "\n",
        "        printcn(OKBLUE, 'Best score is %.1f at epoch %d' % \\\n",
        "                (100*self.best_score, self.best_epoch))\n",
        "\n",
        "    @property\n",
        "    def best_epoch(self):\n",
        "        if len(self.scores) > 0:\n",
        "            # Get the key of the maximum value from a dict\n",
        "            return max(self.scores, key=self.scores.get)\n",
        "        else:\n",
        "            return np.inf\n",
        "\n",
        "    @property\n",
        "    def best_score(self):\n",
        "        if len(self.scores) > 0:\n",
        "            # Get the maximum value from a dict\n",
        "            return self.scores[self.best_epoch]\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "# Aliases.\n",
        "eval_singleclip = eval_singleclip_gt_bbox\n",
        "eval_singleclip_generator = eval_singleclip_gt_bbox_generator"
      ],
      "metadata": {
        "id": "vm9vmLjfV_Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training both datasets**"
      ],
      "metadata": {
        "id": "XkRKDwiwWS0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Changing the neural network into VGG-16**"
      ],
      "metadata": {
        "id": "76t9-dZ6WYPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class sppnet():\n",
        "  def prediction_branch(self,x, cfg, pred_activate=True, replica=None,\n",
        "        forward_maps=True, name=None):\n",
        "\n",
        "    num_pred = cfg.num_joints\n",
        "\n",
        "    num_features = K.int_shape(x)[-1]\n",
        "\n",
        "    x = relu(x, name=appstr(name, '_act1'))\n",
        "    pred_maps = conv2d(x, num_pred, (1, 1), name=appstr(name, '_conv1'))\n",
        "\n",
        "    if replica:\n",
        "        replica = conv2d(x, num_pred, (1, 1),\n",
        "                name=appstr(name, '_conv1_replica'))\n",
        "\n",
        "    if forward_maps:\n",
        "        x = conv2d(x, num_pred, (1, 1), name=appstr(name, '_fw_maps'))\n",
        "        x = concatenate([x, pred_maps])\n",
        "    else:\n",
        "        x = pred_maps\n",
        "\n",
        "    if pred_activate:\n",
        "        x = relu(x, name=appstr(name, '_act2'))\n",
        "    x = conv2d(x, num_features, (1, 1), name=appstr(name, '_conv2'))\n",
        "\n",
        "    return x, pred_maps, replica\n",
        "\n",
        "\n",
        "  def action_prediction_early_fusion(self,xa, p, c, af, cfg, name=None):\n",
        "\n",
        "      num_actions = cfg.num_actions\n",
        "\n",
        "      num_features = max(cfg.num_pose_features, cfg.num_visual_features)\n",
        "      num_pose_features = cfg.num_pose_features\n",
        "      num_visual_features = cfg.num_visual_features\n",
        "\n",
        "      shortname = name[0:7] if name is not None else None\n",
        "\n",
        "      action = []\n",
        "\n",
        "      \"\"\"Apply individual softmax per dataset (set of actions).\"\"\"\n",
        "      def _individual_action_prediction(hlist, name=None):\n",
        "          for i in range(len(hlist)):\n",
        "              x = global_max_min_pooling(hlist[i])\n",
        "              x = Activation('softmax', name=appstr(name, '%d' % i))(x)\n",
        "              action.append(x)\n",
        "\n",
        "      \"\"\"Generic prediction block for both pose and apperance features.\"\"\"\n",
        "      def _prediction(x, name=None, shortname=None):\n",
        "          num_features = K.int_shape(x)[-1]\n",
        "\n",
        "          ident = x\n",
        "          x = BatchNormalization(name=appstr(name, '_bn1'))(x)\n",
        "          x = relu(x, name=appstr(name, '_act1'))\n",
        "          x1 = conv2d(x, num_features, (3, 3), name=appstr(name, '_conv1'))\n",
        "\n",
        "          x = max_min_pooling(x1, (2, 2))\n",
        "          x = BatchNormalization(name=appstr(name, '_bn2'))(x)\n",
        "          x = relu(x, name=appstr(name, '_act2'))\n",
        "          hlist = []\n",
        "          for i in range(len(num_actions)):\n",
        "              nact = num_actions[i]\n",
        "              h = conv2d(x, nact, (3, 3), name=appstr(name, '_conv2h%d' % i))\n",
        "              hlist.append(h)\n",
        "\n",
        "          _individual_action_prediction(hlist, name=shortname)\n",
        "          h = concat_tensorlist(hlist)\n",
        "\n",
        "          x = UpSampling2D((2, 2))(h)\n",
        "          x = relu(x, name=appstr(name, '_act3'))\n",
        "          x = conv2d(x, num_features, (3, 3), name=appstr(name, '_conv3'))\n",
        "          x = add([ident, x1, x])\n",
        "\n",
        "          return x\n",
        "\n",
        "      \"\"\"Define padding strategy.\"\"\"\n",
        "      num_frames, num_joints = K.int_shape(p)[1:3]\n",
        "      time_stride = 2 if num_frames >= 16 else 1\n",
        "      get_pad = lambda div, n: int(div*np.ceil(n / div) - n)\n",
        "      joints_pad = get_pad(4, num_joints)\n",
        "      frames_pad = get_pad(2 * time_stride, num_frames)\n",
        "      top_pad = frames_pad // 2\n",
        "      bottom_pad = (frames_pad + 1) // 2\n",
        "      left_pad = joints_pad // 2\n",
        "      right_pad = (joints_pad + 1) // 2\n",
        "\n",
        "      \"\"\"Pose features.\"\"\"\n",
        "      mask = Lambda(lambda x: K.tile(x, (1, 1, 1, K.int_shape(p)[-1])))(c)\n",
        "      x = Lambda(lambda x: x[0] * x[1])([p, mask])\n",
        "\n",
        "      a = conv2d(x, num_pose_features // 16, (3, 1),\n",
        "              name=appstr(name, '_p_conv0a'))\n",
        "      b = conv2d(x, num_pose_features // 8, (3, 3),\n",
        "              name=appstr(name, '_p_conv0b'))\n",
        "      c = conv2d(x, num_pose_features // 4, (3, 5),\n",
        "              name=appstr(name, '_p_conv0c'))\n",
        "      x = concatenate([a, b, c])\n",
        "\n",
        "      x = residual(x, (3, 3), out_size=num_pose_features, convtype='normal',\n",
        "              features_div=2, name=appstr(name, '_r1'))\n",
        "\n",
        "      if top_pad + bottom_pad + left_pad + right_pad > 0:\n",
        "          x = ZeroPadding2D(((top_pad, bottom_pad), (left_pad, right_pad)))(x)\n",
        "      x1 = maxpooling2d(x, (2, 2), strides=(time_stride, 2))\n",
        "\n",
        "      \"\"\"Appearance features.\"\"\"\n",
        "      x = conv2d(af, num_visual_features, (1, 1), name=appstr(name, '_v_conv0'))\n",
        "\n",
        "      if top_pad + bottom_pad + left_pad + right_pad > 0:\n",
        "          x = ZeroPadding2D(((top_pad, bottom_pad), (left_pad, right_pad)))(x)\n",
        "      x2 = maxpooling2d(x, (2, 2), strides=(time_stride, 2))\n",
        "\n",
        "      \"\"\"Feature fusion.\"\"\"\n",
        "      fusion = [x1, x2]\n",
        "      if xa is not None:\n",
        "          fusion.append(xa)\n",
        "\n",
        "      x = concat_tensorlist(fusion)\n",
        "      # x = add_tensorlist(fusion)\n",
        "      x = residual(x, (3, 3), out_size=num_features, convtype='normal',\n",
        "              features_div=4, name=appstr(name, '_r2'))\n",
        "\n",
        "      xa = _prediction(x, name=appstr(name, '_pred'),\n",
        "              shortname=appstr(shortname, '_a'))\n",
        "\n",
        "      return action, xa\n",
        "\n",
        "\n",
        "  def prediction_block(self,xp, xa, zp, outlist, cfg, do_action, name=None):\n",
        "\n",
        "      dim = cfg.dim\n",
        "      kernel_size = cfg.kernel_size\n",
        "      xmin = cfg.xmin\n",
        "      ymin = cfg.ymin\n",
        "      sam_alpha = cfg.sam_alpha\n",
        "      num_features = K.int_shape(xp)[-1]\n",
        "      replica = cfg.pose_replica and do_action\n",
        "      dbg_decoupled_pose = cfg.dbg_decoupled_pose and do_action\n",
        "      dbg_decoupled_h = cfg.dbg_decoupled_h and do_action\n",
        "\n",
        "      xp = residual(xp, kernel_size, name=appstr(name, '_r1'))\n",
        "      reinject = [xp]\n",
        "\n",
        "      xp = BatchNormalization(name=appstr(name, '_bn1'))(xp)\n",
        "      xp = relu(xp, name=appstr(name, '_act1'))\n",
        "      xp = sepconv2d(xp, num_features, kernel_size, name=appstr(name, '_conv1'))\n",
        "      reinject.append(xp)\n",
        "\n",
        "      xp = BatchNormalization(name=appstr(name, '_bn2'))(xp)\n",
        "\n",
        "      \"\"\"2D pose estimation.\"\"\"\n",
        "      x1, org_h, rep_h = self.prediction_branch(xp, cfg, pred_activate=True,\n",
        "              replica=replica, name=appstr(name, '_heatmaps'))\n",
        "      reinject.append(x1)\n",
        "\n",
        "      h = Activation(channel_softmax_2d(alpha=sam_alpha),\n",
        "              name=appstr(name, '_probmaps'))(org_h)\n",
        "\n",
        "      p = softargmax2d(h, limits=(xmin, ymin, 1-xmin, 1-ymin),\n",
        "              name=appstr(name, '_xy'))\n",
        "      c = keypoint_confidence(h, name=appstr(name, '_vis'))\n",
        "\n",
        "      if dbg_decoupled_pose:\n",
        "          \"\"\"Output decoupled poses in debug mode.\"\"\"\n",
        "          dbg_h = Activation(channel_softmax_2d(alpha=sam_alpha),\n",
        "                  name=appstr(name, '_dbg_h'))(rep_h)\n",
        "          dbg_p = softargmax2d(dbg_h, limits=(xmin, ymin, 1-xmin, 1-ymin),\n",
        "                  name=appstr(name, '_dbg_xy'))\n",
        "\n",
        "          dbg_c = keypoint_confidence(dbg_h, name=appstr(name, '_dbg_vis'))\n",
        "\n",
        "      \"\"\"Depth estimation.\"\"\"\n",
        "      if dim == 3:\n",
        "          x1, org_d, rep_d = self.prediction_branch(xp, cfg, pred_activate=False,\n",
        "                  replica=replica, forward_maps=False,\n",
        "                  name=appstr(name, '_depthmaps'))\n",
        "          reinject.append(x1)\n",
        "\n",
        "          d = Activation('sigmoid')(org_d)\n",
        "          z = multiply([d, h])\n",
        "          z = Lambda(lambda x: K.sum(x, axis=(-2, -3)))(z)\n",
        "          z = Lambda(lambda x: K.expand_dims(x, axis=-1))(z)\n",
        "          p = concatenate([p, z], name=appstr(name, '_xyz'))\n",
        "\n",
        "      \"\"\"Visual features (for action only).\"\"\"\n",
        "      action = []\n",
        "      if do_action:\n",
        "          if 'act_cnt' not in globals():\n",
        "              global act_cnt\n",
        "              act_cnt = 0\n",
        "          act_cnt += 1\n",
        "          act_name = 'act%d' % act_cnt\n",
        "\n",
        "          act_h = rep_h if replica else org_h\n",
        "          act_h = Activation(channel_softmax_2d(alpha=sam_alpha),\n",
        "                  name=appstr(act_name, '_probmaps2'))(act_h)\n",
        "          act_p = softargmax2d(act_h, limits=(xmin, ymin, 1-xmin, 1-ymin),\n",
        "                  name=appstr(act_name, '_xy2'))\n",
        "          act_c = keypoint_confidence(act_h, name=appstr(act_name, '_vis2'))\n",
        "\n",
        "          if dim == 3:\n",
        "              act_d = rep_d if replica else org_d\n",
        "              act_d = Activation('sigmoid')(act_d)\n",
        "              act_z = multiply([act_d, act_h])\n",
        "              act_z = Lambda(lambda x: K.sum(x, axis=(-2, -3)))(act_z)\n",
        "              act_z = Lambda(lambda x: K.expand_dims(x, axis=-1))(act_z)\n",
        "              act_p = concatenate([act_p, act_z],\n",
        "                      name=appstr(act_name, '_xyz2'))\n",
        "\n",
        "          af = kronecker_prod(act_h, zp, name=appstr(act_name, '_kron'))\n",
        "\n",
        "          action, xa = self.action_prediction_early_fusion(xa, act_p, act_c, af, cfg,\n",
        "                  name=appstr(act_name, '_action'))\n",
        "\n",
        "      xp = add_tensorlist(reinject)\n",
        "      outlist[0].append(concatenate([p, c], name=name))\n",
        "      if do_action:\n",
        "          outlist[1] += action\n",
        "\n",
        "      if dbg_decoupled_pose:\n",
        "          outlist[2].append(concatenate([dbg_p, dbg_c]))\n",
        "          outlist[3].append(dbg_h)\n",
        "\n",
        "      sys.stdout.flush()\n",
        "\n",
        "      return xp, xa\n",
        "\n",
        "\n",
        "  def downscaling_pyramid(self,lp, la, lzp, outlist, cfg, do_action, name=None):\n",
        "\n",
        "      assert len(lp) == len(la), \\\n",
        "              'Pose and action must have the same number of levels!'\n",
        "      xp = lp[0]\n",
        "      xa = la[0]\n",
        "      if lzp[0] is None:\n",
        "          lzp[0] = xp\n",
        "\n",
        "      for i in range(1, len(lp)):\n",
        "          num_features = K.int_shape(xp)[-1] + cfg.growth\n",
        "\n",
        "          xp = downscaling(xp, cfg, out_size=num_features,\n",
        "                  name=appstr(name, '_du%d' % i))\n",
        "\n",
        "          if lzp[i] is None:\n",
        "              lzp[i] = xp\n",
        "\n",
        "          if lp[i] is not None:\n",
        "              xp = add([xp, lp[i]])\n",
        "\n",
        "          if xa is not None and do_action:\n",
        "              xa = residual(xa, (3, 3), name=appstr(name, '_du%d_action_r0' % i))\n",
        "              if la[i] is not None:\n",
        "                  xa = add([xa, la[i]])\n",
        "\n",
        "          xp, xa = self.prediction_block(xp, xa, lzp[i], outlist, cfg, do_action,\n",
        "                  name=appstr(name, '_pb%d' % i))\n",
        "\n",
        "          lp[i] = xp # lateral pose connection\n",
        "          la[i] = xa # lateral action connection\n",
        "\n",
        "\n",
        "  def upscaling_pyramid(self,lp, la, lzp, outlist, cfg, do_action, name=None):\n",
        "\n",
        "      assert len(lp) == len(la), \\\n",
        "              'Pose and action must have the same number of levels!'\n",
        "      xp = lp[-1]\n",
        "      xa = la[-1]\n",
        "      if lzp[0] is None:\n",
        "          lzp[0] = xp\n",
        "\n",
        "      for i in range(len(lp)-1)[::-1]:\n",
        "          num_features = K.int_shape(xp)[-1] - cfg.growth\n",
        "\n",
        "          xp = upscaling(xp, cfg, out_size=num_features,\n",
        "                  name=appstr(name, '_uu%d' % i))\n",
        "\n",
        "          if lzp[i] is None:\n",
        "              lzp[i] = xp\n",
        "\n",
        "          if lp[i] is not None:\n",
        "              xp = add([xp, lp[i]])\n",
        "\n",
        "          if xa is not None and do_action:\n",
        "              xa = residual(xa, (3, 3), name=appstr(name, '_uu%d_action_r0' % i))\n",
        "              if la[i] is not None:\n",
        "                  xa = add([xa, la[i]])\n",
        "\n",
        "          xp, xa = self.prediction_block(xp, xa, lzp[i], outlist, cfg, do_action,\n",
        "                  name=appstr(name, '_pb%d' % i))\n",
        "\n",
        "          lp[i] = xp # lateral pose connection\n",
        "          la[i] = xa # lateral action connection\n",
        "\n",
        "  def entry_flow(self, x, cfg):\n",
        "\n",
        "      # Initialize ResNet50 without the top classification layer\n",
        "      base_resnet = ResNet50(include_top=False, weights='imagenet')\n",
        "\n",
        "      # Create a model to get the output of an intermediate layer (e.g., 'conv4_block6_out')\n",
        "      intermediate_layer_model = Model(inputs=base_resnet.input,\n",
        "                                     outputs=base_resnet.get_layer('conv4_block6_out').output)\n",
        "\n",
        "      # Apply TimeDistributed to process each frame\n",
        "      td_resnet = TimeDistributed(intermediate_layer_model)(x)\n",
        "\n",
        "      # Freeze ResNet layers, if desired\n",
        "      for layer in intermediate_layer_model.layers:\n",
        "          layer.trainable = False\n",
        "\n",
        "      # Apply additional layers if needed to adjust the size\n",
        "      # Example: Upsample to increase spatial dimensions\n",
        "      upsampled_output = TimeDistributed(tf.keras.layers.UpSampling2D(size=(2, 2)))(td_resnet)\n",
        "\n",
        "      return upsampled_output\n",
        "\n",
        "\n",
        "  def build(self,cfg, stop_grad_stem=False):\n",
        "      \"\"\"Sequential Pyramid Networks for 3D human pose estimation and\n",
        "      action recognition.\n",
        "      \"\"\"\n",
        "      assert type(cfg) == ModelConfig, \\\n",
        "              'type(cfg) ({}) is not ModelConfig'.format(type(cfg))\n",
        "\n",
        "      input_shape = cfg.input_shape\n",
        "      assert len(input_shape) in [3, 4], \\\n",
        "              'Invalid input_shape ({})'.format(input_shape)\n",
        "\n",
        "      inp = Input(shape=input_shape)\n",
        "      outlist = [] # Holds [[poses], [dbg1], [action1], [actions2], ...]\n",
        "      for i in range(len(cfg.num_actions) + 1 + 2*cfg.dbg_decoupled_pose):\n",
        "          outlist.append([])\n",
        "\n",
        "      if len(input_shape) == 3:\n",
        "          num_rows, num_cols, _ = input_shape\n",
        "      else:\n",
        "          num_frames, num_rows, num_cols, _ = input_shape\n",
        "\n",
        "      cfg.xmin = 1 / (2 * num_cols)\n",
        "      cfg.ymin = 1 / (2 * num_rows)\n",
        "\n",
        "      x = self.entry_flow(inp, cfg)\n",
        "      print(x)\n",
        "      if stop_grad_stem:\n",
        "          x = Lambda(lambda x: K.stop_gradient(x))(x)\n",
        "\n",
        "      lp = []\n",
        "      la = []\n",
        "      lzp = []\n",
        "      for i in range(cfg.num_levels):\n",
        "          lp.append(None)\n",
        "          la.append(None)\n",
        "          lzp.append(None)\n",
        "\n",
        "      lp[0] = x\n",
        "      for pyr in range(cfg.num_pyramids):\n",
        "\n",
        "          do_action = (pyr + 1) in cfg.action_pyramids\n",
        "\n",
        "          if pyr % 2 == 0: # Even pyramids (0, 2, ...)\n",
        "              self.downscaling_pyramid(lp, la, lzp, outlist, cfg, do_action,\n",
        "                      name='dp%d' % (pyr+1))\n",
        "\n",
        "          else: # Odd pyramids (1, 3, ...)\n",
        "              self.upscaling_pyramid(lp, la, lzp, outlist, cfg, do_action,\n",
        "                      name='up%d' % (pyr+1))\n",
        "\n",
        "      outputs = []\n",
        "      for o in outlist:\n",
        "          outputs += o\n",
        "\n",
        "      model = Model(inputs=inp, outputs=outputs, name='SPNet')\n",
        "\n",
        "      return model\n",
        "\n",
        "\n",
        "  def get_num_predictions(self,num_pyramids, num_levels):\n",
        "      return num_pyramids * (num_levels - 1)\n",
        "\n",
        "\n",
        "  def split_model(self,full_model, cfg, interlaced=False, model_names=[None, None]):\n",
        "\n",
        "      num_pose_pred = self.get_num_predictions(cfg.num_pyramids, cfg.num_levels)\n",
        "      num_act_pred = self.get_num_predictions(len(cfg.action_pyramids), cfg.num_levels)\n",
        "      assert len(full_model.outputs) == \\\n",
        "              num_pose_pred + len(cfg.num_actions)*num_act_pred, \\\n",
        "              'The given model and config are not compatible!'\n",
        "      assert num_act_pred > 0, 'You are trying to split a \"pose only\" model.'\n",
        "\n",
        "      if interlaced:\n",
        "          out_p = []\n",
        "          out_a = []\n",
        "\n",
        "          idx = 0\n",
        "          for i in range(num_pose_pred):\n",
        "              out_p.append(full_model.outputs[idx])\n",
        "              idx += 1\n",
        "              if len(out_a) < len(cfg.num_actions)*num_act_pred:\n",
        "                  for aidx in range(len(cfg.num_actions)):\n",
        "                      out_a.append(full_model.outputs[idx])\n",
        "                      idx += 1\n",
        "\n",
        "          modelp = Model(full_model.input, out_p, name=model_names[0])\n",
        "          modela = Model(full_model.input, out_a, name=model_names[1])\n",
        "\n",
        "      else:\n",
        "          modelp = Model(full_model.input, full_model.outputs[:num_pose_pred],\n",
        "                  name=model_names[0])\n",
        "          modela = Model(full_model.input, full_model.outputs[num_pose_pred:],\n",
        "                  name=model_names[1])\n",
        "\n",
        "      return [modelp, modela]\n",
        "\n",
        "\n",
        "  def compile_split_models(self,full_model, cfg, optimizer,\n",
        "            pose_trainable=False,\n",
        "            copy_replica=False,\n",
        "            ar_loss_weights=0.01,\n",
        "            interlaced=False,\n",
        "            verbose=0):\n",
        "\n",
        "        if copy_replica:\n",
        "            copy_replica_layers(full_model)\n",
        "\n",
        "        \"\"\"Split the model into pose estination and action recognition parts.\"\"\"\n",
        "        models = self.split_model(full_model, cfg, interlaced=interlaced,\n",
        "                model_names=['Pose', 'Action'])\n",
        "\n",
        "        pose_loss = pose_regression_loss('l1l2bincross', 0.01)\n",
        "        action_loss = 'categorical_crossentropy'\n",
        "\n",
        "        set_trainable_layers(full_model, 'action', None, pose_trainable)\n",
        "        loss_weights_pe = len(models[0].outputs) * [1.0]\n",
        "        loss_weights_ar = len(models[1].outputs) * [ar_loss_weights]\n",
        "\n",
        "        models[0].compile(loss=pose_loss, optimizer=optimizer,\n",
        "                loss_weights=loss_weights_pe)\n",
        "        models[1].compile(loss=action_loss, optimizer=optimizer,\n",
        "                loss_weights=loss_weights_ar)\n",
        "\n",
        "        def print_layer(self,layer, prefix=''):\n",
        "            c = FAIL if layer.trainable else OKGREEN\n",
        "            printc(c, prefix + '%s\\t| ' % (layer.name))\n",
        "            try:\n",
        "                nparam = np.sum([np.prod(K.int_shape(p))\n",
        "                    for p in layer._trainable_weights])\n",
        "                printcn(c, prefix + '%s\\t| %s\\t| %d' % (str(type(layer)),\n",
        "                    str(layer.output_shape), nparam))\n",
        "            except:\n",
        "                print('')\n",
        "\n",
        "        if verbose:\n",
        "            for i in range(2):\n",
        "                printcn(HEADER, 'Model %s trainable layers:' % models[i].name)\n",
        "                for m in models[i].layers:\n",
        "                    print_layer(m)\n",
        "                    if type(m) == TimeDistributed:\n",
        "                        print_layer(m.layer, prefix='td:\\t')\n",
        "                    elif type(m) == Model:\n",
        "                        for n in m.layers:\n",
        "                            print_layer(n, prefix='>> \\t')\n",
        "\n",
        "        return models"
      ],
      "metadata": {
        "id": "NQajxbliV2ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CPU/GPU backend**"
      ],
      "metadata": {
        "id": "db-OFYnrW77O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to use GPU if available, otherwise use CPU\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    tf.config.set_visible_devices(tf.config.list_physical_devices('GPU'), 'GPU')\n",
        "    print(\"GPU available. Using GPU.\")\n",
        "else:\n",
        "    tf.config.set_visible_devices([], 'GPU')\n",
        "    print(\"No GPU available. Using CPU.\")"
      ],
      "metadata": {
        "id": "9ZFtSX65XAN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model building**"
      ],
      "metadata": {
        "id": "wIUibiWbXTiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spnet=sppnet()\n",
        "logdir = '/home/jovyan/workspace/log'\n",
        "if len(sys.argv) > 1:\n",
        "    logdir = sys.argv[1]\n",
        "    mkdir(logdir)\n",
        "    sys.stdout = open(str(logdir) + '/log.txt', 'w')\n",
        "\n",
        "num_frames = 4\n",
        "cfg = ModelConfig((num_frames,) + pennaction_dataconf.input_shape, pa16j2d,\n",
        "        num_actions=[15], num_pyramids=2, action_pyramids=[1, 2],\n",
        "        num_levels=4, pose_replica=False,\n",
        "        num_pose_features=160, num_visual_features=160)\n",
        "\n",
        "num_predictions = spnet.get_num_predictions(cfg.num_pyramids, cfg.num_levels)\n",
        "num_action_predictions = \\\n",
        "        spnet.get_num_predictions(len(cfg.action_pyramids), cfg.num_levels)\n",
        "\n",
        "start_lr = 0.001\n",
        "action_weight = 0.01\n",
        "batch_size_mpii = int(0.8 * num_frames)\n",
        "# batch_size_penn = num_frames - batch_size_mpii\n",
        "batch_size_penn = num_frames\n",
        "batch_clips = 4 # 8/4\n",
        "\n",
        "\"\"\"Load datasets\"\"\"\n",
        "mpii = MpiiSinglePerson('/home/jovyan/workspace/deephar/datasets/MPII', dataconf=mpii_dataconf,\n",
        "        poselayout=pa16j2d)\n",
        "\n",
        "penn_sf = PennAction('/home/jovyan/workspace/deephar/datasets/PennAction', pennaction_pe_dataconf,\n",
        "        poselayout=pa16j2d, topology='frames', use_gt_bbox=True)\n",
        "\n",
        "penn_seq = PennAction('/home/jovyan/workspace/deephar/datasets/PennAction', pennaction_dataconf,\n",
        "        poselayout=pa16j2d, topology='sequences', use_gt_bbox=True,\n",
        "        clip_size=num_frames)\n",
        "# pe_data_tr = BatchLoader([mpii, penn_sf], ['frame'], ['pose'], TRAIN_MODE,\n",
        "pe_data_tr = BatchLoader([mpii], ['frame'], ['pose'], TRAIN_MODE,\n",
        "        # batch_size=[batch_size_mpii, batch_size_penn], shuffle=True)\n",
        "        batch_size=[batch_size_penn], shuffle=True)\n",
        "pe_data_tr = BatchLoader(pe_data_tr, ['frame'], ['pose'], TRAIN_MODE,\n",
        "        batch_size=batch_clips, num_predictions=num_predictions, shuffle=False)\n",
        "\n",
        "ar_data_tr = BatchLoader(penn_seq, ['frame'], ['pennaction'], TRAIN_MODE,\n",
        "        batch_size=batch_clips, num_predictions=num_action_predictions,\n",
        "        shuffle=True)\n",
        "\"\"\"Build the full model\"\"\"\n",
        "full_model = spnet.build(cfg)\n",
        "\n",
        "\n",
        "\"\"\"Trick to pre-load validation samples and generate the eval. callback.\"\"\"\n",
        "mpii_val = BatchLoader(mpii, ['frame'], ['pose', 'afmat', 'headsize'],\n",
        "        VALID_MODE, batch_size=mpii.get_length(VALID_MODE), shuffle=False)\n",
        "printnl('Pre-loading MPII validation data...')\n",
        "[x_val], [p_val, afmat_val, head_val] = mpii_val[0]\n",
        "\n",
        "penn_te = BatchLoader(penn_seq, ['frame'], ['pennaction'], TEST_MODE,\n",
        "        batch_size=1, shuffle=False)\n",
        "\n",
        "\"\"\"Save model callback.\"\"\"\n",
        "save_model = SaveModel(os.path.join(logdir,\n",
        "    'weights_mpii+penn_ar_{epoch:03d}.hdf5'), model_to_save=full_model)"
      ],
      "metadata": {
        "id": "WZT1Jf7cXM-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "mTwk2eXNXiGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_training(pose_trainable, lr):\n",
        "    optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=lr)\n",
        "    models = spnet.compile_split_models(full_model, cfg, optimizer,\n",
        "            pose_trainable=pose_trainable, ar_loss_weights=action_weight,\n",
        "            copy_replica=cfg.pose_replica)\n",
        "    full_model.summary()\n",
        "\n",
        "    \"\"\"Create validation callbacks.\"\"\"\n",
        "    mpii_callback = MpiiEvalCallback(x_val, p_val, afmat_val, head_val,\n",
        "            eval_model=models[0], pred_per_block=1, batch_size=1, logdir=logdir)\n",
        "    penn_callback = PennActionEvalCallback(penn_te, eval_model=models[1],\n",
        "            logdir=logdir)\n",
        "\n",
        "    def end_of_epoch_callback(epoch):\n",
        "\n",
        "        save_model.on_epoch_end(epoch)\n",
        "        mpii_callback.on_epoch_end(epoch)\n",
        "        penn_callback.on_epoch_end(epoch)\n",
        "\n",
        "        if epoch in [15, 25]:\n",
        "            lr = float(K.get_value(optimizer.lr))\n",
        "            newlr = 0.1*lr\n",
        "            K.set_value(optimizer.lr, newlr)\n",
        "            printcn(WARNING, 'lr_scheduler: lr %g -> %g @ %d' \\\n",
        "                    % (lr, newlr, epoch))\n",
        "\n",
        "    return end_of_epoch_callback, models\n",
        "\n",
        "steps_per_epoch = mpii.get_length(TRAIN_MODE) // batch_size_mpii\n",
        "\n",
        "fcallback, models = prepare_training(False, start_lr)"
      ],
      "metadata": {
        "id": "JGk6hPXFXlqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = MultiModelTrainer(models[1:], [ar_data_tr], workers=12,\n",
        "        print_full_losses=True)\n",
        "trainer.train(2, steps_per_epoch=steps_per_epoch, initial_epoch=0,\n",
        "        end_of_epoch_callback=fcallback)\n",
        "\n",
        "\"\"\"Joint learning the full model.\"\"\"\n",
        "fcallback, models = prepare_training(True, start_lr)\n",
        "trainer = MultiModelTrainer(models, [pe_data_tr, ar_data_tr], workers=12,\n",
        "        print_full_losses=True)\n",
        "trainer.train(30, steps_per_epoch=steps_per_epoch, initial_epoch=2,\n",
        "        end_of_epoch_callback=fcallback)"
      ],
      "metadata": {
        "id": "nqc8YXD7X3fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating and testing results**"
      ],
      "metadata": {
        "id": "28B7LRp8X86w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spnet=sppnet()\n",
        "\n",
        "logdir = './'\n",
        "if len(sys.argv) > 1:\n",
        "    logdir = sys.argv[1]\n",
        "    mkdir(logdir)\n",
        "    sys.stdout = open(str(logdir) + '/log_eval.txt', 'w')\n",
        "\n",
        "num_frames = 1\n",
        "cfg = ModelConfig((num_frames,) + pennaction_dataconf.input_shape, pa16j2d,\n",
        "        num_actions=[15], num_pyramids=6, action_pyramids=[5, 6],\n",
        "        num_levels=4, pose_replica=True,\n",
        "        num_pose_features=160, num_visual_features=160)\n",
        "\n",
        "num_predictions = spnet.get_num_predictions(cfg.num_pyramids, cfg.num_levels)\n",
        "num_action_predictions = \\\n",
        "        spnet.get_num_predictions(len(cfg.action_pyramids), cfg.num_levels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Build the full model\"\"\"\n",
        "full_model = spnet.build(cfg)\n",
        "\n",
        "weights_file = '/content/drive/MyDrive/project_data/datasets/weights_mpii+penn_ar_010.hdf5'\n",
        "if os.path.isfile(weights_file) == False:\n",
        "    print (f'Error: file {weights_file} not found!')\n",
        "    print (f'\\nPlease download it from https://drive.google.com/file/d/106yIhqNN-TrI34SX81q2xbU-NczcQj6I/view?usp=sharing')\n",
        "    sys.stdout.flush()\n",
        "    sys.exit()\n",
        "\n",
        "\"\"\"Load pre-trained weights from pose estimation and copy replica layers.\"\"\"\n",
        "full_model.load_weights(weights_file, by_name=True)\n",
        "\n",
        "models = spnet.split_model(full_model, cfg, interlaced=False,\n",
        "        model_names=['2DPose', '2DAction'])\n",
        "\n",
        "\n",
        "\"\"\"Evaluate on 2D action recognition (PennAction).\"\"\"\n",
        "s = eval_singleclip_generator(models[1], penn_te)\n",
        "print ('Best score on PennAction (single-clip): ' + str(s))\n",
        "\n",
        "s = eval_multiclip_dataset(models[1], penn_seq,subsampling=pennaction_dataconf.fixed_subsampling)\n",
        "print ('Best score on PennAction (multi-clip): ' + str(s))\n",
        "\n",
        "\"\"\"Evaluate on 2D pose estimation (MPII).\"\"\"\n",
        "s = eval_singleperson_pckh(models[0], x_val, p_val[:, :, 0:2], afmat_val, head_val)\n",
        "print ('Best score on MPII: ' + str(s))\n",
        "\n"
      ],
      "metadata": {
        "id": "i4R6zqGmX4kQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}